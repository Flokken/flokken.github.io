(window.webpackJsonp=window.webpackJsonp||[]).push([[56],{387:function(a,t,s){"use strict";s.r(t);var r=s(4),p=Object(r.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"deep-bayeis"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#deep-bayeis"}},[a._v("#")]),a._v(" Deep Bayeis")]),a._v(" "),t("h3",{attrs:{id:"etm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#etm"}},[a._v("#")]),a._v(" ETM")]),a._v(" "),t("p",[a._v("ETM是LDA的深度学习的版本")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230716224426537.png",alt:"image-20230716224426537"}})]),a._v(" "),t("p",[a._v("这里有一个Logistic-norm，其实也是为了重参数，让他可以做反向传播，之所以没有用vae的那个，是因为这篇论文更早，还没发明这个trick，因此编了一个。和高斯分布那个重参数化很像。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717101339930.png",alt:"image-20230717101339930"}})]),a._v(" "),t("h3",{attrs:{id:"wae"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wae"}},[a._v("#")]),a._v(" WAE")]),a._v(" "),t("p",[a._v("WAE可以看做是 WGAN 和 VAE 的产物，可以说既解决了 GAN 的一些问题，也解决了 VAE 的顽疾。")]),a._v(" "),t("p",[t("strong",[a._v("主要是把只把 KL 散度那一项改进了")])]),a._v(" "),t("p",[a._v("复习VAE的elbo")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717103710460.png",alt:"image-20230717103710460"}})]),a._v(" "),t("p",[t("strong",[a._v("WAE的的改进就是不用KL散度度量两个分布之间距离了，而是使用MMD损失")])]),a._v(" "),t("h4",{attrs:{id:"maximum-mean-discrepancy-最大平均偏差-mmd"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#maximum-mean-discrepancy-最大平均偏差-mmd"}},[a._v("#")]),a._v(" Maximum Mean Discrepancy 最大平均偏差 MMD")]),a._v(" "),t("p",[t("strong",[a._v("MMD (Maximum Mean Discrepancy)")]),a._v(" 最早是 WGAN 为了解决 GAN 相关问题时引入的，后来被 WAE 引入到了 VAE 的计算框架下，实际上也是机器学习、统计学中早就开始用的东西。")]),a._v(" "),t("p",[a._v("一言以蔽之，MMD 告诉我们，想要"),t("strong",[a._v("度量两个分布之间的距离")]),a._v("，可以通过"),t("strong",[a._v("度量从两个分布中采样的点之间的距离")]),a._v("来完成。这样我们"),t("strong",[a._v("不需要计算两个分布之间 KL 散度的表达式，更不需要做 reparameterization trick")]),a._v("，只需要从分布中采样点，用 MMD 即可计算两个分布之间的距离，作为 VAE 损失中的正则化项。")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230716224440844.png",alt:"image-20230716224440844"}})]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717103819875.png",alt:"image-20230717103819875"}})]),a._v(" "),t("blockquote",[t("p",[a._v("MMDLoss   就代表了  Wasserstein distance")])]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104336222.png",alt:"image-20230717104336222"}})]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104326885.png",alt:"image-20230717104326885"}})]),a._v(" "),t("blockquote",[t("p",[a._v("为什么用高位映射？低维的线性不可分到了高维，也许就是线性可分的了")])]),a._v(" "),t("h4",{attrs:{id:"核函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#核函数"}},[a._v("#")]),a._v(" 核函数")]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104541998.png",alt:"image-20230717104541998"}})]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104550156.png",alt:"image-20230717104550156"}})]),a._v(" "),t("p",[t("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104648939.png",alt:"image-20230717104648939"}})]),a._v(" "),t("h4",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[a._v("#")]),a._v(" 总结")]),a._v(" "),t("p",[t("strong",[a._v("值得注意的是，WAE 通过 MMD 做到了：")])]),a._v(" "),t("ol",[t("li",[a._v("WAE 告诉我们，度量对两个分布中采样的点的距离，也可以作为 度量分布之间距离的方法")]),a._v(" "),t("li",[a._v("WAE 帮我们解决了 VAE 中 KL 计算复杂、无法应用于复杂分布 的问题")]),a._v(" "),t("li",[a._v("WAE 损失计算关键在 MMD，关键在于 kernel 的选择*")])])])}),[],!1,null,null,null);t.default=p.exports}}]);