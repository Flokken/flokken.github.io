(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{383:function(t,a,s){"use strict";s.r(a);var m=s(4),i=Object(m.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"lda"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda"}},[t._v("#")]),t._v(" LDA")]),t._v(" "),a("blockquote",[a("p",[t._v("Latent Dirichlet Allocation LDA 主题模型")])]),t._v(" "),a("p",[a("strong",[t._v("主要是以LDA 为例思考通过贝叶斯生成模型给问题建模、并解决问题的方式")]),t._v("。")]),t._v(" "),a("p",[t._v("生成式模型求解问题的思路总结起来就三步：")]),t._v(" "),a("ol",[a("li",[t._v("想想数据的生成过程")]),t._v(" "),a("li",[t._v("通过概率模型描述你想的这个生成过程")]),t._v(" "),a("li",[t._v("运用各种手段解模型参数")])]),t._v(" "),a("h3",{attrs:{id:"lda介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda介绍"}},[t._v("#")]),t._v(" LDA介绍")]),t._v(" "),a("p",[a("strong",[t._v("LDA (Latent Dirichlet Allocation)")]),t._v(" 是 NLP 中解决**主题模型（topic modeling）**这一子任务的模型，中文名叫潜在狄利克雷分配。")]),t._v(" "),a("p",[t._v("**主题模型（topic modeling）**这一任务实际上说的就是，给定一堆文档（博客、评论、朋友圈动态等，总之就是一段文本，可长可短）"),a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/7f1b68a2484cb07eaaca91d4a5f05d53.svg",alt:"img"}}),t._v("，模型的任务就是输入一篇文档，输出这篇文档对应的主题（topic）是什么，比如体育、新闻、经济等。一篇文章可以对应多个主题，同样也可能有多篇文档符合同一个主题。")]),t._v(" "),a("p",[t._v("LDA 中的这三个词也比较好理解啦：")]),t._v(" "),a("ul",[a("li",[t._v("Latent 就是隐空间、隐变量的“隐”，指的是 LDA 是通过隐变量来建模的，这也符合贝叶斯学习框架")]),t._v(" "),a("li",[t._v("Dirichlet 狄利克雷，指的是 LDA 模型中用的一个主要概率分布是狄利克雷分布")]),t._v(" "),a("li",[t._v("Allocation 分配，指的是  LDA 是通过“分配”的方式来考虑一篇文档的生成过程")])]),t._v(" "),a("h4",{attrs:{id:"bow词袋模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bow词袋模型"}},[t._v("#")]),t._v(" BoW词袋模型")]),t._v(" "),a("blockquote",[a("p",[t._v("https://blog.csdn.net/Elenstone/article/details/105134863")])]),t._v(" "),a("p",[t._v("BoW(Bag of Words)词袋模型最初被用在文本分类中，将文档表示成"),a("strong",[t._v("特征矢量")]),t._v("。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。")]),t._v(" "),a("p",[t._v("如三个句子如下：")]),t._v(" "),a("p",[t._v("句子1：小孩喜欢吃零食。\n句子2：小孩喜欢玩游戏，不喜欢运动。\n句子3 ：大人不喜欢吃零食，喜欢运动。\n首先根据语料中出现的句子分词，然后构建词袋（每一个出现的词都加进来）。计算机不认识字，只认识数字，那在计算机中怎么表示词袋模型呢？其实很简单，给每个词一个位置索引就可以了。小孩放在第一个位置，喜欢放在第二个位置，以此类推。")]),t._v(" "),a("p",[a("code",[t._v("{“小孩”:1，“喜欢”:2，“吃”:3，“零食”:4，“玩”:5，“游戏”:6，“大人”:7，“不”:8，“运动”:9}")])]),t._v(" "),a("p",[t._v("其中key为词，value为词的索引，语料中共有9个单词， 那么每个文本我们就可以使用一个9维的向量来表示。\n如果文本中含有的一个词出现了一次，就让那个词的位置置为1，词出现几次就置为几，那么上述文本可以表示为：")]),t._v(" "),a("p",[t._v("句子1：[1,1,1,1,0,0,0,0,0]\n句子2：[1,2,0,0,1,1,0,1,1]\n句子3：[0,2,1,1,0,0,1,1,1]")]),t._v(" "),a("p",[t._v("该向量与原来文本中单词出现的顺序没有关系，仅仅是词典中每个单词在文本中出现的频率。")]),t._v(" "),a("p",[a("strong",[t._v("总结一下：")])]),t._v(" "),a("ul",[a("li",[t._v("向量的维度等于"),a("strong",[t._v("词汇表")]),t._v("的大小，"),a("strong",[t._v("一般为所有文档中不重复词的个数")])]),t._v(" "),a("li",[t._v("向量没有体现原文档中词的顺序")]),t._v(" "),a("li",[t._v("文档中每个元素代表了该单词在文档中出现的次数 —— 词频")])]),t._v(" "),a("h3",{attrs:{id:"lda-for-topic-modeling-lda-的话题识别思路"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda-for-topic-modeling-lda-的话题识别思路"}},[t._v("#")]),t._v(" LDA for  topic modeling LDA 的话题识别思路")]),t._v(" "),a("p",[t._v("首先要强调的是，"),a("strong",[t._v("LDA 是无监督生成模型")]),t._v("，而不是有监督的判别模型。如果是一般判别模型的思路来解 topic modeling 问题，"),a("strong",[t._v("可能就是给好几篇文档，然后给每个文档打上不同话题的标签，最后做一个分类。")])]),t._v(" "),a("p",[t._v("而 LDA 作为一种无监督生成模型，我们的目标是识别每一篇文档对应的话题，那么自然就要"),a("strong",[t._v("对一篇文档的生成方式进行建模")]),t._v("。")]),t._v(" "),a("p",[t._v("LDA 的话题分类思路是，以话题为指标对文档进行无监督聚类。通过无监督聚类解决分类问题的思路是：")]),t._v(" "),a("ol",[a("li",[t._v("首先指定簇个数"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"K"}})],1)],1)],1),t._v("为超参数")],1),t._v(" "),a("li",[t._v("让模型在训练集上聚类，将样本点聚集成"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"K"}})],1)],1)],1),t._v("簇")],1),t._v(" "),a("li",[t._v("训练结束后，人为给每一个簇打上标签"),a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/66be86c07518b7ce2c5a16163c627e49.svg",alt:"img"}})]),t._v(" "),a("li",[t._v("在模型部署或测试时，模型预测输入样本的类别，就是它被模型分派到的簇上对应的类别")])]),t._v(" "),a("p",[t._v("举例：")]),t._v(" "),a("p",[t._v("而 LDA 识别文档主体的思路就是一种无监督聚类的方法，如下图所示")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713223352537.png"}}),t._v(" "),a("ol",[a("li",[t._v("首先我们指定模型的超参数，即要聚类的话题的个数K=3")]),t._v(" "),a("li",[t._v("随后在训练集中对以上文档依据话题进行聚类")]),t._v(" "),a("li",[t._v("训练完成后，人为的观察每一个簇并给每一个指定一个具体的 topic，也就是上图中的 Science, Politics 以及 Sports。")]),t._v(" "),a("li",[t._v("最后在模型测试过程中，可以将模型用于话题分类任务")])]),t._v(" "),a("blockquote",[a("p",[t._v("这样的好处是，不需要人为给数据集打标签（指的是对每篇文档逐一打标签），灵活性很高，而且"),a("strong",[t._v("模型也许能发掘一些人们想不到的 topic")])]),t._v(" "),a("p",[t._v("坏处是仍然要人干预，每个人指定的 topic 可能都不相同；而且当话题个数增大时，人力成本也会显著提高")])]),t._v(" "),a("ul",[a("li",[a("p",[t._v("由于 "),a("strong",[t._v("LDA 是概率模型")]),t._v("，所以在上图的三角中，文档越靠近顶点，说明模型越确信它输入那一类 topic，也就是概率越大；反之越远离三角形顶点，模型认为它属于单一 topic 的概率越小")])]),t._v(" "),a("li",[a("ul",[a("li",[t._v("上面的三角形实际上是"),a("a",{attrs:{href:"https://www.yuque.com/herormlihaotian/pugkgw/bc3i4b",target:"_blank",rel:"noopener noreferrer"}},[t._v("狄利克雷分布（dirichlet distribution）"),a("OutboundLink")],1),t._v("的图")])])]),t._v(" "),a("li",[a("p",[t._v("如何观察每一个簇并给每一个指定一个具体的 topic？是观察每个topic 下的文档？")])]),t._v(" "),a("li",[a("ul",[a("li",[t._v("如果通过观察每一个 topic 簇下的文档来判断这个 topic 具体是关于什么的，工作量未免有点大")]),t._v(" "),a("li",[t._v("事实上，因为我们"),a("strong",[t._v("将文档建模成了词的集合")]),t._v("，所以指定文档的话题就变成了识别一组单词的话题。所以 LDA 最后会返回"),a("strong",[t._v("每个话题下最经常出现的词汇都有哪些")]),t._v("，"),a("strong",[t._v("这样人们可以通过判断这些词共同描述了那一件类似的事情，进而给 topic 打标签")])])])])]),t._v(" "),a("h3",{attrs:{id:"lda文本生成过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda文本生成过程"}},[t._v("#")]),t._v(" LDA文本生成过程")]),t._v(" "),a("p",[t._v("为了生成一篇文档，并且能够结合 topic modeling 的问题假设，LDA 引入了 "),a("strong",[t._v("topic 这个隐变量")]),t._v("来辅助生成文档。")]),t._v(" "),a("p",[t._v("LDA 认为，"),a("strong",[t._v("话题是由单词（的分布）定义的")]),t._v("，而"),a("strong",[t._v("不同话题 topic 之间的差异在于，每个话题下单词出现的频率是不一样的")]),t._v("。如下图所示")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714100423889.png",alt:"image-20230714100423889"}})]),t._v(" "),a("p",[t._v("可以看出，因为蓝色 topic 下“Galaxy”“Planet”出现的概率较高，所以将其打成“Science”的标签，其他话题同理。这样反过来讲，"),a("strong",[t._v("每个单词也会有自己的主题，它属于不同主题的概率是不同的")]),t._v("。")]),t._v(" "),a("p",[t._v("基于上面两点观察，我们引出 LDA 的"),a("strong",[t._v("两个重要假设")]),t._v("：")]),t._v(" "),a("ol",[a("li",[t._v("每个单词（word）也最好只属于一个主题（topic）—— 文档"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("中每个单词"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/c237419c2f00f32dfd25dd4571132c6c.svg",alt:"img"}}),t._v("都有自己的主题")]),t._v(" "),a("li",[t._v("每篇文档（document）最好只有一个主题（topic）—— 文档"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("的主题就是其下所有单词所属最多的主题")])]),t._v(" "),a("p",[a("strong",[t._v("结论")])]),t._v(" "),a("p",[t._v("由于 LDA 采用的是词袋模型，文档 documents 被抽象成了词汇的序列。所以在 LDA 看来生成一篇文档"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v(",本质上就是如何生成一个具体的词汇"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1)],1),t._v(" "),a("p",[t._v("的问题，把这个过程重复"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"N"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v("遍，就可以得到一篇文档"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v("，把生成文档的过程重复M遍，就得到了整个数据集"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"D"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-TeXAtom",{attrs:{space:"4"}},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"22EF"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"."}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"."}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"M"}})],1)],1)],1)],1)],1)],1)],1),t._v(" "),a("p",[a("strong",[t._v("也就是只需要描述 LDA 是怎么生成一篇文档"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("下的一个具体词汇"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1)],1)]),t._v(" "),a("h3",{attrs:{id:"词汇的生成"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#词汇的生成"}},[t._v("#")]),t._v(" 词汇的生成")]),t._v(" "),a("p",[t._v("首先要明确那句话，LDA 是个概率模型，描述所有问题都是"),a("strong",[t._v("从概率出发的")]),t._v("。换句话说，document 属于哪个 topic 是有概率的，具体一个 word 属于哪个 topic 也是有一定概率的，这些都不是确定的：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("doc - topic 概率")]),t._v("：每篇文档属于哪个主题的概率是不同的")]),t._v(" "),a("li",[a("strong",[t._v("topic - word 概率")]),t._v("：每个 topic 下不同 word 出现的概率是不同的")])]),t._v(" "),a("p",[t._v("首先给定超参数 topic 的数量"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"K"}})],1)],1)],1),t._v(",用"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"Z"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-TeXAtom",{attrs:{space:"4"}},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"22EF"}})],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"22EF"}})],1),a("mjx-msub",{attrs:{space:"2"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"K"}})],1)],1)],1)],1)],1)],1),t._v("表示所有话题集合")],1),t._v(" "),a("p",[t._v("LDA生成一篇文档"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"d"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"m"}})],1)],1)],1)],1)],1),t._v("下一个具体词汇"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("分为两步")],1),t._v(" "),a("ol",[a("li",[t._v("根据"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("的 doc - topic 概率决定（采样)一个主题"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1)],1)],1),t._v(" "),a("li",[t._v("根据"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1)],1),t._v("的的 topic - word 概率决定（采样）得到"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("这个单词")],1)]),t._v(" "),a("p",[t._v("将上述两步重复相应的次数就可以得到整篇文档"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("以及整个数据集"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"D"}})],1)],1)],1)],1),t._v(" "),a("h4",{attrs:{id:"topic-word"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#topic-word"}},[t._v("#")]),t._v(" "),a("strong",[t._v("topic->word")])]),t._v(" "),a("p",[t._v("先来说如果通过一个给定的话题"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1)],1),t._v("决定（采样）一个"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1)],1)],1),t._v("。"),a("strong",[t._v("这里 LDA 采用了 unigram 模型")]),t._v("，那就是")],1),t._v(" "),a("ul",[a("li",[a("p",[t._v("上面我们说 LDA 假设 document 是一个一个 word 构成的序列，这实际上就是 unigram 模型的第一个假设 —— 即先给定文档的数学模型")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("unigram 生成一个单词的时候认为：")])])]),t._v(" "),a("li",[a("ul",[a("li",[t._v("有一个"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg",alt:"img"}}),t._v("个面硬币（或者或是骰子），代表词汇表中的"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg",alt:"img"}}),t._v("个单词，且每个面的朝上的概率不同")]),t._v(" "),a("li",[t._v("当生成第"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/4760e2f007e23d820825ba241c47ce3b.svg",alt:"img"}}),t._v("个文档的第"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/df378375e7693bdcf9535661c023c02e.svg",alt:"img"}}),t._v("个词"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("时，上帝抛出了这个"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg",alt:"img"}}),t._v("个面的硬币，如果第v个面朝上，则"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("就是词汇表中第"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1),t._v("个单词"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1)],1)],1)],1)])]),t._v(" "),a("li"),t._v(" "),a("li"),t._v(" "),a("li",[a("p",[t._v("Unigram 生成一个单词的过程如下图所示：这实际上就是一个"),a("strong",[t._v("从多个单词中依照每个单词出现的概率选一个")]),t._v("的问题")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104011782.png",alt:"image-20230714104011782"}})]),t._v(" "),a("p",[t._v("这个"),a("strong",[t._v("骰子代表的就是一个具体话题 topic")]),t._v(" "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"z"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"k"}})],1)],1)],1)],1)],1),t._v("，更准确地说是 topic - word 概率分布。上面也说过，"),a("strong",[t._v("每个话题下各个单词出现的频率（概率）是不一样的")]),t._v("，所以超参数假设有"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg",alt:"img"}}),t._v("个不同的话题，这里也就会有"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg",alt:"img"}}),t._v("个不同的骰子。")],1),t._v(" "),a("p",[t._v("这"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg",alt:"img"}}),t._v("个骰子每个面取值概率就是 LDA 要学习的参数之一。")]),t._v(" "),a("h4",{attrs:{id:"document-topic"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#document-topic"}},[t._v("#")]),t._v(" "),a("strong",[t._v("document -> topic")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104223105.png",alt:"image-20230714104223105"}})]),t._v(" "),a("h3",{attrs:{id:"文档生成过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#文档生成过程"}},[t._v("#")]),t._v(" 文档生成过程")]),t._v(" "),a("p",[t._v("总结一下 LDA 模型对整个数据集生成的过程")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104313119.png",alt:"image-20230714104313119"}})]),t._v(" "),a("p",[t._v("需要注意的是：")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("所有筛子"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v(","),a("em",[t._v("φ")]),t._v("都是"),a("strong",[t._v("提前生成好")]),t._v("的（需要结合先验来理解)")],1)]),t._v(" "),a("li",[a("p",[t._v("所有文档的生成互相独立的，而一篇文档内的单词出现的概率也是相互独立")])]),t._v(" "),a("li",[a("ul",[a("li",[t._v("所以叫做 unigram 模型，如果考虑一个单词的生成依赖于之前 k 个单词，这就是 k-gram 模型")])])])]),t._v(" "),a("h3",{attrs:{id:"lda的训练"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lda的训练"}},[t._v("#")]),t._v(" LDA的训练")]),t._v(" "),a("blockquote",[a("p",[t._v("Training LDA with Gibbs sampling 用吉布斯采样训练 LDA")])]),t._v(" "),a("p",[t._v("如何学习模型参数训练 LDA?   用吉布斯采样")]),t._v(" "),a("blockquote",[a("p",[t._v("在统计学和机器学习中，我们经常需要从一个复杂的概率分布中获取样本，以便进行推断、估计或模型训练。然而，有时直接从这种概率分布中采样是困难的或不可行的。Gibbs sampling 提供了一种通过对各个变量进行逐一采样的方法，从而实现从联合分布中采样的技术。")]),t._v(" "),a("p",[t._v("Gibbs sampling 基于马尔可夫链的思想。它通过定义一个马尔可夫链，其中每个状态都是变量的一个取值组合。在每一步中，Gibbs sampling 从当前状态中选择一个变量，然后根据该变量的条件分布对它进行采样。在采样过程中，其他变量的取值被保持不变。然后，该变量的新取值被用于更新当前状态，然后继续进行下一步。这个过程会经过多个步骤，直到达到收敛。")])]),t._v(" "),a("p",[t._v("举例:")]),t._v(" "),a("p",[t._v("现在有三个主题，用红绿蓝表示，有四篇文档，每篇文档五个单词如图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714105240210.png",alt:"image-20230714105240210"}})]),t._v(" "),a("p",[t._v("用 Gibbs sampling 的方法确定每个单词的 topic：")]),t._v(" "),a("ol",[a("li",[t._v("首先给所有 word "),a("strong",[t._v("随机赋予一个 topic")]),t._v("，然后"),a("strong",[t._v("一个一个单词挨着来决定 topic")]),t._v("。")]),t._v(" "),a("li",[t._v("在决定每一个单词的 topic 时，都假定其他单词的 topic 都是已经确定好的（就像房间里已经摆好位置的物品）。")]),t._v(" "),a("li",[t._v("给该单词赋予“离它最近”的 topic")])]),t._v(" "),a("p",[t._v("那什么叫离它最近呢？—— 我们可以这样考虑：")]),t._v(" "),a("ul",[a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("这个单词很有可能被赋予该篇文档"),a("img",{attrs:{src:"https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg",alt:"img"}}),t._v("中出现次数最多的 topic")],1),t._v(" "),a("li",[t._v("假设"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("是单词表中第"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1),t._v("个单词"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1)],1)],1),t._v(",因为同性相吸，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"m"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"n"}})],1)],1)],1)],1)],1)],1),t._v("也有可能是"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1)],1)],1),t._v("被赋予次数最多的topic（注意一开始所有 word 的 topic 是随机初始化的，所以不同文档中同一个"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-msub",[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"w"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"v"}})],1)],1)],1)],1)],1),t._v("可能被赋予不同的 topic)")],1)]),t._v(" "),a("p",[t._v("直接来看最终结果")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714105741649.png",alt:"image-20230714105741649"}})])])}),[],!1,null,null,null);a.default=i.exports}}]);