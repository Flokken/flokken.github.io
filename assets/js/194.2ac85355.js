(window.webpackJsonp=window.webpackJsonp||[]).push([[194],{522:function(t,a,s){"use strict";s.r(a);var n=s(4),r=Object(n.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"_1-hadoop框架"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-hadoop框架"}},[t._v("#")]),t._v(" 1 Hadoop框架")]),t._v(" "),a("p",[t._v("Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172205779.png",alt:"image-20230122172205779"}})]),t._v(" "),a("p",[a("strong",[t._v("其中核心技术是MapReduce（分布式计算框架）和HDFS（分布式存储系统）。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172236349.png",alt:"image-20230122172236349"}})]),t._v(" "),a("h3",{attrs:{id:"_1-1-mapreduce"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-mapreduce"}},[t._v("#")]),t._v(" 1.1 MapReduce")]),t._v(" "),a("p",[t._v("Map和Reduce是大规模集群并行计算过程的高度抽象。")]),t._v(" "),a("p",[a("strong",[t._v("核心思想")]),t._v("：")]),t._v(" "),a("p",[t._v("Map: 将数据进行"),a("strong",[t._v("拆分")]),t._v("，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。")]),t._v(" "),a("p",[t._v("Reduce:对数据进行"),a("strong",[t._v("汇总")]),t._v(",即对map阶段的结果进行全局汇总。")]),t._v(" "),a("p",[a("strong",[t._v("抽象模型")]),t._v("：")]),t._v(" "),a("p",[a("strong",[t._v("Map")]),t._v(": 对一组数据元素进行某种重复式的处理；")]),t._v(" "),a("p",[a("strong",[t._v("Reduce:")]),t._v(" 对Map的中间结果进行某种进一步的结果整理。")]),t._v(" "),a("p",[t._v("MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现，可以看到，"),a("strong",[t._v("MapReduce处理的数据类型是<key,value>键值对:")])]),t._v(" "),a("ul",[a("li",[t._v("map: [k1,v1] → [(k2,v2)]")]),t._v(" "),a("li",[t._v("reduce: [k2, {v2,…}] → [k3, v3]")])]),t._v(" "),a("p",[a("strong",[t._v("工作流程")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172246932.png",alt:"image-20230122172246932"}})]),t._v(" "),a("h3",{attrs:{id:"_1-2-hdfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-hdfs"}},[t._v("#")]),t._v(" 1.2 HDFS")]),t._v(" "),a("p",[t._v("设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统（Distributed File System）")]),t._v(" "),a("p",[a("strong",[t._v("设计结构：")])]),t._v(" "),a("p",[t._v("HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件的访问操作；集群中的DataNode管理存储的数据。")]),t._v(" "),a("p",[t._v("HDFS架构图")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172256602.png",alt:"image-20230122172256602"}})]),t._v(" "),a("h3",{attrs:{id:"_1-3-yarn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-yarn"}},[t._v("#")]),t._v(" 1.3 Yarn")]),t._v(" "),a("p",[t._v("yarn（yet another resource negotiator）是一个"),a("strong",[t._v("通用分布式资源管理系统和调度平台")]),t._v("，为上层应用提供统一的资源管理和调度。在集群利用率、资源统一管理和数据共享等方面带来巨大好处。")]),t._v(" "),a("ul",[a("li",[t._v("资源管理系统：集群的硬件资源，如内存、CPU等。")]),t._v(" "),a("li",[t._v("调度平台：多个程序同时申请计算资源如何分配，调度的规则(算法)。")]),t._v(" "),a("li",[t._v("通用：不仅仅支持mapreduce程序，理论上支持各种计算程序。yarn不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。")])]),t._v(" "),a("p",[t._v("可以把yarn理解为—个分布式的操作系统平台，而mapreduce等计算程序则相当于运行于操作系统之上的应用程序**，yarn为这些程序提供运算所需的资源(内存、CPU等)**。hadoop能有今天这个地位，yarn可以说是功不可没。因为有了yarn，更多计算框架可以接入到hdfs中，而不单单是mapreduce，正是因为yarn的包容，使得其他计算框架能专注于计算性能的提升。架构图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172303532.png",alt:"image-20230122172303532"}})]),t._v(" "),a("p",[t._v("YARN 是一个资源管理、任务调度框架，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）、Container")]),t._v(" "),a("ul",[a("li",[t._v("ResourceManager # 负责所有应用程序（整个集群）资源分配与管理以及接收作业的提交；")]),t._v(" "),a("li",[t._v("NodeManager # 负责每一个节点的维护；")]),t._v(" "),a("li",[t._v("ApplicationMaster # 负责每一个应用程序的资源分配（资源二次分配）以及监控所有任务的运行状态；")])]),t._v(" "),a("h2",{attrs:{id:"_2-spark"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-spark"}},[t._v("#")]),t._v(" 2 spark")]),t._v(" "),a("h3",{attrs:{id:"_2-1-spark-相较于hadoop"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-spark-相较于hadoop"}},[t._v("#")]),t._v(" 2.1 spark 相较于Hadoop")]),t._v(" "),a("p",[t._v("以spark2.x为例")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172309679.png",alt:"image-20230122172309679"}})]),t._v(" "),a("table",[a("thead",[a("tr",[a("th"),t._v(" "),a("th",[t._v("Hadoop")]),t._v(" "),a("th",[t._v("Spark")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("场景")]),t._v(" "),a("td",[t._v("大数据数据集的批处理")]),t._v(" "),a("td",[t._v("迭代计算、流计算")])]),t._v(" "),a("tr",[a("td",[t._v("编程范式")]),t._v(" "),a("td",[t._v("Map+Reduce API较低层，适应性差")]),t._v(" "),a("td",[t._v("RDD组成DAG有向无环图，API顶层，方便使用")])]),t._v(" "),a("tr",[a("td",[t._v("存储")]),t._v(" "),a("td",[t._v("中间结果在磁盘，延迟大")]),t._v(" "),a("td",[t._v("RDD结果在内存，延迟小")])]),t._v(" "),a("tr",[a("td",[t._v("运行方式")]),t._v(" "),a("td",[t._v("Task以进程方式维护，启动任务慢")]),t._v(" "),a("td",[t._v("Task以线程方式维护，启动快")])])])]),t._v(" "),a("h4",{attrs:{id:"_2-1-1-原理比较"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-1-原理比较"}},[t._v("#")]),t._v(" 2.1.1 原理比较")]),t._v(" "),a("p",[t._v("Hadoop和"),a("a",{attrs:{href:"https://so.csdn.net/so/search?q=Spark&spm=1001.2101.3001.7020",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark"),a("OutboundLink")],1),t._v("都是并行计算，")]),t._v(" "),a("p",[a("strong",[t._v("Hadoop")]),t._v("一个作业称为一个Job，Job里面分为Map Task和Reduce Task阶段，每个MapTask/ReduceTesk都是"),a("strong",[t._v("进程")]),t._v("级别的！当Task结束时，"),a("strong",[t._v("进程")]),t._v("也会随之结束；\n好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，\n但是问题在于"),a("strong",[t._v("task之间不方便共享数据，执行效率比较低")]),t._v("。比如多个MapTask读取不同数据源文件需要将数据源加载到每个MapTask中，造成重复加载和浪费内存。")]),t._v(" "),a("p",[a("strong",[t._v("Spark")]),t._v("的任务称为application，一个SparkContext对应一个application；\napplication中存在多个job，每触发一次行动算子就会产生一个job；\n每个job中有多个stage，stage是shuffle过程中DAGScheduler通过RDD之间的依赖关系划分job而来的，stage数量=宽依赖（shuffle）数量+1 （默认有一个ResultStage）；\n每个stage里面有多个task，组成taskset，由TaskScheduler分发到各个executor中执行；\nexecutor的生命周期是和application一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。\nSpark基于"),a("strong",[t._v("线程")]),t._v("的方式计算是为了数据共享和提高执行效率！\nSpark采用了"),a("strong",[t._v("线程")]),t._v("的最小的执行单位，但缺点是线程之间会有资源竞争。")]),t._v(" "),a("h4",{attrs:{id:"_2-1-2-应用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-2-应用场景"}},[t._v("#")]),t._v(" 2.1.2 应用场景")]),t._v(" "),a("p",[a("strong",[t._v("Hadoop MapReduce")]),t._v(" 其设计初衷是"),a("strong",[t._v("一次性数据计算")]),t._v("（一个job中 只有一次map和reduce），并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题(使用磁盘交互，进度非常慢)。")]),t._v(" "),a("p",[a("strong",[t._v("Spark")]),t._v(" 应运而生，Spark 就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的"),a("strong",[t._v("RDD")]),t._v(" 计算模型。\n"),a("strong",[t._v("Spark")]),t._v("将job的结果放到了"),a("strong",[t._v("内存")]),t._v("当中，为下一次计算提供了更加便利的处理方式，所以Spark做迭代效率更高。")]),t._v(" "),a("p",[a("strong",[t._v("Hadoop")]),t._v("适合处理静态数据，对于迭代式流式数据的处理能力差；\n"),a("strong",[t._v("Spark")]),t._v("通过在内存中缓存处理的数据，提高了处理流式数据和迭代式数据的性能；")]),t._v(" "),a("h4",{attrs:{id:"_2-1-3-处理速度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-3-处理速度"}},[t._v("#")]),t._v(" 2.1.3 处理速度")]),t._v(" "),a("p",[a("strong",[t._v("Hadoop")]),t._v("是磁盘级计算，"),a("strong",[t._v("计算时需要在磁盘中读取数据")]),t._v("；其采用的是MapReduce的逻辑，把数据进行切片计算用这种方式来处理大量的离线数据.；")]),t._v(" "),a("p",[a("strong",[t._v("Spark")]),t._v("它会在内存中以接近“实时”的时间完成所有的数据分析。Spark的批处理速度比MapReduce快近10倍，【内存中】的数据分析速度则快近100倍。")]),t._v(" "),a("h4",{attrs:{id:"_2-1-4-启动速度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-4-启动速度"}},[t._v("#")]),t._v(" 2.1.4 启动速度")]),t._v(" "),a("p",[t._v("Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。")]),t._v(" "),a("h4",{attrs:{id:"_2-1-5-中间结果存储"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-5-中间结果存储"}},[t._v("#")]),t._v(" 2.1.5 中间结果存储")]),t._v(" "),a("p",[a("strong",[t._v("Hadoop")]),t._v("中 ，中间结果存放在"),a("strong",[t._v("HDFS")]),t._v("中，每次MR都需要刷写-调用\n"),a("strong",[t._v("Spark")]),t._v("中间结果存放优先存放在"),a("strong",[t._v("内存")]),t._v("中，内存不够再存放在磁盘中，不放入HDFS，避免了大量的IO和刷写读取操作；")]),t._v(" "),a("h4",{attrs:{id:"_2-1-6-根本差异"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-6-根本差异"}},[t._v("#")]),t._v(" 2.1.6 根本差异")]),t._v(" "),a("p",[t._v("Spark 和Hadoop 的"),a("strong",[t._v("根本差异")]),t._v("是"),a("strong",[t._v("多个作业之间的数据通信问题")]),t._v(" : Spark 多个作业之间数据通信是基于"),a("strong",[t._v("内存")]),t._v("，而 Hadoop 是基于"),a("strong",[t._v("磁盘")]),t._v("。\nSpark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172316174.png",alt:"image-20230122172316174"}})]),t._v(" "),a("h3",{attrs:{id:"_2-2-概念介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-概念介绍"}},[t._v("#")]),t._v(" 2.2 概念介绍")]),t._v(" "),a("ul",[a("li",[t._v("RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布 式内存的一个抽象概念，提供了一种高度受限的共享内存模型")]),t._v(" "),a("li",[t._v("DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依 赖关系")]),t._v(" "),a("li",[t._v("Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task")]),t._v(" "),a("li",[t._v("应用（Application）：用户编写的Spark应用程序")]),t._v(" "),a("li",[t._v("任务（ Task ）：运行在Executor上的工作单元")]),t._v(" "),a("li",[t._v("作业（ Job ）：一个作业包含多个RDD及作用于相应RDD上的各种操作")]),t._v(" "),a("li",[t._v("阶段（ Stage ）：是作业的基本调度单位，一个作业会分为多组任务，每 组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间 没有Shuffle依赖关系的任务组成的任务集")])]),t._v(" "),a("p",[t._v("Spark任务运行流程图（Spark对象代表对集群的一个连接）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172321304.png",alt:"image-20230122172321304"}})]),t._v(" "),a("p",[t._v("（1）首先为应用构建起基本的 运行环境，即由Driver创建一个 SparkContext，进行资源的申请 、任务的分配和监控")]),t._v(" "),a("p",[t._v("（2）资源管理器为Executor分 配资源，并启动Executor进程")]),t._v(" "),a("p",[t._v("（3）SparkContext根据RDD的 依赖关系构建DAG图，DAG图 提交给DAGScheduler解析成 Stage，然后把一个个TaskSet提 交给底层调度器TaskScheduler 处理；Executor向SparkContext 申请Task，Task Scheduler将 Task发放给Executor运行，并提 供应用程序代码 （4）Task在Executor上运行， 把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完毕后写 入数据并释放所有资源")]),t._v(" "),a("h4",{attrs:{id:"_2-2-1-rdd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-1-rdd"}},[t._v("#")]),t._v(" 2.2.1 RDD")]),t._v(" "),a("p",[t._v("RDD典型的执行过程如下：")]),t._v(" "),a("p",[t._v("（1）创建RDD对象")]),t._v(" "),a("p",[t._v("（2）SparkContext负责计算RDD之间的依赖关系，构建DAG；")]),t._v(" "),a("p",[t._v("（3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个 Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172329803.png",alt:"image-20230122172329803"}})]),t._v(" "),a("p",[t._v("RDD特性")]),t._v(" "),a("p",[t._v("Spark采用RDD以后能够实现高效计算的原因主要在 于：")]),t._v(" "),a("p",[t._v("（1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚 系统、重算过程在不同节点之间并行、只记录粗 粒度的操作")]),t._v(" "),a("p",[t._v("（2）中间结果持久化到内存，数据在内存中的多个 RDD操作之间进行传递，避免了不必要的读写磁盘开销")]),t._v(" "),a("p",[t._v("（3）存放的数据可以是Java对象，避免了不必要的对 象序列化和反序列化")]),t._v(" "),a("p",[t._v("RDD之间的依赖关系")]),t._v(" "),a("p",[a("strong",[t._v("Shuffle")]),t._v("操作")]),t._v(" "),a("p",[t._v("是否包含Shuffle操作是区分窄依赖和宽依赖的根据")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172334327.png",alt:"image-20230122172334327"}})]),t._v(" "),a("p",[a("strong",[t._v("宽依赖与窄依赖")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175248603.png",alt:"image-20230122175248603"}})]),t._v(" "),a("h4",{attrs:{id:"_2-2-2-spark-on-yarn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-2-spark-on-yarn"}},[t._v("#")]),t._v(" 2.2.2 spark on yarn")]),t._v(" "),a("p",[t._v("架构如下："),a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172344429.png",alt:"image-20230122172344429"}})]),t._v(" "),a("h4",{attrs:{id:"_2-2-3-spark-submit"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-3-spark-submit"}},[t._v("#")]),t._v(" 2.2.3 spark submit")]),t._v(" "),a("p",[t._v("spark提交任务可以在本地运行，也可以在集群中运行，这里介绍spark提交任务到yarn集群。")]),t._v(" "),a("p",[a("strong",[t._v("前提")]),t._v("：")]),t._v(" "),a("p",[t._v("假设集群资源：1台master，4个slave，每个机器40个Vcores，总共可用内存290G")]),t._v(" "),a("p",[t._v("举例：")]),t._v(" "),a("p",[t._v("spark-submit --master yarn --deploy-mode cluster --num-executors 30 --driver-memory 4g --executor-memory 9g --executor-cores 4 --conf spark.kryoserializer.buffer.max=512m --conf spark.executor.memoryOverhead=1024 /root/wj/word_count/pre_save_result.py 2>&1 | tee pre_save_result.output.logs")]),t._v(" "),a("p",[t._v("具体提交参数")]),t._v(" "),a("ul",[a("li",[t._v("executor-cores 每个 executor 的最大核数。根据经验实践，设定在 3~6 之间比较合理。 "),a("strong",[t._v("当然具体也要根据集群机器的核心数和内存大小确定")])]),t._v(" "),a("li",[t._v("num-executors 该参数值=每个节点的 executor 数 * work 节点数 每个 node 的 executor 数 = 单节点 yarn 总核数 / 每个 executor 的最大 cpu 核数 考虑到系统基础服务和 HDFS 等组件的余量，yarn.nodemanager.resource.cpu-vcores 配 置为：，参数 executor-cores 的值为：4，那么每个 node 的 executor 数 = 40/4 = 10,集 群节点为5，那么 num-executors = 5* 10 = 50\n"),a("ul",[a("li",[t._v("这里必须注意每个节点如果可用资源不一样，出的executor就不一样")])])]),t._v(" "),a("li",[t._v("executor-memory 该参数值=yarn-nodemanager.resource.memory-mb / 每个节点的 executor 数量 如果 yarn 的参数配置为 100G，那么每个 Executor 大概就是 100G/10≈10G,"),a("strong",[t._v("同时要注意 yarn 配置中每个容器允许的最大内存是否匹配。")])]),t._v(" "),a("li",[t._v("conf spark.executor.memoryOverhead，堆外内存")])]),t._v(" "),a("h4",{attrs:{id:"_2-2-4常见问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-4常见问题"}},[t._v("#")]),t._v(" 2.2.4常见问题")]),t._v(" "),a("h5",{attrs:{id:"堆外内存设置"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#堆外内存设置"}},[t._v("#")]),t._v(" 堆外内存设置")]),t._v(" "),a("p",[t._v("1堆外内存参数 讲到堆外内存，就必须去提一个东西，那就是去 yarn 申请资源的单位，容器。Spark on yarn 模式，一个容器到底申请多少内存资源。 一个容器最多可以申请多大资源，是由 yarn 参数 yarn.scheduler.maximum-allocationmb 决定， 需要满足： spark.executor.memoryOverhead + spark.executor.memory + spark.memory.offHeap.size ≤ yarn.scheduler.maximum-allocation-mb 参数解释：")]),t._v(" "),a("p",[t._v("➢ spark.executor.memory：提交任务时指定的堆内内存。")]),t._v(" "),a("p",[t._v("➢ spark.executor.memoryOverhead：堆外内存参数，内存额外开销。 默认开启，默认值为 spark.executor.memory*0.1 并且会与最小值 384mb 做对比， 取最大值。所以 spark on yarn 任务堆内内存申请 1 个 g，而实际去 yarn 申请的内 存大于 1 个 g 的原因。 ➢ spark.memory.offHeap.size ： 堆 外 内 存 参 数 ， spark 中 默 认 关 闭 ， 需 要 将 spark.memory.enable.offheap.enable 参数设置为 true。 注意：很多网上资料说 spark.executor.memoryOverhead 包含 spark.memory.offHeap.size，")]),t._v(" "),a("h5",{attrs:{id:"故障排除一-控制-reduce-端缓冲大小以避免-oom"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#故障排除一-控制-reduce-端缓冲大小以避免-oom"}},[t._v("#")]),t._v(" "),a("strong",[t._v("故障排除一：控制 reduce 端缓冲大小以避免 OOM")])]),t._v(" "),a("p",[t._v("在 Shuffle 过程，reduce 端 task 并不是等到 map 端 task 将其数据全部写入磁盘后再去 拉取，而是 map 端写一点数据，reduce 端 task 就会拉取一小部分数据，然后立即进行后面 的聚合、算子函数的使用等操作。 reduce 端 task 能够拉取多少数据，由 reduce 拉取数据的缓冲区 buffer 来决定，因为拉 取过来的数据都是先放在 buffer 中，然后再进行后续的处理，buffer 的默认大小为 48MB。 reduce 端 task 会一边拉取一边计算，不一定每次都会拉满 48MB 的数据，可能大多数 时候拉取一部分数据就处理掉了。 虽然说增大 reduce 端缓冲区大小可以减少拉取次数，提升 Shuffle 性能，但是有时 map 端的数据量非常大，写出的速度非常快，此时 reduce 端的所有 task 在拉取的时候，有 可能全部达到自己缓冲的最大极限值，即 48MB，此时，再加上 reduce 端执行的聚合函数 的代码，可能会创建大量的对象，这可难会导致内存溢出，即 OOM。 如果一旦出现 reduce 端内存溢出的问题，我们可以考虑减小 reduce 端拉取数据缓冲 区的大小，例如减少为 12MB。 在实际生产环境中是出现过这种问题的，这是典型的以性能换执行的原理。reduce 端 拉取数据的缓冲区减小，不容易导致 OOM，但是相应的，reudce 端的拉取次数增加，造成 更多的网络传输开销，造成性能的下降。 注意，要保证任务能够运行，再考虑性能的优化。")]),t._v(" "),a("h5",{attrs:{id:"故障排除二-jvm-gc-导致的-shuffle-文件拉取失败"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#故障排除二-jvm-gc-导致的-shuffle-文件拉取失败"}},[t._v("#")]),t._v(" 故障排除二：JVM GC 导致的 shuffle 文件拉取失败")]),t._v(" "),a("p",[t._v("在 Spark 作业中，有时会出现 shuffle file not found 的错误，这是非常常见的一个报错， 有时出现这种错误以后，选择重新执行一遍，就不再报出这种错误。 出现上述问题可能的原因是 Shuffle 操作中，后面 stage 的 task 想要去上一个 stage 的 task 所在的 Executor 拉取数据，结果对方正在执行 GC，执行 GC 会导致 Executor 内所有的 工作现场全部停止，比如 BlockManager、基于 netty 的网络通信等，这就会导致后面的 task 拉取数据拉取了半天都没有拉取到，就会报出 shuffle file not found 的错误，而第二次 再次执行就不会再出现这种错误。")]),t._v(" "),a("p",[t._v("可以通过调整 reduce 端拉取数据重试次数和 reduce 端拉取数据时间间隔这两个参数 来对 Shuffle 性能进行调整，增大参数值，使得 reduce 端拉取数据的重试次数增加，并且 每次失败后等待的时间间隔加长。")]),t._v(" "),a("p",[a("code",[t._v('val conf = new SparkConf() .set("spark.shuffle.io.maxRetries", "60") .set("spark.shuffle.io.retryWait", "60s")')])]),t._v(" "),a("h3",{attrs:{id:"_2-3-rdd编程基础"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-rdd编程基础"}},[t._v("#")]),t._v(" 2.3 RDD编程基础")]),t._v(" "),a("p",[t._v("Spark采用textFile()方法来从文件系统中加载数据创建RDD")]),t._v(" "),a("ul",[a("li",[t._v("该方法把文件的URI作为参数，这个URI可以是：")]),t._v(" "),a("li",[t._v("本地文件系统的地址")]),t._v(" "),a("li",[t._v("或者是分布式文件系统HDFS的地址 •或者是Amazon S3的地址等")])]),t._v(" "),a("p",[a("strong",[t._v("EX：")])]),t._v(" "),a("p",[a("strong",[t._v("读入")]),t._v('：lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")')]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172353867.png",alt:"image-20230122172353867"}})]),t._v(" "),a("h4",{attrs:{id:"_2-3-1-创建rdd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-1-创建rdd"}},[t._v("#")]),t._v(" 2.3.1 创建RDD")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175318394.png",alt:"image-20230122175318394"}})]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",{staticStyle:{"text-align":"center"}},[t._v("操作")]),t._v(" "),a("th",{staticStyle:{"text-align":"center"}},[t._v("含义")])])]),t._v(" "),a("tbody",[a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("filter(func)")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("筛选出满足函数func的元素，并返回一个新的数据 集")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("map(func)")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("将每个元素传递到函数func中，并将结果返回为一 个新的数据集")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("flatMap(func)")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("与map()相似，但每个输入元素都可以映射到0或多 个输出结果")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("groupByKey()")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集")])]),t._v(" "),a("tr",[a("td",{staticStyle:{"text-align":"center"}},[t._v("reduceByKey(func)")]),t._v(" "),a("td",{staticStyle:{"text-align":"center"}},[t._v("应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中每个值是将每个key传递到 函数func中进行聚合后的结果")])])])]),t._v(" "),a("p",[t._v("图示：")]),t._v(" "),a("p",[a("strong",[t._v("Map")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172359791.png",alt:"image-20230122172359791"}})]),t._v(" "),a("p",[a("strong",[t._v("flatMap")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172403966.png",alt:"image-20230122172403966"}})]),t._v(" "),a("p",[a("strong",[t._v("groupByKey")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172408695.png",alt:"image-20230122172408695"}})]),t._v(" "),a("p",[a("strong",[t._v("reduceByKey：")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122174903653.png",alt:"image-20230122174903653"}})]),t._v(" "),a("p",[a("strong",[t._v("综合案例")])]),t._v(" "),a("div",{staticClass:"language-shell line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" lines "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc. "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v(". textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/rdd/word.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" wordCount "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines.flatMap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lambda line:line.split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v(". "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v(". map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lambda word:"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("word,1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v(".reduceByKey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lambda a,b:a+b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" print"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wordCount.collect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("))")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'good'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Spark'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'is'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'better'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hadoop'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fast'")]),t._v(", "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172413103.png",alt:"image-20230122172413103"}})]),t._v(" "),a("h4",{attrs:{id:"_2-3-2-分区"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-2-分区"}},[t._v("#")]),t._v(" 2.3.2 分区")]),t._v(" "),a("p",[a("strong",[t._v("分区原则")]),t._v("：")]),t._v(" "),a("p",[t._v("RDD分区的一个原则是使得分区的个数尽量等于集群中的CPU核心 （core）数目")]),t._v(" "),a("p",[t._v("对于不同的Spark部署模式而言（本地模式、Standalone模式、 YARN模式、Mesos模式），都可以通过设置 spark.default.parallelism这个参数的值，来配置默认的分区数目，一 般而言：")]),t._v(" "),a("p",[t._v("本地模式：默认为本地机器的CPU数目，若设置了local[N],则默认 为N")]),t._v(" "),a("p",[t._v("*Apache Mesos：默认的分区数为8")]),t._v(" "),a("p",[t._v("*Standalone或YARN：在“集群中所有CPU核心数目总和”和“2” 二者中取较大值作为默认值")]),t._v(" "),a("p",[a("strong",[t._v("设置分区个数")]),t._v("：")]),t._v(" "),a("p",[t._v("（1）创建RDD时手动指定分区个数 在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法 格式如下： sc.textFile(path, partitionNum) 其中，path参数用于指定要加载的文件的地址，partitionNum参数用于 指定分区个数。")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallelize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("设置两个分区\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("p",[t._v("（2）使用reparititon方法重新设置分区个数 通过转换操作得到新 RDD 时，直接调用 repartition 方法即可。例如：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallelize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glom"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#显示data这个RDD的分区数量")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repartition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#对data这个RDD进行重新分区")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glom"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("collect"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#显示rdd这个RDD的分区数量")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("实例：")]),t._v(" "),a("p",[t._v("根据key值的最后一位数字，写到不同的文件 例如： 10写入到part-00000 11写入到part-00001 . . . 19写入到part-00009")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkConf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SparkContext\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("MyPartitioner")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MyPartitioner is running"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The key is %d'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" key"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"The main function is running"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkConf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"local"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MyApp"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n sc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallelize"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("partitionBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("MyPartitioner"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("saveAsTextFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/rdd/partitioner"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n main"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br")])]),a("h4",{attrs:{id:"_2-3-3-数据读写"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-3-数据读写"}},[t._v("#")]),t._v(" 2.3.3 数据读写")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("从本地文件读入")]),t._v(" "),a("ul",[a("li",[a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/rdd/word.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hadoop is good'")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("读入\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/rdd/word.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("保存\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" saveAsTextFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/rdd/writeback"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])])])])]),t._v(" "),a("li",[a("p",[t._v("从分布式文件系统读写（文本文件格式）")]),t._v(" "),a("ul",[a("li",[a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs://localhost:9000/user/hadoop/word.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("first"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("saveAsTextFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"writeback"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])])])])]),t._v(" "),a("li",[a("p",[t._v("从HBase读入")]),t._v(" "),a("ul",[a("li",[t._v("HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、 列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多 个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义 列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行 数据类型转换")]),t._v(" "),a("li",[t._v("表：HBase采用表来组织数据，表由行 和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每 个行由行键（row key）来标识。")]),t._v(" "),a("li",[t._v("列族：一个HBase表被分组成许多“列 族”（Column Family）的集合，它是 基本的访问控制单元")]),t._v(" "),a("li",[t._v("列限定符：列族里的数据通过列限定符 （或列）来定位")]),t._v(" "),a("li",[t._v("单元格：在HBase表中，通过行、列族 和列限定符确定一个“单元格”（cell ），单元格中存储的数据没有数据类型 ，总被视为字节数组byte[]")]),t._v(" "),a("li",[t._v("时间戳：每个单元格都保存着同一份数 据的多个版本，这些版本采用时间戳进行索引")]),t._v(" "),a("li",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172418980.png",alt:"image-20230122172418980"}})])])])]),t._v(" "),a("h3",{attrs:{id:"_2-4-spark-sql"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-spark-sql"}},[t._v("#")]),t._v(" 2.4 Spark SQL")]),t._v(" "),a("h4",{attrs:{id:"_2-4-1-shark-到spark-sql"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-1-shark-到spark-sql"}},[t._v("#")]),t._v(" 2.4.1 shark 到spark SQL")]),t._v(" "),a("p",[t._v("​\tShark即Hive on Spark，为了实现与Hive兼容，Shark在 HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、 执行计划优化等逻辑，可以近似认为仅将物理执行计划从 MapReduce作业替换成了Spark作业，通过Hive的HiveQL解 析，把HiveQL翻译成Spark上的RDD操作")]),t._v(" "),a("p",[t._v("​\t2014年6月1日Shark项目和Spark SQL项目的主持人Reynold Xin宣布： 停止对Shark的开发，团队将所有资源放在Spark SQL项目上，至此， Shark的发展画上了句号，但也因此发展出两个分支：Spark SQL和 Hive on Spar")]),t._v(" "),a("ul",[a("li",[t._v("Spark SQL作为Spark生态的一 员继续发展，而不再受限于Hive， 只是兼容Hive")]),t._v(" "),a("li",[t._v("Hive on Spark是一个Hive的发 展计划，该计划将Spark作为 Hive的底层引擎之一，也就是说， Hive将不再受限于一个引擎，可 以采用Map-Reduce、Tez、 Spark等引擎")])]),t._v(" "),a("h4",{attrs:{id:"_2-4-2-dataframe"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-2-dataframe"}},[t._v("#")]),t._v(" 2.4.2 Dataframe")]),t._v(" "),a("p",[t._v("DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，不仅比原 有的RDD转化方式更加简单易用，而且获得了更高的计算性能")]),t._v(" "),a("p",[t._v("Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询")]),t._v(" "),a("p",[a("strong",[t._v("RDD与Dataframe比较")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172427794.png",alt:"image-20230122172427794"}})]),t._v(" "),a("ul",[a("li",[t._v("RDD是分布式的 Java对象的集合，但是，对象内部结构对于RDD而言却 是不可知的")]),t._v(" "),a("li",[t._v("DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息")])]),t._v(" "),a("p",[t._v("从Spark2.0以上版本开始，Spark使用全新的SparkSession接口替代 Spark1.6中的SQLContext及HiveContext接口来实现其对数据加载、转 换、处理等功能。SparkSession实现了SQLContext及HiveContext所有 功能")]),t._v(" "),a("p",[t._v("SparkSession支持从不同的数据源加载数据，并把数据转换成 DataFrame，并且支持把DataFrame转换成SQLContext自身中的表， 然后使用SQL语句来操作数据。")]),t._v(" "),a("p",[t._v("SparkSession亦提供了HiveQL以及其 他依赖于Hive的功能的支持 可以通过如下语句创建一个SparkSession对象：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("SparkConf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession\nspark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkConf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("创建Dataframe")]),t._v(" "),a("p",[t._v("在创建DataFrame时，可以使用spark.read操作，从不同类型的文 件中加载数据创建DataFrame，例如：")]),t._v(" "),a("p",[t._v('•spark.read.format("text").load("people.txt")：读取文本文件 people.json创建DataFrame； •spark.read.format("json").load("people.json")：读取JSON文件 people.json创建DataFrame； •spark.read.format("parquet").load("people.parquet")：读取 Parquet文件people.parquet创建DataFrame。')]),t._v(" "),a("p",[a("strong",[t._v("数据")]),t._v("：")]),t._v(" "),a("p",[t._v("假设有数据在“/usr/local/spark/examples/src/main/resources/”这个目录下，这个目录下有 两个样例数据people.json和people.txt。 people.json文件的内容如下：")]),t._v(" "),a("p",[a("code",[t._v('{"name":"Michael"}')])]),t._v(" "),a("p",[a("code",[t._v('{"name":"Andy", "age":30}')])]),t._v(" "),a("p",[a("code",[t._v('{"name":"Justin", "age":19}')])]),t._v(" "),a("p",[t._v("people.txt文件的内容如下：")]),t._v(" "),a("p",[a("code",[t._v("Michael, 29")])]),t._v(" "),a("p",[a("code",[t._v("Andy, 30")])]),t._v(" "),a("p",[a("code",[t._v("Justin, 19")])]),t._v(" "),a("p",[t._v("保存Dataframe")]),t._v(" "),a("p",[t._v("下面从示例文件people.json中创建一个DataFrame，名称为 peopleDF，把peopleDF保存到另外一个JSON文件中，然后，再从 peopleDF中选取一个列（即name列），把该列数据保存到一个文本 文件中")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" peopleDF "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"json"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/examples/src/main/resources/people.json"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" peopleDF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"age"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"json"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" save"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/sparksql/newpeople.json"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" peopleDF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" save"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/mycode/sparksql/newpeople.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[t._v("会新生成一个名称为newpeople.json的目录（不是文件）和一个名称 为newpeople.txt的目录（不是文件） part-00000-3db90180-ec7c-4291-ad05-df8e45c77f4d.json _SUCCESS")]),t._v(" "),a("p",[t._v("常用函数：")]),t._v(" "),a("p",[t._v("printSchema()，select()，filter()，groupBy等等")]),t._v(" "),a("h4",{attrs:{id:"_2-4-3-rdd转换为dataframe"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-3-rdd转换为dataframe"}},[t._v("#")]),t._v(" 2.4.3 RDD转换为Dataframe")]),t._v(" "),a("p",[t._v("利用反射机制推断RDD模式（schema）")]),t._v(" "),a("p",[t._v("示例：")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Row\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" people "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/examples/src/main/resources/people.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" age"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" schemaPeople "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("people"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#必须注册为临时表才能供下面的查询使用")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" schemaPeople"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createOrReplaceTempView"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"people"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" personsDF "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"select name,age from people where age > 20"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#DataFrame中的每个元素都是一行记录，包含name和age两个字段，分别用")]),t._v("\np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name和p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("age来获取值\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" personsRDD"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("personsDF"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Name: "')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v('"Age'),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v('\n"'),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" personsRDD"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreach"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Michael"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("29")]),t._v("\nName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Andy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br")])]),a("p",[t._v("自动推断过程：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172434466.png",alt:"image-20230122172434466"}})]),t._v(" "),a("p",[a("strong",[t._v("使用编程方式定义RDD模式")])]),t._v(" "),a("p",[t._v("当无法提前获知数据结构时，就需要采用编程方式定义RDD模式。 比如，现在需要通过编程方式把people.txt加载进来生成 DataFrame，并完成SQL查询。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172438728.png",alt:"image-20230122172438728"}})]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("types "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Row\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#下面生成“表头”")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" schemaString "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name age"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" fields "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("StructField"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("field_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" StringType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" field_name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v("\nschemaString"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" schema "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StructType"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fields"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#下面生成“表中的记录”")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" lines "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" textFile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:///usr/local/spark/examples/src/main/resources/people.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" parts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('","')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" people "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Row"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" p"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#下面把“表头”和“表中的记录”拼装在一起")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" schemaPeople "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDataFrame"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("people"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" schema"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br")])]),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172444117.png",alt:"image-20230122172444117"}})]),t._v(" "),a("h2",{attrs:{id:"_3-spark-streaming"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-spark-streaming"}},[t._v("#")]),t._v(" 3 Spark Streaming")]),t._v(" "),a("h3",{attrs:{id:"_3-1-静态数据和流数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-静态数据和流数据"}},[t._v("#")]),t._v(" 3.1 静态数据和流数据")]),t._v(" "),a("p",[a("strong",[t._v("静态数据")]),t._v("：")]),t._v(" "),a("p",[t._v("很多企业为了支持决策分析而构建的数据仓库系统，其中 存放的大量历史数据就是静态数据。技术人员可以利用数 据挖掘和OLAP（On-Line Analytical Processing）分析工 具从静态数据中找到对企业有价值的信息")]),t._v(" "),a("p",[a("strong",[t._v("流数据")]),t._v("：")]),t._v(" "),a("p",[t._v("近年来，在Web应用、网络监控、传感监测等领域，兴 起了一种新的数据密集型应用——流数据，即数据以大量 、快速、时变的流形式持续到达 。实例：PM2.5检测、电子商务网站用户点击流")]),t._v(" "),a("p",[t._v("静态数据和流数据 流数据具有如下特征：")]),t._v(" "),a("ul",[a("li",[t._v("数据快速持续到达，潜在大小也许是无穷无尽的")]),t._v(" "),a("li",[t._v("数据来源众多，格式复杂")]),t._v(" "),a("li",[t._v("数据量大，但是不十分关注存储，一旦经过处理，要 么被丢弃，要么被归档存储")]),t._v(" "),a("li",[t._v("注重数据的整体价值，不过分关注个别数据")]),t._v(" "),a("li",[t._v("数据顺序颠倒，或者不完整，系统无法控制将要处理 的新到达的数据元素的顺序")])]),t._v(" "),a("p",[t._v("对静态数据和流数据的处理，对应着两种截然不同的计算模式："),a("strong",[t._v("批量 计算和实时计算")])]),t._v(" "),a("p",[a("strong",[t._v("批量计算")])]),t._v(" "),a("p",[t._v("充裕时间处理静态数据， 如Hadoop")]),t._v(" "),a("p",[a("strong",[t._v("实时计算:")])]),t._v(" "),a("p",[t._v("**流数据不适合采用批量计算，**因为流数据不适合用传统的关系模型建模")]),t._v(" "),a("p",[t._v("流数据必须采用实时计算，响应时间为秒级")]),t._v(" "),a("p",[t._v("数据量少时，不是问题，但是，在大数据时代，数据格式复杂、来源众多、 数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计 算——流计算，应运而生")]),t._v(" "),a("p",[a("strong",[t._v("流计算")]),t._v("：")]),t._v(" "),a("p",[a("strong",[t._v("针对流数据的实时计算")]),t._v("。"),a("strong",[t._v("实时获取来自不同数据源的海量数据")]),t._v("，经过实时 分析处理，获得有价值的信息")]),t._v(" "),a("p",[t._v("流计算秉承一个基本理念，"),a("strong",[t._v("即数据的价值随着时间的流逝 而降低")]),t._v("，如用户点击流。因此，当事件出现时就应该立即 进行处理，而不是缓存起来进行批量处理。为了及时处理 流数据，就需要一个低延迟、可扩展、高可靠的处理引擎")]),t._v(" "),a("p",[t._v("特点：")]),t._v(" "),a("p",[t._v("对于一个流计算系统来说，它应达到如下需求：")]),t._v(" "),a("ul",[a("li",[t._v("高性能：处理大数据的基本要求，如每秒处理几十万 条数据")]),t._v(" "),a("li",[t._v("海量式：支持TB级甚至是PB级的数据规模")]),t._v(" "),a("li",[t._v("实时性：保证较低的延迟时间，达到秒级别，甚至是 毫秒级别")]),t._v(" "),a("li",[t._v("分布式：支持大数据的基本架构，必须能够平滑扩展")]),t._v(" "),a("li",[t._v("易用性：能够快速进行开发和部署")]),t._v(" "),a("li",[t._v("可靠性：能可靠地处理流数据")])]),t._v(" "),a("p",[t._v("举例：")]),t._v(" "),a("p",[t._v("较为常见的是开源流计算框架，代表如下：")]),t._v(" "),a("p",[t._v("– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高 效、可靠地处理大量的流数据")]),t._v(" "),a("p",[t._v("– Yahoo! S4（Simple Scalable Streaming System）：开源流计算 平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的 流式系统")]),t._v(" "),a("h3",{attrs:{id:"_3-2-流计算处理过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-流计算处理过程"}},[t._v("#")]),t._v(" 3.2 流计算处理过程")]),t._v(" "),a("p",[t._v("流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算 、实时查询服务"),a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114172510455.png",alt:"image-20230114172510455"}})]),t._v(" "),a("h3",{attrs:{id:"_3-3-spark-streaming简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-spark-streaming简介"}},[t._v("#")]),t._v(" 3.3 Spark Streaming简介")]),t._v(" "),a("p",[t._v("Spark Streaming的基本原理是将实时输入数据流以时 间片（秒级）为单位进行拆分，然后经Spark引擎以类 似批处理的方式处理每个时间片数据"),a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173002042.png",alt:"image-20230114173002042"}})]),t._v(" "),a("p",[t._v("Spark Streaming最主要的抽象是"),a("strong",[t._v("DStream（Discretized Stream，离散化数据 流）")]),t._v("，表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按 照时间片（如1秒）分成一段一段**，每一段数据转换为Spark中的RDD，这些分 段就是Dstream，**并且对DStream的操作都最终转变为对相应的RDD的操作")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173047816.png",alt:"image-20230114173047816"}})]),t._v(" "),a("p",[t._v("Spark Streaming和Storm最大的区别在于，"),a("strong",[t._v("Spark Streaming无法实现毫秒级的流计算，而Storm可以实 现毫秒级响应")])]),t._v(" "),a("p",[t._v("Spark Streaming采用的小批量处理的方式使得它可 以同时兼容批量和实时数据处理的逻辑和算法，因此， 方便了一些需要历史数据和实时数据联合分析的特定 应用场合")]),t._v(" "),a("p",[a("strong",[t._v("案例")]),t._v("：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173616774.png",alt:"image-20230114173616774"}})]),t._v(" "),a("ul",[a("li",[a("p",[t._v("在Spark Streaming中，会有一个组件Receiver，作为一个长期运 行的task跑在一个Executor上")])]),t._v(" "),a("li",[a("p",[t._v("每个Receiver都会负责一个input DStream（比如从文件中读取数据 的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）")])]),t._v(" "),a("li",[a("p",[t._v("Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173657107.png",alt:"image-20230114173657107"}})])])]),t._v(" "),a("p",[t._v("编写Stream数据基本步骤：")]),t._v(" "),a("ol",[a("li",[t._v("通过创建输入DStream来定义输入源")]),t._v(" "),a("li",[t._v("通过对DStream应用转换操作和输出操作来定义流计算")]),t._v(" "),a("li",[t._v("用streamingContext.start()来开始接收数据和处理流程")]),t._v(" "),a("li",[t._v("通过streamingContext.awaitTermination()方法来等待 处理结束（手动结束或因为错误而结束）")]),t._v(" "),a("li",[t._v("可以通过streamingContext.stop()来手动结束流计算 进程")])]),t._v(" "),a("p",[t._v("EX：")]),t._v(" "),a("p",[t._v("如果要运行一个Spark Streaming程序，就需要首先生成一个 StreamingContext对象，它是Spark Streaming程序的主入口")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SparkConf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StreamingContext\nconf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkConf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TestDStream'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local[2]'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nssc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StreamingContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])]),a("h3",{attrs:{id:"_3-4-输入源"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-输入源"}},[t._v("#")]),t._v(" 3.4 输入源")]),t._v(" "),a("p",[a("strong",[t._v("文件流")])]),t._v(" "),a("ul",[a("li",[a("p",[t._v("在pyspark中创建文件流")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("$ cd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("mycode\n$ mkdir streaming\n$ cd streaming\n$ mkdir logfile\n$ cd logfile\n\n\n\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkContext\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StreamingContext\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" ssc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StreamingContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" lines "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" \\\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v(" textFileStream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'file:///usr/local/spark/mycode/streaming/logfile'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatMap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" wordCounts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" words"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" wordCounts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pprint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" ssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" ssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n上面在pyspark中执行的程序，一旦你输入ssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("以后，程序就开\n始自动进入循环监听状态。\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br")])])]),t._v(" "),a("li",[a("p",[t._v("采用独立应用方式新建")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("$ cd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("mycode\n$ cd streaming\n$ cd logfile\n$ vim FileStreaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" SparkConf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" StreamingContext\nconf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkConf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setAppName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'TestDStream'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nconf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setMaster"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'local[2]'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" conf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nssc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" StreamingContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlines "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("textFileStream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'file:///usr/local/spark/mycode/streaming/logfile'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nwords "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flatMap"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" line"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("' '")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nwordCounts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" words"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduceByKey"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nwordCounts"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pprint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nssc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n$ cd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("mycode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("streaming"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("logfile"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("\n$ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("submit FileStreaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n$ cd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("mycode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("streaming"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("logfile"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("\n$ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("submit FileStreaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br")])])])]),t._v(" "),a("p",[t._v("下面的以后再补")]),t._v(" "),a("p",[a("strong",[t._v("套接字流")])]),t._v(" "),a("p",[a("strong",[t._v("RDD")]),t._v("队列流")]),t._v(" "),a("p",[t._v("高级数据源KafKa")]),t._v(" "),a("h3",{attrs:{id:"_3-5-struct-streaming"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-struct-streaming"}},[t._v("#")]),t._v(" "),a("strong",[t._v("3.5")]),t._v(" "),a("strong",[t._v("Struct Streaming")])]),t._v(" "),a("p",[t._v("Structured Streaming的关键思想是将实时数据流视为一张正 在不断添加数据的表")]),t._v(" "),a("p",[t._v("可以把流计算等同于在一个静态表上的批处理查询，Spark会 在不断添加数据的无界输入表上运行计算，并进行增量查询")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191114903.png",alt:"image-20230114191114903"}})]),t._v(" "),a("p",[t._v("在无界表上对输入的查询将生成结果表，系统每隔一定的周期会 触发对无界表的计算并更新结果表")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191141121.png",alt:"image-20230114191141121"}})]),t._v(" "),a("h4",{attrs:{id:"_3-5-1-两种处理模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-1-两种处理模型"}},[t._v("#")]),t._v(" 3.5.1 两种处理模型")]),t._v(" "),a("p",[a("strong",[t._v("微批处理模型")]),t._v("：")]),t._v(" "),a("p",[t._v("Structured Streaming默认使用微批处理执行模型，这意味着Spark 流计算引擎会定期检查流数据源，并对自上一批次结束后到达的新 数据执行批量查询")]),t._v(" "),a("p",[t._v("数据到达和得到处理并输出结果之间的延时超过100毫秒")]),t._v(" "),a("p",[a("strong",[t._v("持续处理模型")]),t._v("：")]),t._v(" "),a("p",[t._v("Spark从2.3.0版本开始引入了持续处理的试验性功能，可以实现流计 算的毫秒级延迟")]),t._v(" "),a("p",[t._v("在持续处理模式下，Spark不再根据触发器来周期性启动任务，而是 启动一系列的连续读取、处理和写入结果的长时间运行的任务")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191303795.png",alt:"image-20230114191303795"}})]),t._v(" "),a("p",[a("strong",[t._v("Struct Streaming 和Spark Streaming对比")]),t._v("：")]),t._v(" "),a("p",[t._v("Structured Streaming处理的数据跟Spark Streaming一样，也是源 源不断的数据流，区别在于，Spark Streaming采用的数据抽象是 DStream（本质上就是一系列RDD），而Structured Streaming采用 的数据抽象是DataFrame。")]),t._v(" "),a("p",[t._v("Structured Streaming可以使用Spark SQL的DataFrame/Dataset来 处理数据流。虽然Spark SQL也是采用DataFrame作为数据抽象， 但是，Spark SQL只能处理静态的数据，而Structured Streaming可 以处理结构化的数据流。这样，Structured Streaming就将Spark SQL和Spark Streaming二者的特性结合了起来。")]),t._v(" "),a("h4",{attrs:{id:"_3-5-2实例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-2实例"}},[t._v("#")]),t._v(" "),a("strong",[t._v("3.5.2实例")])]),t._v(" "),a("p",[t._v("编写Structured Streaming程序的基本步骤包括：")]),t._v(" "),a("ul",[a("li",[t._v("导入pyspark模块")]),t._v(" "),a("li",[t._v("创建SparkSession对象")]),t._v(" "),a("li",[t._v("创建输入数据源")]),t._v(" "),a("li",[t._v("定义流计算过程")]),t._v(" "),a("li",[t._v("启动流计算并输出结果")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#实例任务：一个包含很多行英文语句的数据流源源不断")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#到达，Structured Streaming程序对每行英文语句进行")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#拆分，并统计每个单词出现的频率")]),t._v("\n\n一导入数据\n导入PySpark模块，代码如下：\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" SparkSession\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" explode\n由于程序中需要用到拆分字符串和展开数组内的所有单词的功能，\n所以引用了来自pyspark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("functions里面的split和explode函数。\n\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.")]),t._v("步骤"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("：创建SparkSession对象\n创建一个SparkSession对象，代码如下：\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"__main__"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n spark "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SparkSession \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("builder \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("appName"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"StructuredNetworkWordCount"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("getOrCreate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sparkContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setLogLevel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'WARN'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("．步骤"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("：创建输入数据源\n创建一个输入数据源，从“监听在本机（localhost）的"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9999")]),t._v("端口上\n的服务”那里接收文本数据，具体语句如下：\n lines "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("readStream \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"socket"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("option"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"host"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"localhost"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("option"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"port"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9999")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.")]),t._v("步骤"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("：定义流计算过程\n有了输入数据源以后，接着需要定义相关的查询语句，具体如下：\n words "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" lines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n explode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lines"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n wordCounts "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" words"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5.")]),t._v("步骤"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("：启动流计算并输出结果\n定义完查询语句后，下面就可以开始真正执行流计算，具体语句如下：\n query "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" wordCounts \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("writeStream \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("outputMode"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"complete"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"console"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trigger"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("processingTime"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"8 seconds"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \\\n "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n query"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("awaitTermination"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n把代码写入文件StructuredNetworkWordCount"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n在执行StructuredNetworkWordCount"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py之前，需要启动HDFS。\n启动HDFS的命令如下：\n\ncd "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("mycode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("structuredstreaming"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("\n$ "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("usr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("local"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("submit StructuredNetworkWordCount"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\t\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br"),a("span",{staticClass:"line-number"},[t._v("42")]),a("br"),a("span",{staticClass:"line-number"},[t._v("43")]),a("br"),a("span",{staticClass:"line-number"},[t._v("44")]),a("br"),a("span",{staticClass:"line-number"},[t._v("45")]),a("br"),a("span",{staticClass:"line-number"},[t._v("46")]),a("br"),a("span",{staticClass:"line-number"},[t._v("47")]),a("br"),a("span",{staticClass:"line-number"},[t._v("48")]),a("br"),a("span",{staticClass:"line-number"},[t._v("49")]),a("br"),a("span",{staticClass:"line-number"},[t._v("50")]),a("br"),a("span",{staticClass:"line-number"},[t._v("51")]),a("br"),a("span",{staticClass:"line-number"},[t._v("52")]),a("br"),a("span",{staticClass:"line-number"},[t._v("53")]),a("br"),a("span",{staticClass:"line-number"},[t._v("54")]),a("br"),a("span",{staticClass:"line-number"},[t._v("55")]),a("br"),a("span",{staticClass:"line-number"},[t._v("56")]),a("br")])]),a("h2",{attrs:{id:"_4-spark-mlib"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-spark-mlib"}},[t._v("#")]),t._v(" 4 Spark MLib")]),t._v(" "),a("h3",{attrs:{id:"_4-1spark-mlib简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1spark-mlib简介"}},[t._v("#")]),t._v(" 4.1Spark MLib简介")]),t._v(" "),a("p",[t._v("Spark提供了一个基于海量数据的机器学习库，它提供 了常用机器学习算法的分布式实现")]),t._v(" "),a("p",[t._v("​\tpyspark的即席查询也是一个关键。算法工程师可以边 写代码边运行，边看结果")]),t._v(" "),a("ul",[a("li",[t._v("需要注意的是，MLlib中只包含能够在集群上运行良好 的并行算法，这一点很重要")]),t._v(" "),a("li",[t._v("有些经典的机器学习算法没有包含在其中，就是因为它 们不能并行执行")]),t._v(" "),a("li",[t._v("相反地，一些较新的研究得出的算法因为适用于集群， 也被包含在MLlib中，例如分布式随机森林算法、最小交 替二乘算法。这样的选择使得MLlib中的每一个算法都适 用于大规模数据集")])]),t._v(" "),a("p",[t._v("MLlib是Spark的机器学习（Machine Learning）库，旨在 简化机器学习的工程实践工作")]),t._v(" "),a("p",[t._v("•MLlib由一些通用的学习算法和工具组成，包括分类、回 归、聚类、协同过滤、降维等，同时还包括底层的优化原 语和高层的流水线（Pipeline）API，具体如下：")]),t._v(" "),a("ul",[a("li",[t._v("算法工具：常用的学习算法，如分类、回归、聚类和协 同过滤；")]),t._v(" "),a("li",[t._v("特征化工具：特征提取、转化、降维和选择工具；")]),t._v(" "),a("li",[t._v("流水线(Pipeline)：用于构建、评估和调整机器学习工 作流的工具; •持久性：保存和加载算法、模型和管道;")]),t._v(" "),a("li",[t._v("实用工具：线性代数、统计、数据处理等工具。")])]),t._v(" "),a("p",[t._v("支持算法举例：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114194027142.png",alt:"image-20230114194027142"}})])])}),[],!1,null,null,null);a.default=r.exports}}]);