(window.webpackJsonp=window.webpackJsonp||[]).push([[196],{525:function(s,e,a){"use strict";a.r(e);var t=a(4),r=Object(t.a)({},(function(){var s=this,e=s._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("p",[s._v("Scrapy 框架，是python爬虫中封装好的 一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式等等。")]),s._v(" "),e("h2",{attrs:{id:"一-框架介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一-框架介绍"}},[s._v("#")]),s._v(" 一 框架介绍")]),s._v(" "),e("p",[s._v("1、Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。")]),s._v(" "),e("p",[s._v("2、Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。")]),s._v(" "),e("p",[s._v("3、Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，")]),s._v(" "),e("p",[s._v("4、Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，"),e("strong",[s._v("并将需要跟进的URL提交给引擎（使用Request）")]),s._v("，再次进入Scheduler(调度器)，")]),s._v(" "),e("p",[s._v("5、Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.")]),s._v(" "),e("p",[s._v("6、Downloader Middlewares（下载中间件）： 下载器中间件是一个与Scrapy的请求/响应处理挂钩的框架。它是一个轻量级的、低层次的系统，"),e("strong",[s._v("用于全面改变Scrapy的request和response")]),s._v("。例如需要用playwrigth模拟访问时就可以自定义一个中间件，使得requests都用浏览器模拟访问的方式。")]),s._v(" "),e("p",[s._v("7、Spider Middlewares（Spider中间件）：spider中间件，在这里你可以插入自定义功能来处理发送到spider的response，以及处理从spider产生的request和response。")]),s._v(" "),e("p",[s._v("其架构图如下：")]),s._v(" "),e("p",[e("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/1324415-20180514145258553-377347092.png",alt:"img"}})]),s._v(" "),e("h2",{attrs:{id:"二-一个实例"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二-一个实例"}},[s._v("#")]),s._v(" 二 一个实例")]),s._v(" "),e("p",[s._v("首先配置环境，可以直接用pip install scrapy安装。")]),s._v(" "),e("p",[s._v("这里IDE使用pycharm，选择解释器为刚才安装了scrapy的环境。下面开始进行demo项目。")]),s._v(" "),e("p",[s._v("常用的控制台命令（方括号为可选参数）：")]),s._v(" "),e("ul",[e("li",[s._v("scrapy startproject <project_name> [project_dir]  新建一个项目")]),s._v(" "),e("li",[s._v("scrapy genspider [-t template] "),e("name",[e("domain",[s._v(" 生成一个spider.py文件")])],1)],1),s._v(" "),e("li",[s._v("scrapy crawl "),e("spider",[s._v(" 运行某个爬虫（注意name是和类中的名字一样才行）")])],1)]),s._v(" "),e("p",[s._v("首先，"),e("code",[s._v("scrapy startproject demoProject")]),s._v(",会在当前目录下创建一个demoProject。")]),s._v(" "),e("p",[e("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230226095752899.png",alt:"image-20230226095752899"}})]),s._v(" "),e("p",[s._v("其中各个模块作用如下：")]),s._v(" "),e("ul",[e("li",[e("p",[s._v("spiders就是我们书写爬虫的地方，但是无需写请求过程，保存数据等，只需关注解析页面和爬取的地址等等。其中每个爬虫都是一个类，继承自scrapy.Spider")])]),s._v(" "),e("li",[e("p",[s._v("items中可以定义自己需要的数据类型，每一个item也是一个类，继承自scrapy.Item")])]),s._v(" "),e("li",[e("p",[s._v("middlewares,中间件，在我们需要对request和response进行一些操作时使用到。自动创建的项目中也会自动生成"),e("code",[s._v("class ScrapyQianSpiderMiddleware")]),s._v("和"),e("code",[s._v("class ScrapyQianDownloaderMiddleware")]),s._v("。")]),s._v(" "),e("div",{staticClass:"language-markdown line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-markdown"}},[e("code",[s._v("1.spider的yeild将request发送给engine\n2.engine对request不做任何处理发送给scheduler\n3.scheduler，生成request交给engine\n4.engine拿到request，通过middleware发送给downloader\n5.downloader在\\获取到response之后，又经过middleware发送给engine\n6.engine获取到response之后，返回给spider，spider的parse()方法对获取到的response进行处理，解析出items或者requests\n7.将解析出来的items或者requests发送给engine\n8.engine获取到items或者requests，将items发送给ItemPipeline，将requests发送给scheduler（注意：只有调度器中不存在request时，程序才停止，及时请求失败scrapy也会重新进行请求）\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br")])])]),s._v(" "),e("li",[e("p",[s._v("pipelines：当spider返回一个item之后，就会发送到pipeline，我们可以在pipeline中定义一些方法来处理item，常见的用途有去重，保存item等等。")])]),s._v(" "),e("li",[e("p",[s._v("setting：配置文件，比如定义的请求头，开关上面的组件（注意自定义一个组件后，，要在setting中写上才能生效）等等。")])])]),s._v(" "),e("p",[s._v("接下来，在spider里新建一个爬虫，这里使用"),e("code",[s._v("scrapy genspider demoSpider www.baidu.com")]),s._v(","),e("strong",[s._v("注意应当先cd 到spider文件夹下")]),s._v("，会生成demoSpider.py这个文件，下面是自动生成的代码")]),s._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" scrapy\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("DemospiderSpider")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("scrapy"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Spider"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    name "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"demoSpider"')]),s._v("\n    allowed_domains "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"www.baidu.com"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    start_urls "),e("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[s._v('"http://www.baidu.com/"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[s._v("parse")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" response"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("pass")]),s._v("\n")])]),s._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[s._v("1")]),e("br"),e("span",{staticClass:"line-number"},[s._v("2")]),e("br"),e("span",{staticClass:"line-number"},[s._v("3")]),e("br"),e("span",{staticClass:"line-number"},[s._v("4")]),e("br"),e("span",{staticClass:"line-number"},[s._v("5")]),e("br"),e("span",{staticClass:"line-number"},[s._v("6")]),e("br"),e("span",{staticClass:"line-number"},[s._v("7")]),e("br"),e("span",{staticClass:"line-number"},[s._v("8")]),e("br"),e("span",{staticClass:"line-number"},[s._v("9")]),e("br")])]),e("p",[s._v("其中一些参数含义")]),s._v(" "),e("ul",[e("li",[s._v("name是该爬虫的名字，也是scrapy crawl中调用所需名字")]),s._v(" "),e("li",[s._v("allowed_domains是运行访问域名，"),e("strong",[s._v("会自动过滤掉不属于该域名的url")]),s._v("，直接删掉就不会过滤了")]),s._v(" "),e("li",[s._v("start_urls:起始爬取的url")])]),s._v(" "),e("h2",{attrs:{id:"参考文档"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考文档"}},[s._v("#")]),s._v(" 参考文档")]),s._v(" "),e("ul",[e("li",[s._v("https://docs.scrapy.org/en/latest/")])])])}),[],!1,null,null,null);e.default=r.exports}}]);