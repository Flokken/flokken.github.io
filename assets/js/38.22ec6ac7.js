(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{367:function(t,a,s){"use strict";s.r(a);var o=s(4),r=Object(o.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"seq2seq"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#seq2seq"}},[t._v("#")]),t._v(" Seq2seq")]),t._v(" "),a("p",[t._v("之前将self-attention时，可以知道，对于一组输入，可以有等长的输出，也可以由模型定义输出长度。")]),t._v(" "),a("p",[t._v("当输出一个seq，输出model决定的seq，就称为seq2seq")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702112139453.png"}}),t._v(" "),a("p",[t._v("QA问题一般就可以用seq2seq Model解决")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154146586.png"}}),t._v(" "),a("h3",{attrs:{id:"应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),a("p",[t._v("Syntactic Parsing （"),a("strong",[t._v("文法剖析")]),t._v("）")]),t._v(" "),a("p",[t._v("核心就是虽然文法剖析任务是树状的，但是可以利用花括号来表示层次关系，就可以硬是给他转成一个seq，这样就可以放到seq2seq model训练")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154504302.png"}}),t._v(" "),a("blockquote",[a("p",[t._v("https://arxiv.org/abs/1412.7449")])]),t._v(" "),a("p",[t._v("Multi-label Classification（多标签分类）")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154728163.png"}}),t._v(" "),a("blockquote",[a("p",[t._v("https://arxiv.org/abs/1909.03434")]),t._v(" "),a("p",[t._v("https://arxiv.org/abs/1707.05495")])]),t._v(" "),a("p",[t._v("甚至还可以做目标检测等等")]),t._v(" "),a("h3",{attrs:{id:"分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分析"}},[t._v("#")]),t._v(" 分析")]),t._v(" "),a("p",[t._v("首先来看下transformer的架构")]),t._v(" "),a("blockquote",[a("p",[t._v("当然，transformer也属于是seq2seq，下面"),a("strong",[t._v("左边是笼统的看seq2seq（就是Encoder和Decoder）")]),t._v("，右边是transformer具体咋实现左边那两个Encode和Decoder")])]),t._v(" "),a("img",{staticStyle:{zoom:"90%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702155531239.png"}}),t._v(" "),a("h4",{attrs:{id:"encoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),a("p",[a("strong",[t._v("首先Encoder就是为了把输入一排向量，输出一排向量，transformer里用了self-attention")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702160611264.png"}}),t._v(" "),a("p",[t._v("transformer的Encoder比较复杂，大致有self-attention和FC但是更加复杂")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702161309505.png"}}),t._v(" "),a("p",[t._v("更详细的说，对于输入的向量，经过self-attention后得到"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1)],1)],1),t._v("，然后用输入的向量"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"b"}})],1)],1)],1),t._v("+"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:"MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"a"}})],1)],1)],1),t._v("(也就是残差)，然后再进行Layer norm，区别于Batch norm,详细计算过程如图")],1),t._v(" "),a("blockquote",[a("p",[t._v("残差就是求和，把输入的向量加上经过某个层变换后得到的输出")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702161510965.png"}}),t._v(" "),a("p",[t._v("所以回到transformer的Encoder真实实现就是")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702162453632.png",alt:"image-20230702162453632"}})]),t._v(" "),a("h4",{attrs:{id:"decoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#decoder"}},[t._v("#")]),t._v(" Decoder")]),t._v(" "),a("p",[t._v("Decoder一般有两种方法实现,AT和NAT，这里主要说AT")]),t._v(" "),a("p",[a("strong",[t._v("Autogregressive")])]),t._v(" "),a("p",[t._v("首先，Dedcoder当然要读入Encoder的结果，并同时读入一个start，表示句子的开始（独热编码表示）。")]),t._v(" "),a("p",[t._v("然后对于输入，比如机器学习，就经过一个softmax，然后选择其中概率最高的当输出，这里也就是机")]),t._v(" "),a("blockquote",[a("p",[t._v("softmax的性质是会给各个向量对应一个输出，可以理解为概率，因为其总和为1")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163046371.png"}}),t._v(" "),a("p",[t._v("然后循环这样得到输出，需要注意的是，"),a("strong",[t._v("Decoder输出时要依赖上一个自己的输出")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163550918.png"}}),t._v(" "),a("blockquote",[a("p",[t._v("这样其实可能导致一步错，步步错，"),a("strong",[t._v("也即是Error propagation")])])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702164854547.png"}}),t._v(" "),a("h5",{attrs:{id:"decoder框架"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#decoder框架"}},[t._v("#")]),t._v(" Decoder框架")]),t._v(" "),a("p",[a("strong",[t._v("下面是Decoder架构，可以看到除开红色框起来的地方，和Encoder完全一样")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163821063.png"}}),t._v(" "),a("h6",{attrs:{id:"mask-self-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mask-self-attention"}},[t._v("#")]),t._v(" Mask-self-attention")]),t._v(" "),a("p",[t._v("从字面上理解，Masked就是遮住的意思。在这里指的是，我们现在的attention，输出的时候不再是考虑全部，而是只考虑当前生成的向量的左边的向量（包括自己），比如本来我们输出a2时，self-attention考虑a1-a4,现在只考虑其 从他开始左边的，也就是a1,a1，"),a("strong",[t._v("所以像遮起来一部分一样，叫Mask-self-attention")])]),t._v(" "),a("blockquote",[a("p",[t._v("这里跟decoder的输出有关，decoder本来就生成的时候考虑的就是他上一个输出，当然这里就不应该把后面的就给他弄进来")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702164137778.png"}}),t._v(" "),a("h6",{attrs:{id:"stop-token"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stop-token"}},[t._v("#")]),t._v(" Stop Token")]),t._v(" "),a("p",[t._v("上面Deocder生成句子时，我们还有一个问题就是不知道该生成多长的句子，所以此时我们需要加一个END表示结尾，否则会一直生成下去")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165310689.png"}}),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165211275.png"}}),t._v(" "),a("p",[t._v("这样就能在合适的时候停止了")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165358823.png"}}),t._v(" "),a("h6",{attrs:{id:"cross-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cross-attention"}},[t._v("#")]),t._v(" Cross Attention")]),t._v(" "),a("p",[t._v("现在我们来考虑之前用红线框起来的地方，"),a("strong",[t._v("也就是Encoder和Decoder之间如何连接的")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170046345.png",alt:"image-20230702170046345"}}),t._v("\\")]),t._v(" "),a("p",[t._v("可以看到两个来自于Eecoder（左边框起来那两个）一个来自于Decoder（右边那个），")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170235171.png"}}),t._v(" "),a("p",[t._v("当然，对于下一个字“机”，会进行一样的处理")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170451526.png"}}),t._v(" "),a("h3",{attrs:{id:"training"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#training"}},[t._v("#")]),t._v(" Training")]),t._v(" "),a("p",[a("strong",[t._v("训练时采用Teaching -Force")])]),t._v(" "),a("p",[t._v("其实我们发现，比较关键的地方就是Decoder每次预测输出时，预测的是那个，")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171103998.png"}}),t._v(" "),a("p",[t._v("因此，我们训练时，直接把正确答案给Decoder，而不是让他用自己生成那个来生成下一个，"),a("strong",[t._v("以此来减小交叉熵")])]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171313088.png"}}),t._v(" "),a("h4",{attrs:{id:"scheduled-sampling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scheduled-sampling"}},[t._v("#")]),t._v(" Scheduled Sampling")]),t._v(" "),a("p",[t._v("之前提到Error propagation，这里一种办法就是输入的时候，就给他一些把中间错了的字给他，让transformer能适应那些有错误情况的例子")]),t._v(" "),a("img",{staticStyle:{zoom:"70%"},attrs:{src:"https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171640513.png"}})])}),[],!1,null,null,null);a.default=r.exports}}]);