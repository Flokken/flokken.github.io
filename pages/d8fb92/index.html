<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Spark基础 | flokken&#39;s blog</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="记录学过的东西">
    <meta name="keywords" content="JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.bc84ce4b.css" as="style"><link rel="preload" href="/assets/js/app.6c1ebbcf.js" as="script"><link rel="preload" href="/assets/js/2.28dcc766.js" as="script"><link rel="preload" href="/assets/js/194.2ac85355.js" as="script"><link rel="prefetch" href="/assets/js/10.dadef33a.js"><link rel="prefetch" href="/assets/js/100.c21337b1.js"><link rel="prefetch" href="/assets/js/101.eeb225e5.js"><link rel="prefetch" href="/assets/js/102.442361dc.js"><link rel="prefetch" href="/assets/js/103.8495a4a1.js"><link rel="prefetch" href="/assets/js/104.be5e7531.js"><link rel="prefetch" href="/assets/js/105.f968e108.js"><link rel="prefetch" href="/assets/js/106.9c7bcb9d.js"><link rel="prefetch" href="/assets/js/107.5418bfc1.js"><link rel="prefetch" href="/assets/js/108.21c8e4b5.js"><link rel="prefetch" href="/assets/js/109.443388e7.js"><link rel="prefetch" href="/assets/js/11.cb3946f4.js"><link rel="prefetch" href="/assets/js/110.aee2ce54.js"><link rel="prefetch" href="/assets/js/111.c1c4c853.js"><link rel="prefetch" href="/assets/js/112.a77d44c2.js"><link rel="prefetch" href="/assets/js/113.3e1c1563.js"><link rel="prefetch" href="/assets/js/114.b73e18ed.js"><link rel="prefetch" href="/assets/js/115.464712f7.js"><link rel="prefetch" href="/assets/js/116.515a3426.js"><link rel="prefetch" href="/assets/js/117.1de6d86e.js"><link rel="prefetch" href="/assets/js/118.c87898a4.js"><link rel="prefetch" href="/assets/js/119.4d47ac77.js"><link rel="prefetch" href="/assets/js/12.3d11ba1b.js"><link rel="prefetch" href="/assets/js/120.adc822aa.js"><link rel="prefetch" href="/assets/js/121.dea1a90f.js"><link rel="prefetch" href="/assets/js/122.1490ee10.js"><link rel="prefetch" href="/assets/js/123.08cfb23f.js"><link rel="prefetch" href="/assets/js/124.28d0e817.js"><link rel="prefetch" href="/assets/js/125.a7630423.js"><link rel="prefetch" href="/assets/js/126.0d5b60c3.js"><link rel="prefetch" href="/assets/js/127.250521db.js"><link rel="prefetch" href="/assets/js/128.508639a2.js"><link rel="prefetch" href="/assets/js/129.5e6579a1.js"><link rel="prefetch" href="/assets/js/13.74ebc418.js"><link rel="prefetch" href="/assets/js/130.1c66619f.js"><link rel="prefetch" href="/assets/js/131.e83e8060.js"><link rel="prefetch" href="/assets/js/132.7ff3654e.js"><link rel="prefetch" href="/assets/js/133.4125eaa7.js"><link rel="prefetch" href="/assets/js/134.4a100ce2.js"><link rel="prefetch" href="/assets/js/135.789a6a26.js"><link rel="prefetch" href="/assets/js/136.9cc03ec3.js"><link rel="prefetch" href="/assets/js/137.8bd376fe.js"><link rel="prefetch" href="/assets/js/138.532d50a9.js"><link rel="prefetch" href="/assets/js/139.53372ba6.js"><link rel="prefetch" href="/assets/js/14.eaf82e4f.js"><link rel="prefetch" href="/assets/js/140.88405785.js"><link rel="prefetch" href="/assets/js/141.38d215a9.js"><link rel="prefetch" href="/assets/js/142.e0163f88.js"><link rel="prefetch" href="/assets/js/143.b523e1ce.js"><link rel="prefetch" href="/assets/js/144.10873c46.js"><link rel="prefetch" href="/assets/js/145.dfe09936.js"><link rel="prefetch" href="/assets/js/146.b05dc164.js"><link rel="prefetch" href="/assets/js/147.c5b7f960.js"><link rel="prefetch" href="/assets/js/148.80f47df1.js"><link rel="prefetch" href="/assets/js/149.b8d840e3.js"><link rel="prefetch" href="/assets/js/15.e64aec1b.js"><link rel="prefetch" href="/assets/js/150.d4361634.js"><link rel="prefetch" href="/assets/js/151.d8431db2.js"><link rel="prefetch" href="/assets/js/152.47852d30.js"><link rel="prefetch" href="/assets/js/153.47ea2a94.js"><link rel="prefetch" href="/assets/js/154.97384b42.js"><link rel="prefetch" href="/assets/js/155.3026447d.js"><link rel="prefetch" href="/assets/js/156.68e37577.js"><link rel="prefetch" href="/assets/js/157.086f0b3f.js"><link rel="prefetch" href="/assets/js/158.bdcb5b98.js"><link rel="prefetch" href="/assets/js/159.f52cb015.js"><link rel="prefetch" href="/assets/js/16.e312a677.js"><link rel="prefetch" href="/assets/js/160.a2739d99.js"><link rel="prefetch" href="/assets/js/161.7212e5c4.js"><link rel="prefetch" href="/assets/js/162.8006dc05.js"><link rel="prefetch" href="/assets/js/163.bf42549e.js"><link rel="prefetch" href="/assets/js/164.5b2eea28.js"><link rel="prefetch" href="/assets/js/165.d531f59f.js"><link rel="prefetch" href="/assets/js/166.86bf0046.js"><link rel="prefetch" href="/assets/js/167.af7a49eb.js"><link rel="prefetch" href="/assets/js/168.362435ae.js"><link rel="prefetch" href="/assets/js/169.b71a8d5d.js"><link rel="prefetch" href="/assets/js/17.e9813cce.js"><link rel="prefetch" href="/assets/js/170.975564fa.js"><link rel="prefetch" href="/assets/js/171.4949b39a.js"><link rel="prefetch" href="/assets/js/172.8d7529ad.js"><link rel="prefetch" href="/assets/js/173.404d8588.js"><link rel="prefetch" href="/assets/js/174.6641d1c4.js"><link rel="prefetch" href="/assets/js/175.0f46859d.js"><link rel="prefetch" href="/assets/js/176.2fd697ec.js"><link rel="prefetch" href="/assets/js/177.0f744d20.js"><link rel="prefetch" href="/assets/js/178.021de7d2.js"><link rel="prefetch" href="/assets/js/179.593f9493.js"><link rel="prefetch" href="/assets/js/18.0bfadb6c.js"><link rel="prefetch" href="/assets/js/180.8305cce0.js"><link rel="prefetch" href="/assets/js/181.5e83f1b6.js"><link rel="prefetch" href="/assets/js/182.87985f96.js"><link rel="prefetch" href="/assets/js/183.80ba1b3c.js"><link rel="prefetch" href="/assets/js/184.f11d0532.js"><link rel="prefetch" href="/assets/js/185.c859c104.js"><link rel="prefetch" href="/assets/js/186.dc782260.js"><link rel="prefetch" href="/assets/js/187.bc43a42b.js"><link rel="prefetch" href="/assets/js/188.abf4a947.js"><link rel="prefetch" href="/assets/js/189.3911f624.js"><link rel="prefetch" href="/assets/js/19.ebbfaca6.js"><link rel="prefetch" href="/assets/js/190.6ba0f02e.js"><link rel="prefetch" href="/assets/js/191.612eaebf.js"><link rel="prefetch" href="/assets/js/192.7545f46e.js"><link rel="prefetch" href="/assets/js/193.8c879c3b.js"><link rel="prefetch" href="/assets/js/195.887d8d44.js"><link rel="prefetch" href="/assets/js/196.82fc2486.js"><link rel="prefetch" href="/assets/js/197.f3d5a63f.js"><link rel="prefetch" href="/assets/js/198.acb21d94.js"><link rel="prefetch" href="/assets/js/199.72a831c8.js"><link rel="prefetch" href="/assets/js/20.7946efec.js"><link rel="prefetch" href="/assets/js/200.d22fdf55.js"><link rel="prefetch" href="/assets/js/201.d9df6873.js"><link rel="prefetch" href="/assets/js/202.001109bc.js"><link rel="prefetch" href="/assets/js/203.ba860144.js"><link rel="prefetch" href="/assets/js/204.0bb6f98e.js"><link rel="prefetch" href="/assets/js/205.d9c979cd.js"><link rel="prefetch" href="/assets/js/206.1d24147d.js"><link rel="prefetch" href="/assets/js/207.f3d10dab.js"><link rel="prefetch" href="/assets/js/208.c3ba92f2.js"><link rel="prefetch" href="/assets/js/209.fe7a19e2.js"><link rel="prefetch" href="/assets/js/21.0b137891.js"><link rel="prefetch" href="/assets/js/210.5d424205.js"><link rel="prefetch" href="/assets/js/211.d84d326b.js"><link rel="prefetch" href="/assets/js/212.1d38af17.js"><link rel="prefetch" href="/assets/js/213.061f3dbf.js"><link rel="prefetch" href="/assets/js/214.ecb8325b.js"><link rel="prefetch" href="/assets/js/215.615caf62.js"><link rel="prefetch" href="/assets/js/216.e23dc266.js"><link rel="prefetch" href="/assets/js/217.9933d9c4.js"><link rel="prefetch" href="/assets/js/218.5cdf2ac4.js"><link rel="prefetch" href="/assets/js/219.72e58688.js"><link rel="prefetch" href="/assets/js/22.3762b636.js"><link rel="prefetch" href="/assets/js/220.fbc6f972.js"><link rel="prefetch" href="/assets/js/221.1c8eeced.js"><link rel="prefetch" href="/assets/js/222.163dee0a.js"><link rel="prefetch" href="/assets/js/223.72e37b20.js"><link rel="prefetch" href="/assets/js/224.e09609e5.js"><link rel="prefetch" href="/assets/js/225.34fd5c90.js"><link rel="prefetch" href="/assets/js/226.9991814a.js"><link rel="prefetch" href="/assets/js/227.1e2b8a98.js"><link rel="prefetch" href="/assets/js/228.1d4d1fcf.js"><link rel="prefetch" href="/assets/js/229.cc91769e.js"><link rel="prefetch" href="/assets/js/23.f7ebea5b.js"><link rel="prefetch" href="/assets/js/230.3d3edd0f.js"><link rel="prefetch" href="/assets/js/231.0475038b.js"><link rel="prefetch" href="/assets/js/232.a9e3d0d3.js"><link rel="prefetch" href="/assets/js/233.2ca519d3.js"><link rel="prefetch" href="/assets/js/234.c51411ea.js"><link rel="prefetch" href="/assets/js/235.35616a91.js"><link rel="prefetch" href="/assets/js/236.db361c9c.js"><link rel="prefetch" href="/assets/js/237.c8e14a33.js"><link rel="prefetch" href="/assets/js/238.0f2fcf3a.js"><link rel="prefetch" href="/assets/js/239.a33a3324.js"><link rel="prefetch" href="/assets/js/24.42946d79.js"><link rel="prefetch" href="/assets/js/240.7bf17817.js"><link rel="prefetch" href="/assets/js/241.613e82c4.js"><link rel="prefetch" href="/assets/js/242.5709b7c9.js"><link rel="prefetch" href="/assets/js/243.4f2501de.js"><link rel="prefetch" href="/assets/js/244.63720e3b.js"><link rel="prefetch" href="/assets/js/245.e756153f.js"><link rel="prefetch" href="/assets/js/246.b709da2f.js"><link rel="prefetch" href="/assets/js/247.e6f5dc6f.js"><link rel="prefetch" href="/assets/js/248.79fff09e.js"><link rel="prefetch" href="/assets/js/249.2c7ad7a2.js"><link rel="prefetch" href="/assets/js/25.a86a3678.js"><link rel="prefetch" href="/assets/js/250.29112339.js"><link rel="prefetch" href="/assets/js/251.634c259b.js"><link rel="prefetch" href="/assets/js/252.f020ace1.js"><link rel="prefetch" href="/assets/js/253.21843ce0.js"><link rel="prefetch" href="/assets/js/254.819b0d0b.js"><link rel="prefetch" href="/assets/js/255.a1a36af5.js"><link rel="prefetch" href="/assets/js/256.b331fde3.js"><link rel="prefetch" href="/assets/js/257.d496873b.js"><link rel="prefetch" href="/assets/js/258.d25fd3f2.js"><link rel="prefetch" href="/assets/js/259.e26ffb21.js"><link rel="prefetch" href="/assets/js/26.5fe90cb9.js"><link rel="prefetch" href="/assets/js/260.173d7a1c.js"><link rel="prefetch" href="/assets/js/261.fd2846ac.js"><link rel="prefetch" href="/assets/js/262.110b431f.js"><link rel="prefetch" href="/assets/js/263.deab1489.js"><link rel="prefetch" href="/assets/js/264.b38ed189.js"><link rel="prefetch" href="/assets/js/265.e50c8ee7.js"><link rel="prefetch" href="/assets/js/266.132d2f80.js"><link rel="prefetch" href="/assets/js/267.df5c367d.js"><link rel="prefetch" href="/assets/js/268.94eee0dd.js"><link rel="prefetch" href="/assets/js/269.27287f29.js"><link rel="prefetch" href="/assets/js/27.0ac244ef.js"><link rel="prefetch" href="/assets/js/270.82414c58.js"><link rel="prefetch" href="/assets/js/271.88d41d2f.js"><link rel="prefetch" href="/assets/js/272.fdea98e3.js"><link rel="prefetch" href="/assets/js/273.291d7bce.js"><link rel="prefetch" href="/assets/js/274.6470bef1.js"><link rel="prefetch" href="/assets/js/275.ae5aaecb.js"><link rel="prefetch" href="/assets/js/276.6249c409.js"><link rel="prefetch" href="/assets/js/277.c281acfe.js"><link rel="prefetch" href="/assets/js/278.4bf4de94.js"><link rel="prefetch" href="/assets/js/279.62240a5f.js"><link rel="prefetch" href="/assets/js/28.7b66a9da.js"><link rel="prefetch" href="/assets/js/280.a05c4459.js"><link rel="prefetch" href="/assets/js/281.f5dbccd2.js"><link rel="prefetch" href="/assets/js/282.3c8929fa.js"><link rel="prefetch" href="/assets/js/283.8632920a.js"><link rel="prefetch" href="/assets/js/284.74b2e3cc.js"><link rel="prefetch" href="/assets/js/285.1c3bb787.js"><link rel="prefetch" href="/assets/js/286.145be126.js"><link rel="prefetch" href="/assets/js/287.0a623eba.js"><link rel="prefetch" href="/assets/js/288.e07989da.js"><link rel="prefetch" href="/assets/js/289.2f5fdfd9.js"><link rel="prefetch" href="/assets/js/29.ebd9ba6b.js"><link rel="prefetch" href="/assets/js/290.0b12e49c.js"><link rel="prefetch" href="/assets/js/291.fc65a93a.js"><link rel="prefetch" href="/assets/js/292.2beb238d.js"><link rel="prefetch" href="/assets/js/293.87c822f4.js"><link rel="prefetch" href="/assets/js/294.450b6510.js"><link rel="prefetch" href="/assets/js/295.9191ddf7.js"><link rel="prefetch" href="/assets/js/296.92ec965c.js"><link rel="prefetch" href="/assets/js/297.68c01079.js"><link rel="prefetch" href="/assets/js/298.768b54ab.js"><link rel="prefetch" href="/assets/js/299.c2b57741.js"><link rel="prefetch" href="/assets/js/3.cd795b12.js"><link rel="prefetch" href="/assets/js/30.05a622e0.js"><link rel="prefetch" href="/assets/js/300.f409e502.js"><link rel="prefetch" href="/assets/js/301.56e89c78.js"><link rel="prefetch" href="/assets/js/302.942b0d94.js"><link rel="prefetch" href="/assets/js/303.d3946671.js"><link rel="prefetch" href="/assets/js/304.a346c8aa.js"><link rel="prefetch" href="/assets/js/305.46fa308b.js"><link rel="prefetch" href="/assets/js/306.c7f27ad8.js"><link rel="prefetch" href="/assets/js/307.0c8cf8c0.js"><link rel="prefetch" href="/assets/js/308.1d2db23d.js"><link rel="prefetch" href="/assets/js/309.2055654b.js"><link rel="prefetch" href="/assets/js/31.cc1bdb96.js"><link rel="prefetch" href="/assets/js/310.b6b7887a.js"><link rel="prefetch" href="/assets/js/311.e605ceab.js"><link rel="prefetch" href="/assets/js/312.52192bb7.js"><link rel="prefetch" href="/assets/js/313.2698bd8e.js"><link rel="prefetch" href="/assets/js/314.5d9073d0.js"><link rel="prefetch" href="/assets/js/315.c409cde3.js"><link rel="prefetch" href="/assets/js/316.2bf0e6df.js"><link rel="prefetch" href="/assets/js/317.5798e7df.js"><link rel="prefetch" href="/assets/js/318.95986f30.js"><link rel="prefetch" href="/assets/js/319.e6def8d6.js"><link rel="prefetch" href="/assets/js/32.ef0d5dba.js"><link rel="prefetch" href="/assets/js/320.8b3e8e94.js"><link rel="prefetch" href="/assets/js/321.f936d520.js"><link rel="prefetch" href="/assets/js/322.cdb3332c.js"><link rel="prefetch" href="/assets/js/323.4e193830.js"><link rel="prefetch" href="/assets/js/324.5da1850e.js"><link rel="prefetch" href="/assets/js/325.ff5ccde4.js"><link rel="prefetch" href="/assets/js/326.160651b1.js"><link rel="prefetch" href="/assets/js/327.db3feae7.js"><link rel="prefetch" href="/assets/js/328.4ebf4ae0.js"><link rel="prefetch" href="/assets/js/329.f71c3883.js"><link rel="prefetch" href="/assets/js/33.3c571bbe.js"><link rel="prefetch" href="/assets/js/330.885ae61c.js"><link rel="prefetch" href="/assets/js/331.b1a938ba.js"><link rel="prefetch" href="/assets/js/332.b142c42b.js"><link rel="prefetch" href="/assets/js/333.b8076f78.js"><link rel="prefetch" href="/assets/js/334.ad22ab75.js"><link rel="prefetch" href="/assets/js/335.88564fd2.js"><link rel="prefetch" href="/assets/js/336.39134b8d.js"><link rel="prefetch" href="/assets/js/337.df1ebc2c.js"><link rel="prefetch" href="/assets/js/34.270daa56.js"><link rel="prefetch" href="/assets/js/35.15d989c9.js"><link rel="prefetch" href="/assets/js/36.2e8aeb80.js"><link rel="prefetch" href="/assets/js/37.7b762275.js"><link rel="prefetch" href="/assets/js/38.22ec6ac7.js"><link rel="prefetch" href="/assets/js/39.bdb3430a.js"><link rel="prefetch" href="/assets/js/4.f7fa33d6.js"><link rel="prefetch" href="/assets/js/40.345f3c53.js"><link rel="prefetch" href="/assets/js/41.e93aab58.js"><link rel="prefetch" href="/assets/js/42.e7446174.js"><link rel="prefetch" href="/assets/js/43.3bbfdb12.js"><link rel="prefetch" href="/assets/js/44.4291d737.js"><link rel="prefetch" href="/assets/js/45.82f2cc4f.js"><link rel="prefetch" href="/assets/js/46.342f85bd.js"><link rel="prefetch" href="/assets/js/47.f2f6fe03.js"><link rel="prefetch" href="/assets/js/48.99fd0146.js"><link rel="prefetch" href="/assets/js/49.2634fd40.js"><link rel="prefetch" href="/assets/js/5.ae42b7a0.js"><link rel="prefetch" href="/assets/js/50.691080e8.js"><link rel="prefetch" href="/assets/js/51.292e4b2b.js"><link rel="prefetch" href="/assets/js/52.30b4a61b.js"><link rel="prefetch" href="/assets/js/53.2c6237df.js"><link rel="prefetch" href="/assets/js/54.49456bfa.js"><link rel="prefetch" href="/assets/js/55.4710856c.js"><link rel="prefetch" href="/assets/js/56.0334fc17.js"><link rel="prefetch" href="/assets/js/57.4f70d0ed.js"><link rel="prefetch" href="/assets/js/58.d9a970bf.js"><link rel="prefetch" href="/assets/js/59.7a4b31fd.js"><link rel="prefetch" href="/assets/js/6.d5889eef.js"><link rel="prefetch" href="/assets/js/60.417e05df.js"><link rel="prefetch" href="/assets/js/61.c7980332.js"><link rel="prefetch" href="/assets/js/62.58f3c155.js"><link rel="prefetch" href="/assets/js/63.7229815d.js"><link rel="prefetch" href="/assets/js/64.0d72da6d.js"><link rel="prefetch" href="/assets/js/65.2c623405.js"><link rel="prefetch" href="/assets/js/66.d170f5ad.js"><link rel="prefetch" href="/assets/js/67.01023aac.js"><link rel="prefetch" href="/assets/js/68.018604c5.js"><link rel="prefetch" href="/assets/js/69.abbe4df3.js"><link rel="prefetch" href="/assets/js/7.bf0f2b67.js"><link rel="prefetch" href="/assets/js/70.962b3ba0.js"><link rel="prefetch" href="/assets/js/71.a5606282.js"><link rel="prefetch" href="/assets/js/72.975041a5.js"><link rel="prefetch" href="/assets/js/73.d93206af.js"><link rel="prefetch" href="/assets/js/74.67fe0aca.js"><link rel="prefetch" href="/assets/js/75.a321d6fc.js"><link rel="prefetch" href="/assets/js/76.2a44bacd.js"><link rel="prefetch" href="/assets/js/77.90387be7.js"><link rel="prefetch" href="/assets/js/78.c6455938.js"><link rel="prefetch" href="/assets/js/79.8c3687c2.js"><link rel="prefetch" href="/assets/js/8.0b70db32.js"><link rel="prefetch" href="/assets/js/80.ed6a6259.js"><link rel="prefetch" href="/assets/js/81.9cc4a70c.js"><link rel="prefetch" href="/assets/js/82.4dfd22c3.js"><link rel="prefetch" href="/assets/js/83.bbe5ca26.js"><link rel="prefetch" href="/assets/js/84.0141f56f.js"><link rel="prefetch" href="/assets/js/85.26069038.js"><link rel="prefetch" href="/assets/js/86.c22416bd.js"><link rel="prefetch" href="/assets/js/87.ad2b839e.js"><link rel="prefetch" href="/assets/js/88.de3f91d8.js"><link rel="prefetch" href="/assets/js/89.abb8ff44.js"><link rel="prefetch" href="/assets/js/9.5282ef41.js"><link rel="prefetch" href="/assets/js/90.8dab802c.js"><link rel="prefetch" href="/assets/js/91.67430952.js"><link rel="prefetch" href="/assets/js/92.c95aaeec.js"><link rel="prefetch" href="/assets/js/93.d68e1554.js"><link rel="prefetch" href="/assets/js/94.f9fd1744.js"><link rel="prefetch" href="/assets/js/95.a4fd9b78.js"><link rel="prefetch" href="/assets/js/96.32d95188.js"><link rel="prefetch" href="/assets/js/97.3bf42546.js"><link rel="prefetch" href="/assets/js/98.a032b733.js"><link rel="prefetch" href="/assets/js/99.fc7abb0d.js">
    <link rel="stylesheet" href="/assets/css/0.styles.bc84ce4b.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/logo.png" alt="flokken's blog" class="logo"> <span class="site-name can-hide">flokken's blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><!----> <span class="title" style="display:;">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>web开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/javascript/" class="nav-link">JavaScript</a></li><li class="dropdown-subitem"><a href="/vue/" class="nav-link">Vue</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="后端开发" class="dropdown-title"><!----> <span class="title" style="display:;">后端开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/java/" class="nav-link">Java开发</a></li><li class="dropdown-item"><!----> <a href="/go/" class="nav-link">Go开发</a></li><li class="dropdown-item"><!----> <a href="/microservice/" class="nav-link">微服务开发</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/KG/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/DL/" class="nav-link">深度学习</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全与运维" class="dropdown-title"><!----> <span class="title" style="display:;">安全与运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/reverse/" class="nav-link">逆向</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><!----> <span class="title" style="display:;">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/spark/" class="nav-link">Spark</a></li><li class="dropdown-item"><!----> <a href="/spider/" class="nav-link">Spider</a></li><li class="dropdown-item"><!----> <a href="/mysql/" class="nav-link">MySQL</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法与数据结构" class="dropdown-title"><!----> <span class="title" style="display:;">算法与数据结构</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/algorithm/" class="nav-link">算法</a></li><li class="dropdown-item"><!----> <a href="/datastructure/" class="nav-link">数据结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><!----> <span class="title" style="display:;">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/tips/" class="nav-link">小知识</a></li></ul></div></div> <a href="https://github.com/flokken" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/avtar.png"> <div class="blogger-info"><h3>flokken</h3> <span>一个大水货</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><!----> <span class="title" style="display:;">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>web开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/javascript/" class="nav-link">JavaScript</a></li><li class="dropdown-subitem"><a href="/vue/" class="nav-link">Vue</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="后端开发" class="dropdown-title"><!----> <span class="title" style="display:;">后端开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/java/" class="nav-link">Java开发</a></li><li class="dropdown-item"><!----> <a href="/go/" class="nav-link">Go开发</a></li><li class="dropdown-item"><!----> <a href="/microservice/" class="nav-link">微服务开发</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/KG/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/DL/" class="nav-link">深度学习</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全与运维" class="dropdown-title"><!----> <span class="title" style="display:;">安全与运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/reverse/" class="nav-link">逆向</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><!----> <span class="title" style="display:;">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/spark/" class="nav-link">Spark</a></li><li class="dropdown-item"><!----> <a href="/spider/" class="nav-link">Spider</a></li><li class="dropdown-item"><!----> <a href="/mysql/" class="nav-link">MySQL</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法与数据结构" class="dropdown-title"><!----> <span class="title" style="display:;">算法与数据结构</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/algorithm/" class="nav-link">算法</a></li><li class="dropdown-item"><!----> <a href="/datastructure/" class="nav-link">数据结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><!----> <span class="title" style="display:;">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/tips/" class="nav-link">小知识</a></li></ul></div></div> <a href="https://github.com/flokken" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Spark</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/d8fb92/" aria-current="page" class="active sidebar-link">Spark基础</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/d8fb92/#_1-hadoop框架" class="sidebar-link">1 Hadoop框架</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_1-1-mapreduce" class="sidebar-link">1.1 MapReduce</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_1-2-hdfs" class="sidebar-link">1.2 HDFS</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_1-3-yarn" class="sidebar-link">1.3 Yarn</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/d8fb92/#_2-spark" class="sidebar-link">2 spark</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_2-1-spark-相较于hadoop" class="sidebar-link">2.1 spark 相较于Hadoop</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-1-原理比较" class="sidebar-link">2.1.1 原理比较</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-2-应用场景" class="sidebar-link">2.1.2 应用场景</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-3-处理速度" class="sidebar-link">2.1.3 处理速度</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-4-启动速度" class="sidebar-link">2.1.4 启动速度</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-5-中间结果存储" class="sidebar-link">2.1.5 中间结果存储</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-1-6-根本差异" class="sidebar-link">2.1.6 根本差异</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_2-2-概念介绍" class="sidebar-link">2.2 概念介绍</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-2-1-rdd" class="sidebar-link">2.2.1 RDD</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-2-2-spark-on-yarn" class="sidebar-link">2.2.2 spark on yarn</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-2-3-spark-submit" class="sidebar-link">2.2.3 spark submit</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-2-4常见问题" class="sidebar-link">2.2.4常见问题</a></li><li class="sidebar-sub-header level5"><a href="/pages/d8fb92/#堆外内存设置" class="sidebar-link">堆外内存设置</a></li><li class="sidebar-sub-header level5"><a href="/pages/d8fb92/#故障排除一-控制-reduce-端缓冲大小以避免-oom" class="sidebar-link">故障排除一：控制 reduce 端缓冲大小以避免 OOM</a></li><li class="sidebar-sub-header level5"><a href="/pages/d8fb92/#故障排除二-jvm-gc-导致的-shuffle-文件拉取失败" class="sidebar-link">故障排除二：JVM GC 导致的 shuffle 文件拉取失败</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_2-3-rdd编程基础" class="sidebar-link">2.3 RDD编程基础</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-3-1-创建rdd" class="sidebar-link">2.3.1 创建RDD</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-3-2-分区" class="sidebar-link">2.3.2 分区</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-3-3-数据读写" class="sidebar-link">2.3.3 数据读写</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_2-4-spark-sql" class="sidebar-link">2.4 Spark SQL</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-4-1-shark-到spark-sql" class="sidebar-link">2.4.1 shark 到spark SQL</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-4-2-dataframe" class="sidebar-link">2.4.2 Dataframe</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_2-4-3-rdd转换为dataframe" class="sidebar-link">2.4.3 RDD转换为Dataframe</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/d8fb92/#_3-spark-streaming" class="sidebar-link">3 Spark Streaming</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_3-1-静态数据和流数据" class="sidebar-link">3.1 静态数据和流数据</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_3-2-流计算处理过程" class="sidebar-link">3.2 流计算处理过程</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_3-3-spark-streaming简介" class="sidebar-link">3.3 Spark Streaming简介</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_3-4-输入源" class="sidebar-link">3.4 输入源</a></li><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_3-5-struct-streaming" class="sidebar-link">3.5 Struct Streaming</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_3-5-1-两种处理模型" class="sidebar-link">3.5.1 两种处理模型</a></li><li class="sidebar-sub-header level4"><a href="/pages/d8fb92/#_3-5-2实例" class="sidebar-link">3.5.2实例</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/d8fb92/#_4-spark-mlib" class="sidebar-link">4 Spark MLib</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/d8fb92/#_4-1spark-mlib简介" class="sidebar-link">4.1Spark MLib简介</a></li></ul></li></ul></li><li><a href="/pages/09f3e1/" class="sidebar-link">HBase与Hive介绍</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Spider</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数据库</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>深入理解分布式系统</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Mit6.824</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>大数据经典论文导读</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>消息队列</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/categories/?category=%E5%A4%A7%E6%95%B0%E6%8D%AE" title="分类" data-v-06225672>大数据</a></li><li data-v-06225672><a href="/categories/?category=Spark" title="分类" data-v-06225672>Spark</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/Flokken" target="_blank" title="作者" class="beLink" data-v-06225672>flokken</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-01-14</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">Spark基础<!----></h1>  <div class="theme-vdoing-content content__default"><h2 id="_1-hadoop框架"><a href="#_1-hadoop框架" class="header-anchor">#</a> 1 Hadoop框架</h2> <p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172205779.png" alt="image-20230122172205779"></p> <p><strong>其中核心技术是MapReduce（分布式计算框架）和HDFS（分布式存储系统）。</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172236349.png" alt="image-20230122172236349"></p> <h3 id="_1-1-mapreduce"><a href="#_1-1-mapreduce" class="header-anchor">#</a> 1.1 MapReduce</h3> <p>Map和Reduce是大规模集群并行计算过程的高度抽象。</p> <p><strong>核心思想</strong>：</p> <p>Map: 将数据进行<strong>拆分</strong>，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。</p> <p>Reduce:对数据进行<strong>汇总</strong>,即对map阶段的结果进行全局汇总。</p> <p><strong>抽象模型</strong>：</p> <p><strong>Map</strong>: 对一组数据元素进行某种重复式的处理；</p> <p><strong>Reduce:</strong> 对Map的中间结果进行某种进一步的结果整理。</p> <p>MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现，可以看到，<strong>MapReduce处理的数据类型是&lt;key,value&gt;键值对:</strong></p> <ul><li>map: [k1,v1] → [(k2,v2)]</li> <li>reduce: [k2, {v2,…}] → [k3, v3]</li></ul> <p><strong>工作流程</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172246932.png" alt="image-20230122172246932"></p> <h3 id="_1-2-hdfs"><a href="#_1-2-hdfs" class="header-anchor">#</a> 1.2 HDFS</h3> <p>设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统（Distributed File System）</p> <p><strong>设计结构：</strong></p> <p>HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件的访问操作；集群中的DataNode管理存储的数据。</p> <p>HDFS架构图</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172256602.png" alt="image-20230122172256602"></p> <h3 id="_1-3-yarn"><a href="#_1-3-yarn" class="header-anchor">#</a> 1.3 Yarn</h3> <p>yarn（yet another resource negotiator）是一个<strong>通用分布式资源管理系统和调度平台</strong>，为上层应用提供统一的资源管理和调度。在集群利用率、资源统一管理和数据共享等方面带来巨大好处。</p> <ul><li>资源管理系统：集群的硬件资源，如内存、CPU等。</li> <li>调度平台：多个程序同时申请计算资源如何分配，调度的规则(算法)。</li> <li>通用：不仅仅支持mapreduce程序，理论上支持各种计算程序。yarn不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。</li></ul> <p>可以把yarn理解为—个分布式的操作系统平台，而mapreduce等计算程序则相当于运行于操作系统之上的应用程序**，yarn为这些程序提供运算所需的资源(内存、CPU等)**。hadoop能有今天这个地位，yarn可以说是功不可没。因为有了yarn，更多计算框架可以接入到hdfs中，而不单单是mapreduce，正是因为yarn的包容，使得其他计算框架能专注于计算性能的提升。架构图：</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172303532.png" alt="image-20230122172303532"></p> <p>YARN 是一个资源管理、任务调度框架，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）、Container</p> <ul><li>ResourceManager # 负责所有应用程序（整个集群）资源分配与管理以及接收作业的提交；</li> <li>NodeManager # 负责每一个节点的维护；</li> <li>ApplicationMaster # 负责每一个应用程序的资源分配（资源二次分配）以及监控所有任务的运行状态；</li></ul> <h2 id="_2-spark"><a href="#_2-spark" class="header-anchor">#</a> 2 spark</h2> <h3 id="_2-1-spark-相较于hadoop"><a href="#_2-1-spark-相较于hadoop" class="header-anchor">#</a> 2.1 spark 相较于Hadoop</h3> <p>以spark2.x为例</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172309679.png" alt="image-20230122172309679"></p> <table><thead><tr><th></th> <th>Hadoop</th> <th>Spark</th></tr></thead> <tbody><tr><td>场景</td> <td>大数据数据集的批处理</td> <td>迭代计算、流计算</td></tr> <tr><td>编程范式</td> <td>Map+Reduce API较低层，适应性差</td> <td>RDD组成DAG有向无环图，API顶层，方便使用</td></tr> <tr><td>存储</td> <td>中间结果在磁盘，延迟大</td> <td>RDD结果在内存，延迟小</td></tr> <tr><td>运行方式</td> <td>Task以进程方式维护，启动任务慢</td> <td>Task以线程方式维护，启动快</td></tr></tbody></table> <h4 id="_2-1-1-原理比较"><a href="#_2-1-1-原理比较" class="header-anchor">#</a> 2.1.1 原理比较</h4> <p>Hadoop和<a href="https://so.csdn.net/so/search?q=Spark&amp;spm=1001.2101.3001.7020" target="_blank" rel="noopener noreferrer">Spark<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>都是并行计算，</p> <p><strong>Hadoop</strong>一个作业称为一个Job，Job里面分为Map Task和Reduce Task阶段，每个MapTask/ReduceTesk都是<strong>进程</strong>级别的！当Task结束时，<strong>进程</strong>也会随之结束；
好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，
但是问题在于<strong>task之间不方便共享数据，执行效率比较低</strong>。比如多个MapTask读取不同数据源文件需要将数据源加载到每个MapTask中，造成重复加载和浪费内存。</p> <p><strong>Spark</strong>的任务称为application，一个SparkContext对应一个application；
application中存在多个job，每触发一次行动算子就会产生一个job；
每个job中有多个stage，stage是shuffle过程中DAGScheduler通过RDD之间的依赖关系划分job而来的，stage数量=宽依赖（shuffle）数量+1 （默认有一个ResultStage）；
每个stage里面有多个task，组成taskset，由TaskScheduler分发到各个executor中执行；
executor的生命周期是和application一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。
Spark基于<strong>线程</strong>的方式计算是为了数据共享和提高执行效率！
Spark采用了<strong>线程</strong>的最小的执行单位，但缺点是线程之间会有资源竞争。</p> <h4 id="_2-1-2-应用场景"><a href="#_2-1-2-应用场景" class="header-anchor">#</a> 2.1.2 应用场景</h4> <p><strong>Hadoop MapReduce</strong> 其设计初衷是<strong>一次性数据计算</strong>（一个job中 只有一次map和reduce），并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题(使用磁盘交互，进度非常慢)。</p> <p><strong>Spark</strong> 应运而生，Spark 就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的<strong>RDD</strong> 计算模型。
<strong>Spark</strong>将job的结果放到了<strong>内存</strong>当中，为下一次计算提供了更加便利的处理方式，所以Spark做迭代效率更高。</p> <p><strong>Hadoop</strong>适合处理静态数据，对于迭代式流式数据的处理能力差；
<strong>Spark</strong>通过在内存中缓存处理的数据，提高了处理流式数据和迭代式数据的性能；</p> <h4 id="_2-1-3-处理速度"><a href="#_2-1-3-处理速度" class="header-anchor">#</a> 2.1.3 处理速度</h4> <p><strong>Hadoop</strong>是磁盘级计算，<strong>计算时需要在磁盘中读取数据</strong>；其采用的是MapReduce的逻辑，把数据进行切片计算用这种方式来处理大量的离线数据.；</p> <p><strong>Spark</strong>它会在内存中以接近“实时”的时间完成所有的数据分析。Spark的批处理速度比MapReduce快近10倍，【内存中】的数据分析速度则快近100倍。</p> <h4 id="_2-1-4-启动速度"><a href="#_2-1-4-启动速度" class="header-anchor">#</a> 2.1.4 启动速度</h4> <p>Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。</p> <h4 id="_2-1-5-中间结果存储"><a href="#_2-1-5-中间结果存储" class="header-anchor">#</a> 2.1.5 中间结果存储</h4> <p><strong>Hadoop</strong>中 ，中间结果存放在<strong>HDFS</strong>中，每次MR都需要刷写-调用
<strong>Spark</strong>中间结果存放优先存放在<strong>内存</strong>中，内存不够再存放在磁盘中，不放入HDFS，避免了大量的IO和刷写读取操作；</p> <h4 id="_2-1-6-根本差异"><a href="#_2-1-6-根本差异" class="header-anchor">#</a> 2.1.6 根本差异</h4> <p>Spark 和Hadoop 的<strong>根本差异</strong>是<strong>多个作业之间的数据通信问题</strong> : Spark 多个作业之间数据通信是基于<strong>内存</strong>，而 Hadoop 是基于<strong>磁盘</strong>。
Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172316174.png" alt="image-20230122172316174"></p> <h3 id="_2-2-概念介绍"><a href="#_2-2-概念介绍" class="header-anchor">#</a> 2.2 概念介绍</h3> <ul><li>RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布 式内存的一个抽象概念，提供了一种高度受限的共享内存模型</li> <li>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依 赖关系</li> <li>Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task</li> <li>应用（Application）：用户编写的Spark应用程序</li> <li>任务（ Task ）：运行在Executor上的工作单元</li> <li>作业（ Job ）：一个作业包含多个RDD及作用于相应RDD上的各种操作</li> <li>阶段（ Stage ）：是作业的基本调度单位，一个作业会分为多组任务，每 组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间 没有Shuffle依赖关系的任务组成的任务集</li></ul> <p>Spark任务运行流程图（Spark对象代表对集群的一个连接）</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172321304.png" alt="image-20230122172321304"></p> <p>（1）首先为应用构建起基本的 运行环境，即由Driver创建一个 SparkContext，进行资源的申请 、任务的分配和监控</p> <p>（2）资源管理器为Executor分 配资源，并启动Executor进程</p> <p>（3）SparkContext根据RDD的 依赖关系构建DAG图，DAG图 提交给DAGScheduler解析成 Stage，然后把一个个TaskSet提 交给底层调度器TaskScheduler 处理；Executor向SparkContext 申请Task，Task Scheduler将 Task发放给Executor运行，并提 供应用程序代码 （4）Task在Executor上运行， 把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完毕后写 入数据并释放所有资源</p> <h4 id="_2-2-1-rdd"><a href="#_2-2-1-rdd" class="header-anchor">#</a> 2.2.1 RDD</h4> <p>RDD典型的执行过程如下：</p> <p>（1）创建RDD对象</p> <p>（2）SparkContext负责计算RDD之间的依赖关系，构建DAG；</p> <p>（3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个 Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172329803.png" alt="image-20230122172329803"></p> <p>RDD特性</p> <p>Spark采用RDD以后能够实现高效计算的原因主要在 于：</p> <p>（1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚 系统、重算过程在不同节点之间并行、只记录粗 粒度的操作</p> <p>（2）中间结果持久化到内存，数据在内存中的多个 RDD操作之间进行传递，避免了不必要的读写磁盘开销</p> <p>（3）存放的数据可以是Java对象，避免了不必要的对 象序列化和反序列化</p> <p>RDD之间的依赖关系</p> <p><strong>Shuffle</strong>操作</p> <p>是否包含Shuffle操作是区分窄依赖和宽依赖的根据</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172334327.png" alt="image-20230122172334327"></p> <p><strong>宽依赖与窄依赖</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175248603.png" alt="image-20230122175248603"></p> <h4 id="_2-2-2-spark-on-yarn"><a href="#_2-2-2-spark-on-yarn" class="header-anchor">#</a> 2.2.2 spark on yarn</h4> <p>架构如下：<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172344429.png" alt="image-20230122172344429"></p> <h4 id="_2-2-3-spark-submit"><a href="#_2-2-3-spark-submit" class="header-anchor">#</a> 2.2.3 spark submit</h4> <p>spark提交任务可以在本地运行，也可以在集群中运行，这里介绍spark提交任务到yarn集群。</p> <p><strong>前提</strong>：</p> <p>假设集群资源：1台master，4个slave，每个机器40个Vcores，总共可用内存290G</p> <p>举例：</p> <p>spark-submit --master yarn --deploy-mode cluster --num-executors 30 --driver-memory 4g --executor-memory 9g --executor-cores 4 --conf spark.kryoserializer.buffer.max=512m --conf spark.executor.memoryOverhead=1024 /root/wj/word_count/pre_save_result.py 2&gt;&amp;1 | tee pre_save_result.output.logs</p> <p>具体提交参数</p> <ul><li>executor-cores 每个 executor 的最大核数。根据经验实践，设定在 3~6 之间比较合理。 <strong>当然具体也要根据集群机器的核心数和内存大小确定</strong></li> <li>num-executors 该参数值=每个节点的 executor 数 * work 节点数 每个 node 的 executor 数 = 单节点 yarn 总核数 / 每个 executor 的最大 cpu 核数 考虑到系统基础服务和 HDFS 等组件的余量，yarn.nodemanager.resource.cpu-vcores 配 置为：，参数 executor-cores 的值为：4，那么每个 node 的 executor 数 = 40/4 = 10,集 群节点为5，那么 num-executors = 5* 10 = 50
<ul><li>这里必须注意每个节点如果可用资源不一样，出的executor就不一样</li></ul></li> <li>executor-memory 该参数值=yarn-nodemanager.resource.memory-mb / 每个节点的 executor 数量 如果 yarn 的参数配置为 100G，那么每个 Executor 大概就是 100G/10≈10G,<strong>同时要注意 yarn 配置中每个容器允许的最大内存是否匹配。</strong></li> <li>conf spark.executor.memoryOverhead，堆外内存</li></ul> <h4 id="_2-2-4常见问题"><a href="#_2-2-4常见问题" class="header-anchor">#</a> 2.2.4常见问题</h4> <h5 id="堆外内存设置"><a href="#堆外内存设置" class="header-anchor">#</a> 堆外内存设置</h5> <p>1堆外内存参数 讲到堆外内存，就必须去提一个东西，那就是去 yarn 申请资源的单位，容器。Spark on yarn 模式，一个容器到底申请多少内存资源。 一个容器最多可以申请多大资源，是由 yarn 参数 yarn.scheduler.maximum-allocationmb 决定， 需要满足： spark.executor.memoryOverhead + spark.executor.memory + spark.memory.offHeap.size ≤ yarn.scheduler.maximum-allocation-mb 参数解释：</p> <p>➢ spark.executor.memory：提交任务时指定的堆内内存。</p> <p>➢ spark.executor.memoryOverhead：堆外内存参数，内存额外开销。 默认开启，默认值为 spark.executor.memory*0.1 并且会与最小值 384mb 做对比， 取最大值。所以 spark on yarn 任务堆内内存申请 1 个 g，而实际去 yarn 申请的内 存大于 1 个 g 的原因。 ➢ spark.memory.offHeap.size ： 堆 外 内 存 参 数 ， spark 中 默 认 关 闭 ， 需 要 将 spark.memory.enable.offheap.enable 参数设置为 true。 注意：很多网上资料说 spark.executor.memoryOverhead 包含 spark.memory.offHeap.size，</p> <h5 id="故障排除一-控制-reduce-端缓冲大小以避免-oom"><a href="#故障排除一-控制-reduce-端缓冲大小以避免-oom" class="header-anchor">#</a> <strong>故障排除一：控制 reduce 端缓冲大小以避免 OOM</strong></h5> <p>在 Shuffle 过程，reduce 端 task 并不是等到 map 端 task 将其数据全部写入磁盘后再去 拉取，而是 map 端写一点数据，reduce 端 task 就会拉取一小部分数据，然后立即进行后面 的聚合、算子函数的使用等操作。 reduce 端 task 能够拉取多少数据，由 reduce 拉取数据的缓冲区 buffer 来决定，因为拉 取过来的数据都是先放在 buffer 中，然后再进行后续的处理，buffer 的默认大小为 48MB。 reduce 端 task 会一边拉取一边计算，不一定每次都会拉满 48MB 的数据，可能大多数 时候拉取一部分数据就处理掉了。 虽然说增大 reduce 端缓冲区大小可以减少拉取次数，提升 Shuffle 性能，但是有时 map 端的数据量非常大，写出的速度非常快，此时 reduce 端的所有 task 在拉取的时候，有 可能全部达到自己缓冲的最大极限值，即 48MB，此时，再加上 reduce 端执行的聚合函数 的代码，可能会创建大量的对象，这可难会导致内存溢出，即 OOM。 如果一旦出现 reduce 端内存溢出的问题，我们可以考虑减小 reduce 端拉取数据缓冲 区的大小，例如减少为 12MB。 在实际生产环境中是出现过这种问题的，这是典型的以性能换执行的原理。reduce 端 拉取数据的缓冲区减小，不容易导致 OOM，但是相应的，reudce 端的拉取次数增加，造成 更多的网络传输开销，造成性能的下降。 注意，要保证任务能够运行，再考虑性能的优化。</p> <h5 id="故障排除二-jvm-gc-导致的-shuffle-文件拉取失败"><a href="#故障排除二-jvm-gc-导致的-shuffle-文件拉取失败" class="header-anchor">#</a> 故障排除二：JVM GC 导致的 shuffle 文件拉取失败</h5> <p>在 Spark 作业中，有时会出现 shuffle file not found 的错误，这是非常常见的一个报错， 有时出现这种错误以后，选择重新执行一遍，就不再报出这种错误。 出现上述问题可能的原因是 Shuffle 操作中，后面 stage 的 task 想要去上一个 stage 的 task 所在的 Executor 拉取数据，结果对方正在执行 GC，执行 GC 会导致 Executor 内所有的 工作现场全部停止，比如 BlockManager、基于 netty 的网络通信等，这就会导致后面的 task 拉取数据拉取了半天都没有拉取到，就会报出 shuffle file not found 的错误，而第二次 再次执行就不会再出现这种错误。</p> <p>可以通过调整 reduce 端拉取数据重试次数和 reduce 端拉取数据时间间隔这两个参数 来对 Shuffle 性能进行调整，增大参数值，使得 reduce 端拉取数据的重试次数增加，并且 每次失败后等待的时间间隔加长。</p> <p><code>val conf = new SparkConf() .set(&quot;spark.shuffle.io.maxRetries&quot;, &quot;60&quot;) .set(&quot;spark.shuffle.io.retryWait&quot;, &quot;60s&quot;)</code></p> <h3 id="_2-3-rdd编程基础"><a href="#_2-3-rdd编程基础" class="header-anchor">#</a> 2.3 RDD编程基础</h3> <p>Spark采用textFile()方法来从文件系统中加载数据创建RDD</p> <ul><li>该方法把文件的URI作为参数，这个URI可以是：</li> <li>本地文件系统的地址</li> <li>或者是分布式文件系统HDFS的地址 •或者是Amazon S3的地址等</li></ul> <p><strong>EX：</strong></p> <p><strong>读入</strong>：lines = sc.textFile(&quot;hdfs://localhost:9000/user/hadoop/word.txt&quot;)</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172353867.png" alt="image-20230122172353867"></p> <h4 id="_2-3-1-创建rdd"><a href="#_2-3-1-创建rdd" class="header-anchor">#</a> 2.3.1 创建RDD</h4> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175318394.png" alt="image-20230122175318394"></p> <table><thead><tr><th style="text-align:center;">操作</th> <th style="text-align:center;">含义</th></tr></thead> <tbody><tr><td style="text-align:center;">filter(func)</td> <td style="text-align:center;">筛选出满足函数func的元素，并返回一个新的数据 集</td></tr> <tr><td style="text-align:center;">map(func)</td> <td style="text-align:center;">将每个元素传递到函数func中，并将结果返回为一 个新的数据集</td></tr> <tr><td style="text-align:center;">flatMap(func)</td> <td style="text-align:center;">与map()相似，但每个输入元素都可以映射到0或多 个输出结果</td></tr> <tr><td style="text-align:center;">groupByKey()</td> <td style="text-align:center;">应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</td></tr> <tr><td style="text-align:center;">reduceByKey(func)</td> <td style="text-align:center;">应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中每个值是将每个key传递到 函数func中进行聚合后的结果</td></tr></tbody></table> <p>图示：</p> <p><strong>Map</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172359791.png" alt="image-20230122172359791"></p> <p><strong>flatMap</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172403966.png" alt="image-20230122172403966"></p> <p><strong>groupByKey</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172408695.png" alt="image-20230122172408695"></p> <p><strong>reduceByKey：</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122174903653.png" alt="image-20230122174903653"></p> <p><strong>综合案例</strong></p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> lines <span class="token operator">=</span> sc. <span class="token punctuation">\</span>
<span class="token punctuation">..</span>. textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> wordCount <span class="token operator">=</span> lines.flatMap<span class="token punctuation">(</span>lambda line:line.split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">))</span>. <span class="token punctuation">\</span>
<span class="token punctuation">..</span>. map<span class="token punctuation">(</span>lambda word:<span class="token punctuation">(</span>word,1<span class="token punctuation">))</span>.reduceByKey<span class="token punctuation">(</span>lambda a,b:a+b<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> print<span class="token punctuation">(</span>wordCount.collect<span class="token punctuation">(</span><span class="token punctuation">))</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'good'</span>, <span class="token number">1</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'Spark'</span>, <span class="token number">2</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'is'</span>, <span class="token number">3</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'better'</span>, <span class="token number">1</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'Hadoop'</span>, <span class="token number">1</span><span class="token punctuation">)</span>, <span class="token punctuation">(</span><span class="token string">'fast'</span>, <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172413103.png" alt="image-20230122172413103"></p> <h4 id="_2-3-2-分区"><a href="#_2-3-2-分区" class="header-anchor">#</a> 2.3.2 分区</h4> <p><strong>分区原则</strong>：</p> <p>RDD分区的一个原则是使得分区的个数尽量等于集群中的CPU核心 （core）数目</p> <p>对于不同的Spark部署模式而言（本地模式、Standalone模式、 YARN模式、Mesos模式），都可以通过设置 spark.default.parallelism这个参数的值，来配置默认的分区数目，一 般而言：</p> <p>本地模式：默认为本地机器的CPU数目，若设置了local[N],则默认 为N</p> <p>*Apache Mesos：默认的分区数为8</p> <p>*Standalone或YARN：在“集群中所有CPU核心数目总和”和“2” 二者中取较大值作为默认值</p> <p><strong>设置分区个数</strong>：</p> <p>（1）创建RDD时手动指定分区个数 在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法 格式如下： sc.textFile(path, partitionNum) 其中，path参数用于指定要加载的文件的地址，partitionNum参数用于 指定分区个数。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">list</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">//</span>设置两个分区
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>（2）使用reparititon方法重新设置分区个数 通过转换操作得到新 RDD 时，直接调用 repartition 方法即可。例如：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> data <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#显示data这个RDD的分区数量</span>
<span class="token number">2</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> rdd <span class="token operator">=</span> data<span class="token punctuation">.</span>repartition<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment">#对data这个RDD进行重新分区</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>rdd<span class="token punctuation">.</span>glom<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#显示rdd这个RDD的分区数量</span>
<span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>实例：</p> <p>根据key值的最后一位数字，写到不同的文件 例如： 10写入到part-00000 11写入到part-00001 . . . 19写入到part-00009</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkConf<span class="token punctuation">,</span> SparkContext
<span class="token keyword">def</span> <span class="token function">MyPartitioner</span><span class="token punctuation">(</span>key<span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;MyPartitioner is running&quot;</span><span class="token punctuation">)</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'The key is %d'</span> <span class="token operator">%</span> key<span class="token punctuation">)</span>
 <span class="token keyword">return</span> key<span class="token operator">%</span><span class="token number">10</span>
<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;The main function is running&quot;</span><span class="token punctuation">)</span>
 conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">&quot;local&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">&quot;MyApp&quot;</span><span class="token punctuation">)</span>
 sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf <span class="token operator">=</span> conf<span class="token punctuation">)</span>
 data <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>
 data<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>partitionBy<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span>MyPartitioner<span class="token punctuation">)</span> \
 <span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/rdd/partitioner&quot;</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
 main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h4 id="_2-3-3-数据读写"><a href="#_2-3-3-数据读写" class="header-anchor">#</a> 2.3.3 数据读写</h4> <ul><li><p>从本地文件读入</p> <ul><li><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile <span class="token operator">=</span> sc<span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile<span class="token punctuation">.</span>first<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Hadoop is good'</span>
<span class="token operator">//</span>读入
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile <span class="token operator">=</span> sc<span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/rdd/word.txt&quot;</span><span class="token punctuation">)</span>
<span class="token operator">//</span>保存
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile<span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> saveAsTextFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/rdd/writeback&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div></li></ul></li> <li><p>从分布式文件系统读写（文本文件格式）</p> <ul><li><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;hdfs://localhost:9000/user/hadoop/word.txt&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile<span class="token punctuation">.</span>first<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> textFile<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">&quot;writeback&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div></li></ul></li> <li><p>从HBase读入</p> <ul><li>HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、 列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多 个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义 列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行 数据类型转换</li> <li>表：HBase采用表来组织数据，表由行 和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每 个行由行键（row key）来标识。</li> <li>列族：一个HBase表被分组成许多“列 族”（Column Family）的集合，它是 基本的访问控制单元</li> <li>列限定符：列族里的数据通过列限定符 （或列）来定位</li> <li>单元格：在HBase表中，通过行、列族 和列限定符确定一个“单元格”（cell ），单元格中存储的数据没有数据类型 ，总被视为字节数组byte[]</li> <li>时间戳：每个单元格都保存着同一份数 据的多个版本，这些版本采用时间戳进行索引</li> <li><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172418980.png" alt="image-20230122172418980"></li></ul></li></ul> <h3 id="_2-4-spark-sql"><a href="#_2-4-spark-sql" class="header-anchor">#</a> 2.4 Spark SQL</h3> <h4 id="_2-4-1-shark-到spark-sql"><a href="#_2-4-1-shark-到spark-sql" class="header-anchor">#</a> 2.4.1 shark 到spark SQL</h4> <p>​	Shark即Hive on Spark，为了实现与Hive兼容，Shark在 HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、 执行计划优化等逻辑，可以近似认为仅将物理执行计划从 MapReduce作业替换成了Spark作业，通过Hive的HiveQL解 析，把HiveQL翻译成Spark上的RDD操作</p> <p>​	2014年6月1日Shark项目和Spark SQL项目的主持人Reynold Xin宣布： 停止对Shark的开发，团队将所有资源放在Spark SQL项目上，至此， Shark的发展画上了句号，但也因此发展出两个分支：Spark SQL和 Hive on Spar</p> <ul><li>Spark SQL作为Spark生态的一 员继续发展，而不再受限于Hive， 只是兼容Hive</li> <li>Hive on Spark是一个Hive的发 展计划，该计划将Spark作为 Hive的底层引擎之一，也就是说， Hive将不再受限于一个引擎，可 以采用Map-Reduce、Tez、 Spark等引擎</li></ul> <h4 id="_2-4-2-dataframe"><a href="#_2-4-2-dataframe" class="header-anchor">#</a> 2.4.2 Dataframe</h4> <p>DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，不仅比原 有的RDD转化方式更加简单易用，而且获得了更高的计算性能</p> <p>Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询</p> <p><strong>RDD与Dataframe比较</strong></p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172427794.png" alt="image-20230122172427794"></p> <ul><li>RDD是分布式的 Java对象的集合，但是，对象内部结构对于RDD而言却 是不可知的</li> <li>DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息</li></ul> <p>从Spark2.0以上版本开始，Spark使用全新的SparkSession接口替代 Spark1.6中的SQLContext及HiveContext接口来实现其对数据加载、转 换、处理等功能。SparkSession实现了SQLContext及HiveContext所有 功能</p> <p>SparkSession支持从不同的数据源加载数据，并把数据转换成 DataFrame，并且支持把DataFrame转换成SQLContext自身中的表， 然后使用SQL语句来操作数据。</p> <p>SparkSession亦提供了HiveQL以及其 他依赖于Hive的功能的支持 可以通过如下语句创建一个SparkSession对象：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span>SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession
spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">.</span>config<span class="token punctuation">(</span>conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>创建Dataframe</p> <p>在创建DataFrame时，可以使用spark.read操作，从不同类型的文 件中加载数据创建DataFrame，例如：</p> <p>•spark.read.format(&quot;text&quot;).load(&quot;people.txt&quot;)：读取文本文件 people.json创建DataFrame； •spark.read.format(&quot;json&quot;).load(&quot;people.json&quot;)：读取JSON文件 people.json创建DataFrame； •spark.read.format(&quot;parquet&quot;).load(&quot;people.parquet&quot;)：读取 Parquet文件people.parquet创建DataFrame。</p> <p><strong>数据</strong>：</p> <p>假设有数据在“/usr/local/spark/examples/src/main/resources/”这个目录下，这个目录下有 两个样例数据people.json和people.txt。 people.json文件的内容如下：</p> <p><code>{&quot;name&quot;:&quot;Michael&quot;}</code></p> <p><code>{&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30}</code></p> <p><code>{&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19}</code></p> <p>people.txt文件的内容如下：</p> <p><code>Michael, 29</code></p> <p><code>Andy, 30</code></p> <p><code>Justin, 19</code></p> <p>保存Dataframe</p> <p>下面从示例文件people.json中创建一个DataFrame，名称为 peopleDF，把peopleDF保存到另外一个JSON文件中，然后，再从 peopleDF中选取一个列（即name列），把该列数据保存到一个文本 文件中</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> peopleDF <span class="token operator">=</span> spark<span class="token punctuation">.</span>read<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">&quot;json&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> load<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/examples/src/main/resources/people.json&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> peopleDF<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">&quot;name&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;age&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">&quot;json&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> save<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/sparksql/newpeople.json&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> peopleDF<span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token string">&quot;name&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> save<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/mycode/sparksql/newpeople.txt&quot;</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>会新生成一个名称为newpeople.json的目录（不是文件）和一个名称 为newpeople.txt的目录（不是文件） part-00000-3db90180-ec7c-4291-ad05-df8e45c77f4d.json _SUCCESS</p> <p>常用函数：</p> <p>printSchema()，select()，filter()，groupBy等等</p> <h4 id="_2-4-3-rdd转换为dataframe"><a href="#_2-4-3-rdd转换为dataframe" class="header-anchor">#</a> 2.4.3 RDD转换为Dataframe</h4> <p>利用反射机制推断RDD模式（schema）</p> <p>示例：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> Row
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> people <span class="token operator">=</span> spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/examples/src/main/resources/people.txt&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;,&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span> Row<span class="token punctuation">(</span>name<span class="token operator">=</span>p<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> age<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> schemaPeople <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>people<span class="token punctuation">)</span>
<span class="token comment">#必须注册为临时表才能供下面的查询使用</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> schemaPeople<span class="token punctuation">.</span>createOrReplaceTempView<span class="token punctuation">(</span><span class="token string">&quot;people&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> personsDF <span class="token operator">=</span> spark<span class="token punctuation">.</span>sql<span class="token punctuation">(</span><span class="token string">&quot;select name,age from people where age &gt; 20&quot;</span><span class="token punctuation">)</span>
<span class="token comment">#DataFrame中的每个元素都是一行记录，包含name和age两个字段，分别用</span>
p<span class="token punctuation">.</span>name和p<span class="token punctuation">.</span>age来获取值
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> personsRDD<span class="token operator">=</span>personsDF<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span><span class="token string">&quot;Name: &quot;</span><span class="token operator">+</span>p<span class="token punctuation">.</span>name<span class="token operator">+</span> <span class="token string">&quot;,&quot;</span><span class="token operator">+</span>&quot;Age<span class="token punctuation">:</span>
&quot;<span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>age<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> personsRDD<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span><span class="token keyword">print</span><span class="token punctuation">)</span>
Name<span class="token punctuation">:</span> Michael<span class="token punctuation">,</span>Age<span class="token punctuation">:</span> <span class="token number">29</span>
Name<span class="token punctuation">:</span> Andy<span class="token punctuation">,</span>Age<span class="token punctuation">:</span> <span class="token number">30</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>自动推断过程：</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172434466.png" alt="image-20230122172434466"></p> <p><strong>使用编程方式定义RDD模式</strong></p> <p>当无法提前获知数据结构时，就需要采用编程方式定义RDD模式。 比如，现在需要通过编程方式把people.txt加载进来生成 DataFrame，并完成SQL查询。</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172438728.png" alt="image-20230122172438728"></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>types <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> Row
<span class="token comment">#下面生成“表头”</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> schemaString <span class="token operator">=</span> <span class="token string">&quot;name age&quot;</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> fields <span class="token operator">=</span> <span class="token punctuation">[</span>StructField<span class="token punctuation">(</span>field_name<span class="token punctuation">,</span> StringType<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">for</span> field_name <span class="token keyword">in</span>
schemaString<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> schema <span class="token operator">=</span> StructType<span class="token punctuation">(</span>fields<span class="token punctuation">)</span>
<span class="token comment">#下面生成“表中的记录”</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> lines <span class="token operator">=</span> spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span>\
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///usr/local/spark/examples/src/main/resources/people.txt&quot;</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> parts <span class="token operator">=</span> lines<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot;,&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> people <span class="token operator">=</span> parts<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span> Row<span class="token punctuation">(</span>p<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> p<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#下面把“表头”和“表中的记录”拼装在一起</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> schemaPeople <span class="token operator">=</span> spark<span class="token punctuation">.</span>createDataFrame<span class="token punctuation">(</span>people<span class="token punctuation">,</span> schema<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172444117.png" alt="image-20230122172444117"></p> <h2 id="_3-spark-streaming"><a href="#_3-spark-streaming" class="header-anchor">#</a> 3 Spark Streaming</h2> <h3 id="_3-1-静态数据和流数据"><a href="#_3-1-静态数据和流数据" class="header-anchor">#</a> 3.1 静态数据和流数据</h3> <p><strong>静态数据</strong>：</p> <p>很多企业为了支持决策分析而构建的数据仓库系统，其中 存放的大量历史数据就是静态数据。技术人员可以利用数 据挖掘和OLAP（On-Line Analytical Processing）分析工 具从静态数据中找到对企业有价值的信息</p> <p><strong>流数据</strong>：</p> <p>近年来，在Web应用、网络监控、传感监测等领域，兴 起了一种新的数据密集型应用——流数据，即数据以大量 、快速、时变的流形式持续到达 。实例：PM2.5检测、电子商务网站用户点击流</p> <p>静态数据和流数据 流数据具有如下特征：</p> <ul><li>数据快速持续到达，潜在大小也许是无穷无尽的</li> <li>数据来源众多，格式复杂</li> <li>数据量大，但是不十分关注存储，一旦经过处理，要 么被丢弃，要么被归档存储</li> <li>注重数据的整体价值，不过分关注个别数据</li> <li>数据顺序颠倒，或者不完整，系统无法控制将要处理 的新到达的数据元素的顺序</li></ul> <p>对静态数据和流数据的处理，对应着两种截然不同的计算模式：<strong>批量 计算和实时计算</strong></p> <p><strong>批量计算</strong></p> <p>充裕时间处理静态数据， 如Hadoop</p> <p><strong>实时计算:</strong></p> <p>**流数据不适合采用批量计算，**因为流数据不适合用传统的关系模型建模</p> <p>流数据必须采用实时计算，响应时间为秒级</p> <p>数据量少时，不是问题，但是，在大数据时代，数据格式复杂、来源众多、 数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计 算——流计算，应运而生</p> <p><strong>流计算</strong>：</p> <p><strong>针对流数据的实时计算</strong>。<strong>实时获取来自不同数据源的海量数据</strong>，经过实时 分析处理，获得有价值的信息</p> <p>流计算秉承一个基本理念，<strong>即数据的价值随着时间的流逝 而降低</strong>，如用户点击流。因此，当事件出现时就应该立即 进行处理，而不是缓存起来进行批量处理。为了及时处理 流数据，就需要一个低延迟、可扩展、高可靠的处理引擎</p> <p>特点：</p> <p>对于一个流计算系统来说，它应达到如下需求：</p> <ul><li>高性能：处理大数据的基本要求，如每秒处理几十万 条数据</li> <li>海量式：支持TB级甚至是PB级的数据规模</li> <li>实时性：保证较低的延迟时间，达到秒级别，甚至是 毫秒级别</li> <li>分布式：支持大数据的基本架构，必须能够平滑扩展</li> <li>易用性：能够快速进行开发和部署</li> <li>可靠性：能可靠地处理流数据</li></ul> <p>举例：</p> <p>较为常见的是开源流计算框架，代表如下：</p> <p>– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高 效、可靠地处理大量的流数据</p> <p>– Yahoo! S4（Simple Scalable Streaming System）：开源流计算 平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的 流式系统</p> <h3 id="_3-2-流计算处理过程"><a href="#_3-2-流计算处理过程" class="header-anchor">#</a> 3.2 流计算处理过程</h3> <p>流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算 、实时查询服务<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114172510455.png" alt="image-20230114172510455"></p> <h3 id="_3-3-spark-streaming简介"><a href="#_3-3-spark-streaming简介" class="header-anchor">#</a> 3.3 Spark Streaming简介</h3> <p>Spark Streaming的基本原理是将实时输入数据流以时 间片（秒级）为单位进行拆分，然后经Spark引擎以类 似批处理的方式处理每个时间片数据<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173002042.png" alt="image-20230114173002042"></p> <p>Spark Streaming最主要的抽象是<strong>DStream（Discretized Stream，离散化数据 流）</strong>，表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按 照时间片（如1秒）分成一段一段**，每一段数据转换为Spark中的RDD，这些分 段就是Dstream，**并且对DStream的操作都最终转变为对相应的RDD的操作</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173047816.png" alt="image-20230114173047816"></p> <p>Spark Streaming和Storm最大的区别在于，<strong>Spark Streaming无法实现毫秒级的流计算，而Storm可以实 现毫秒级响应</strong></p> <p>Spark Streaming采用的小批量处理的方式使得它可 以同时兼容批量和实时数据处理的逻辑和算法，因此， 方便了一些需要历史数据和实时数据联合分析的特定 应用场合</p> <p><strong>案例</strong>：</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173616774.png" alt="image-20230114173616774"></p> <ul><li><p>在Spark Streaming中，会有一个组件Receiver，作为一个长期运 行的task跑在一个Executor上</p></li> <li><p>每个Receiver都会负责一个input DStream（比如从文件中读取数据 的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）</p></li> <li><p>Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173657107.png" alt="image-20230114173657107"></p></li></ul> <p>编写Stream数据基本步骤：</p> <ol><li>通过创建输入DStream来定义输入源</li> <li>通过对DStream应用转换操作和输出操作来定义流计算</li> <li>用streamingContext.start()来开始接收数据和处理流程</li> <li>通过streamingContext.awaitTermination()方法来等待 处理结束（手动结束或因为错误而结束）</li> <li>可以通过streamingContext.stop()来手动结束流计算 进程</li></ol> <p>EX：</p> <p>如果要运行一个Spark Streaming程序，就需要首先生成一个 StreamingContext对象，它是Spark Streaming程序的主入口</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'TestDStream'</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">'local[2]'</span><span class="token punctuation">)</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf <span class="token operator">=</span> conf<span class="token punctuation">)</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="_3-4-输入源"><a href="#_3-4-输入源" class="header-anchor">#</a> 3.4 输入源</h3> <p><strong>文件流</strong></p> <ul><li><p>在pyspark中创建文件流</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>$ cd <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span>mycode
$ mkdir streaming
$ cd streaming
$ mkdir logfile
$ cd logfile



<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span> \
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> textFileStream<span class="token punctuation">(</span><span class="token string">'file:///usr/local/spark/mycode/streaming/logfile'</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> wordCounts <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x <span class="token punctuation">:</span>
<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span>b<span class="token punctuation">:</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>
上面在pyspark中执行的程序，一旦你输入ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>以后，程序就开
始自动进入循环监听状态。

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div></li> <li><p>采用独立应用方式新建</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>$ cd <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span>mycode
$ cd streaming
$ cd logfile
$ vim FileStreaming<span class="token punctuation">.</span>py


<span class="token keyword">from</span> pyspark <span class="token keyword">import</span> SparkContext<span class="token punctuation">,</span> SparkConf
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>streaming <span class="token keyword">import</span> StreamingContext
conf <span class="token operator">=</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">'TestDStream'</span><span class="token punctuation">)</span>
conf<span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">'local[2]'</span><span class="token punctuation">)</span>
sc <span class="token operator">=</span> SparkContext<span class="token punctuation">(</span>conf <span class="token operator">=</span> conf<span class="token punctuation">)</span>
ssc <span class="token operator">=</span> StreamingContext<span class="token punctuation">(</span>sc<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
lines <span class="token operator">=</span> ssc<span class="token punctuation">.</span>textFileStream<span class="token punctuation">(</span><span class="token string">'file:///usr/local/spark/mycode/streaming/logfile'</span><span class="token punctuation">)</span>
words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span><span class="token keyword">lambda</span> line<span class="token punctuation">:</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
wordCounts <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x <span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span><span class="token keyword">lambda</span> a<span class="token punctuation">,</span>b<span class="token punctuation">:</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span>
wordCounts<span class="token punctuation">.</span>pprint<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
ssc<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>

$ cd <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span>mycode<span class="token operator">/</span>streaming<span class="token operator">/</span>logfile<span class="token operator">/</span>
$ <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span><span class="token builtin">bin</span><span class="token operator">/</span>spark<span class="token operator">-</span>submit FileStreaming<span class="token punctuation">.</span>py
$ cd <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span>mycode<span class="token operator">/</span>streaming<span class="token operator">/</span>logfile<span class="token operator">/</span>
$ <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span><span class="token builtin">bin</span><span class="token operator">/</span>spark<span class="token operator">-</span>submit FileStreaming<span class="token punctuation">.</span>py
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div></li></ul> <p>下面的以后再补</p> <p><strong>套接字流</strong></p> <p><strong>RDD</strong>队列流</p> <p>高级数据源KafKa</p> <h3 id="_3-5-struct-streaming"><a href="#_3-5-struct-streaming" class="header-anchor">#</a> <strong>3.5</strong> <strong>Struct Streaming</strong></h3> <p>Structured Streaming的关键思想是将实时数据流视为一张正 在不断添加数据的表</p> <p>可以把流计算等同于在一个静态表上的批处理查询，Spark会 在不断添加数据的无界输入表上运行计算，并进行增量查询</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191114903.png" alt="image-20230114191114903"></p> <p>在无界表上对输入的查询将生成结果表，系统每隔一定的周期会 触发对无界表的计算并更新结果表</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191141121.png" alt="image-20230114191141121"></p> <h4 id="_3-5-1-两种处理模型"><a href="#_3-5-1-两种处理模型" class="header-anchor">#</a> 3.5.1 两种处理模型</h4> <p><strong>微批处理模型</strong>：</p> <p>Structured Streaming默认使用微批处理执行模型，这意味着Spark 流计算引擎会定期检查流数据源，并对自上一批次结束后到达的新 数据执行批量查询</p> <p>数据到达和得到处理并输出结果之间的延时超过100毫秒</p> <p><strong>持续处理模型</strong>：</p> <p>Spark从2.3.0版本开始引入了持续处理的试验性功能，可以实现流计 算的毫秒级延迟</p> <p>在持续处理模式下，Spark不再根据触发器来周期性启动任务，而是 启动一系列的连续读取、处理和写入结果的长时间运行的任务</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191303795.png" alt="image-20230114191303795"></p> <p><strong>Struct Streaming 和Spark Streaming对比</strong>：</p> <p>Structured Streaming处理的数据跟Spark Streaming一样，也是源 源不断的数据流，区别在于，Spark Streaming采用的数据抽象是 DStream（本质上就是一系列RDD），而Structured Streaming采用 的数据抽象是DataFrame。</p> <p>Structured Streaming可以使用Spark SQL的DataFrame/Dataset来 处理数据流。虽然Spark SQL也是采用DataFrame作为数据抽象， 但是，Spark SQL只能处理静态的数据，而Structured Streaming可 以处理结构化的数据流。这样，Structured Streaming就将Spark SQL和Spark Streaming二者的特性结合了起来。</p> <h4 id="_3-5-2实例"><a href="#_3-5-2实例" class="header-anchor">#</a> <strong>3.5.2实例</strong></h4> <p>编写Structured Streaming程序的基本步骤包括：</p> <ul><li>导入pyspark模块</li> <li>创建SparkSession对象</li> <li>创建输入数据源</li> <li>定义流计算过程</li> <li>启动流计算并输出结果</li></ul> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment">#实例任务：一个包含很多行英文语句的数据流源源不断</span>
<span class="token comment">#到达，Structured Streaming程序对每行英文语句进行</span>
<span class="token comment">#拆分，并统计每个单词出现的频率</span>

一导入数据
导入PySpark模块，代码如下：
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql <span class="token keyword">import</span> SparkSession
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>functions <span class="token keyword">import</span> split
<span class="token keyword">from</span> pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>functions <span class="token keyword">import</span> explode
由于程序中需要用到拆分字符串和展开数组内的所有单词的功能，
所以引用了来自pyspark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>functions里面的split和explode函数。

<span class="token number">2.</span>步骤<span class="token number">2</span>：创建SparkSession对象
创建一个SparkSession对象，代码如下：
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">&quot;__main__&quot;</span><span class="token punctuation">:</span>
 spark <span class="token operator">=</span> SparkSession \
 <span class="token punctuation">.</span>builder \
 <span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">&quot;StructuredNetworkWordCount&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>
 spark<span class="token punctuation">.</span>sparkContext<span class="token punctuation">.</span>setLogLevel<span class="token punctuation">(</span><span class="token string">'WARN'</span><span class="token punctuation">)</span>
    
<span class="token number">3</span>．步骤<span class="token number">3</span>：创建输入数据源
创建一个输入数据源，从“监听在本机（localhost）的<span class="token number">9999</span>端口上
的服务”那里接收文本数据，具体语句如下：
 lines <span class="token operator">=</span> spark \
 <span class="token punctuation">.</span>readStream \
 <span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">&quot;socket&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">&quot;host&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;localhost&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">&quot;port&quot;</span><span class="token punctuation">,</span> <span class="token number">9999</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token number">4.</span>步骤<span class="token number">4</span>：定义流计算过程
有了输入数据源以后，接着需要定义相关的查询语句，具体如下：
 words <span class="token operator">=</span> lines<span class="token punctuation">.</span>select<span class="token punctuation">(</span>
 explode<span class="token punctuation">(</span>
 split<span class="token punctuation">(</span>lines<span class="token punctuation">.</span>value<span class="token punctuation">,</span> <span class="token string">&quot; &quot;</span><span class="token punctuation">)</span>
 <span class="token punctuation">)</span><span class="token punctuation">.</span>alias<span class="token punctuation">(</span><span class="token string">&quot;word&quot;</span><span class="token punctuation">)</span>
 <span class="token punctuation">)</span>
 wordCounts <span class="token operator">=</span> words<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token string">&quot;word&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token number">5.</span>步骤<span class="token number">5</span>：启动流计算并输出结果
定义完查询语句后，下面就可以开始真正执行流计算，具体语句如下：
 query <span class="token operator">=</span> wordCounts \
 <span class="token punctuation">.</span>writeStream \
 <span class="token punctuation">.</span>outputMode<span class="token punctuation">(</span><span class="token string">&quot;complete&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token string">&quot;console&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>trigger<span class="token punctuation">(</span>processingTime<span class="token operator">=</span><span class="token string">&quot;8 seconds&quot;</span><span class="token punctuation">)</span> \
 <span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
 query<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>


把代码写入文件StructuredNetworkWordCount<span class="token punctuation">.</span>py
在执行StructuredNetworkWordCount<span class="token punctuation">.</span>py之前，需要启动HDFS。
启动HDFS的命令如下：

cd <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span>mycode<span class="token operator">/</span>structuredstreaming<span class="token operator">/</span>
$ <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>spark<span class="token operator">/</span><span class="token builtin">bin</span><span class="token operator">/</span>spark<span class="token operator">-</span>submit StructuredNetworkWordCount<span class="token punctuation">.</span>py	
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br></div></div><h2 id="_4-spark-mlib"><a href="#_4-spark-mlib" class="header-anchor">#</a> 4 Spark MLib</h2> <h3 id="_4-1spark-mlib简介"><a href="#_4-1spark-mlib简介" class="header-anchor">#</a> 4.1Spark MLib简介</h3> <p>Spark提供了一个基于海量数据的机器学习库，它提供 了常用机器学习算法的分布式实现</p> <p>​	pyspark的即席查询也是一个关键。算法工程师可以边 写代码边运行，边看结果</p> <ul><li>需要注意的是，MLlib中只包含能够在集群上运行良好 的并行算法，这一点很重要</li> <li>有些经典的机器学习算法没有包含在其中，就是因为它 们不能并行执行</li> <li>相反地，一些较新的研究得出的算法因为适用于集群， 也被包含在MLlib中，例如分布式随机森林算法、最小交 替二乘算法。这样的选择使得MLlib中的每一个算法都适 用于大规模数据集</li></ul> <p>MLlib是Spark的机器学习（Machine Learning）库，旨在 简化机器学习的工程实践工作</p> <p>•MLlib由一些通用的学习算法和工具组成，包括分类、回 归、聚类、协同过滤、降维等，同时还包括底层的优化原 语和高层的流水线（Pipeline）API，具体如下：</p> <ul><li>算法工具：常用的学习算法，如分类、回归、聚类和协 同过滤；</li> <li>特征化工具：特征提取、转化、降维和选择工具；</li> <li>流水线(Pipeline)：用于构建、评估和调整机器学习工 作流的工具; •持久性：保存和加载算法、模型和管道;</li> <li>实用工具：线性代数、统计、数据处理等工具。</li></ul> <p>支持算法举例：</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114194027142.png" alt="image-20230114194027142"></p></div></div>  <div class="page-edit"><div class="edit-link"><a href="https://github.com/flokken/edit/master/docs/大数据/01.Spark/01.Spark基础.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="tags"><a href="/tags/?tag=Spark" title="标签">#Spark</a></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2023/03/15, 09:28:20</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><!----> <a href="/pages/09f3e1/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">HBase与Hive介绍</div></a></div> <div class="page-nav"><p class="inner"><!----> <span class="next"><a href="/pages/09f3e1/">HBase与Hive介绍</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/fd9dbf/"><div>
            线程池
            <!----></div></a> <span class="date">09-16</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/64ddd0/"><div>
            消息队列基础知识
            <!----></div></a> <span class="date">09-08</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/4b5092/"><div>
            树状数组和线段树
            <!----></div></a> <span class="date">08-22</span></dt></dl> <dl><dd></dd> <dt><a href="/archives" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:2878846959@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/Flokken" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/my/m/music/playlist?id=807177837" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2023-2024
    <span>flokken | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.6c1ebbcf.js" defer></script><script src="/assets/js/2.28dcc766.js" defer></script><script src="/assets/js/194.2ac85355.js" defer></script>
  </body>
</html>
