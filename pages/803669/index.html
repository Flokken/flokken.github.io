<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>神经网络的学习技巧 | flokken&#39;s blog</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="记录学过的东西">
    <meta name="keywords" content="JavaScript,js,ES6,TypeScript,vue,python,css3,html5,Node,git,github,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.bc84ce4b.css" as="style"><link rel="preload" href="/assets/js/app.6c1ebbcf.js" as="script"><link rel="preload" href="/assets/js/2.28dcc766.js" as="script"><link rel="preload" href="/assets/js/46.342f85bd.js" as="script"><link rel="prefetch" href="/assets/js/10.dadef33a.js"><link rel="prefetch" href="/assets/js/100.c21337b1.js"><link rel="prefetch" href="/assets/js/101.eeb225e5.js"><link rel="prefetch" href="/assets/js/102.442361dc.js"><link rel="prefetch" href="/assets/js/103.8495a4a1.js"><link rel="prefetch" href="/assets/js/104.be5e7531.js"><link rel="prefetch" href="/assets/js/105.f968e108.js"><link rel="prefetch" href="/assets/js/106.9c7bcb9d.js"><link rel="prefetch" href="/assets/js/107.5418bfc1.js"><link rel="prefetch" href="/assets/js/108.21c8e4b5.js"><link rel="prefetch" href="/assets/js/109.443388e7.js"><link rel="prefetch" href="/assets/js/11.cb3946f4.js"><link rel="prefetch" href="/assets/js/110.aee2ce54.js"><link rel="prefetch" href="/assets/js/111.c1c4c853.js"><link rel="prefetch" href="/assets/js/112.a77d44c2.js"><link rel="prefetch" href="/assets/js/113.3e1c1563.js"><link rel="prefetch" href="/assets/js/114.b73e18ed.js"><link rel="prefetch" href="/assets/js/115.464712f7.js"><link rel="prefetch" href="/assets/js/116.515a3426.js"><link rel="prefetch" href="/assets/js/117.1de6d86e.js"><link rel="prefetch" href="/assets/js/118.c87898a4.js"><link rel="prefetch" href="/assets/js/119.4d47ac77.js"><link rel="prefetch" href="/assets/js/12.3d11ba1b.js"><link rel="prefetch" href="/assets/js/120.adc822aa.js"><link rel="prefetch" href="/assets/js/121.dea1a90f.js"><link rel="prefetch" href="/assets/js/122.1490ee10.js"><link rel="prefetch" href="/assets/js/123.08cfb23f.js"><link rel="prefetch" href="/assets/js/124.28d0e817.js"><link rel="prefetch" href="/assets/js/125.a7630423.js"><link rel="prefetch" href="/assets/js/126.0d5b60c3.js"><link rel="prefetch" href="/assets/js/127.250521db.js"><link rel="prefetch" href="/assets/js/128.508639a2.js"><link rel="prefetch" href="/assets/js/129.5e6579a1.js"><link rel="prefetch" href="/assets/js/13.74ebc418.js"><link rel="prefetch" href="/assets/js/130.1c66619f.js"><link rel="prefetch" href="/assets/js/131.e83e8060.js"><link rel="prefetch" href="/assets/js/132.7ff3654e.js"><link rel="prefetch" href="/assets/js/133.4125eaa7.js"><link rel="prefetch" href="/assets/js/134.4a100ce2.js"><link rel="prefetch" href="/assets/js/135.789a6a26.js"><link rel="prefetch" href="/assets/js/136.9cc03ec3.js"><link rel="prefetch" href="/assets/js/137.8bd376fe.js"><link rel="prefetch" href="/assets/js/138.532d50a9.js"><link rel="prefetch" href="/assets/js/139.53372ba6.js"><link rel="prefetch" href="/assets/js/14.eaf82e4f.js"><link rel="prefetch" href="/assets/js/140.88405785.js"><link rel="prefetch" href="/assets/js/141.38d215a9.js"><link rel="prefetch" href="/assets/js/142.e0163f88.js"><link rel="prefetch" href="/assets/js/143.b523e1ce.js"><link rel="prefetch" href="/assets/js/144.10873c46.js"><link rel="prefetch" href="/assets/js/145.dfe09936.js"><link rel="prefetch" href="/assets/js/146.b05dc164.js"><link rel="prefetch" href="/assets/js/147.c5b7f960.js"><link rel="prefetch" href="/assets/js/148.80f47df1.js"><link rel="prefetch" href="/assets/js/149.b8d840e3.js"><link rel="prefetch" href="/assets/js/15.e64aec1b.js"><link rel="prefetch" href="/assets/js/150.d4361634.js"><link rel="prefetch" href="/assets/js/151.d8431db2.js"><link rel="prefetch" href="/assets/js/152.47852d30.js"><link rel="prefetch" href="/assets/js/153.47ea2a94.js"><link rel="prefetch" href="/assets/js/154.97384b42.js"><link rel="prefetch" href="/assets/js/155.3026447d.js"><link rel="prefetch" href="/assets/js/156.68e37577.js"><link rel="prefetch" href="/assets/js/157.086f0b3f.js"><link rel="prefetch" href="/assets/js/158.bdcb5b98.js"><link rel="prefetch" href="/assets/js/159.f52cb015.js"><link rel="prefetch" href="/assets/js/16.e312a677.js"><link rel="prefetch" href="/assets/js/160.a2739d99.js"><link rel="prefetch" href="/assets/js/161.7212e5c4.js"><link rel="prefetch" href="/assets/js/162.8006dc05.js"><link rel="prefetch" href="/assets/js/163.bf42549e.js"><link rel="prefetch" href="/assets/js/164.5b2eea28.js"><link rel="prefetch" href="/assets/js/165.d531f59f.js"><link rel="prefetch" href="/assets/js/166.86bf0046.js"><link rel="prefetch" href="/assets/js/167.af7a49eb.js"><link rel="prefetch" href="/assets/js/168.362435ae.js"><link rel="prefetch" href="/assets/js/169.b71a8d5d.js"><link rel="prefetch" href="/assets/js/17.e9813cce.js"><link rel="prefetch" href="/assets/js/170.975564fa.js"><link rel="prefetch" href="/assets/js/171.4949b39a.js"><link rel="prefetch" href="/assets/js/172.8d7529ad.js"><link rel="prefetch" href="/assets/js/173.404d8588.js"><link rel="prefetch" href="/assets/js/174.6641d1c4.js"><link rel="prefetch" href="/assets/js/175.0f46859d.js"><link rel="prefetch" href="/assets/js/176.2fd697ec.js"><link rel="prefetch" href="/assets/js/177.0f744d20.js"><link rel="prefetch" href="/assets/js/178.021de7d2.js"><link rel="prefetch" href="/assets/js/179.593f9493.js"><link rel="prefetch" href="/assets/js/18.0bfadb6c.js"><link rel="prefetch" href="/assets/js/180.8305cce0.js"><link rel="prefetch" href="/assets/js/181.5e83f1b6.js"><link rel="prefetch" href="/assets/js/182.87985f96.js"><link rel="prefetch" href="/assets/js/183.80ba1b3c.js"><link rel="prefetch" href="/assets/js/184.f11d0532.js"><link rel="prefetch" href="/assets/js/185.c859c104.js"><link rel="prefetch" href="/assets/js/186.dc782260.js"><link rel="prefetch" href="/assets/js/187.bc43a42b.js"><link rel="prefetch" href="/assets/js/188.abf4a947.js"><link rel="prefetch" href="/assets/js/189.3911f624.js"><link rel="prefetch" href="/assets/js/19.ebbfaca6.js"><link rel="prefetch" href="/assets/js/190.6ba0f02e.js"><link rel="prefetch" href="/assets/js/191.612eaebf.js"><link rel="prefetch" href="/assets/js/192.7545f46e.js"><link rel="prefetch" href="/assets/js/193.8c879c3b.js"><link rel="prefetch" href="/assets/js/194.2ac85355.js"><link rel="prefetch" href="/assets/js/195.887d8d44.js"><link rel="prefetch" href="/assets/js/196.82fc2486.js"><link rel="prefetch" href="/assets/js/197.f3d5a63f.js"><link rel="prefetch" href="/assets/js/198.acb21d94.js"><link rel="prefetch" href="/assets/js/199.72a831c8.js"><link rel="prefetch" href="/assets/js/20.7946efec.js"><link rel="prefetch" href="/assets/js/200.d22fdf55.js"><link rel="prefetch" href="/assets/js/201.d9df6873.js"><link rel="prefetch" href="/assets/js/202.001109bc.js"><link rel="prefetch" href="/assets/js/203.ba860144.js"><link rel="prefetch" href="/assets/js/204.0bb6f98e.js"><link rel="prefetch" href="/assets/js/205.d9c979cd.js"><link rel="prefetch" href="/assets/js/206.1d24147d.js"><link rel="prefetch" href="/assets/js/207.f3d10dab.js"><link rel="prefetch" href="/assets/js/208.c3ba92f2.js"><link rel="prefetch" href="/assets/js/209.fe7a19e2.js"><link rel="prefetch" href="/assets/js/21.0b137891.js"><link rel="prefetch" href="/assets/js/210.5d424205.js"><link rel="prefetch" href="/assets/js/211.d84d326b.js"><link rel="prefetch" href="/assets/js/212.1d38af17.js"><link rel="prefetch" href="/assets/js/213.061f3dbf.js"><link rel="prefetch" href="/assets/js/214.ecb8325b.js"><link rel="prefetch" href="/assets/js/215.615caf62.js"><link rel="prefetch" href="/assets/js/216.e23dc266.js"><link rel="prefetch" href="/assets/js/217.9933d9c4.js"><link rel="prefetch" href="/assets/js/218.5cdf2ac4.js"><link rel="prefetch" href="/assets/js/219.72e58688.js"><link rel="prefetch" href="/assets/js/22.3762b636.js"><link rel="prefetch" href="/assets/js/220.fbc6f972.js"><link rel="prefetch" href="/assets/js/221.1c8eeced.js"><link rel="prefetch" href="/assets/js/222.163dee0a.js"><link rel="prefetch" href="/assets/js/223.72e37b20.js"><link rel="prefetch" href="/assets/js/224.e09609e5.js"><link rel="prefetch" href="/assets/js/225.34fd5c90.js"><link rel="prefetch" href="/assets/js/226.9991814a.js"><link rel="prefetch" href="/assets/js/227.1e2b8a98.js"><link rel="prefetch" href="/assets/js/228.1d4d1fcf.js"><link rel="prefetch" href="/assets/js/229.cc91769e.js"><link rel="prefetch" href="/assets/js/23.f7ebea5b.js"><link rel="prefetch" href="/assets/js/230.3d3edd0f.js"><link rel="prefetch" href="/assets/js/231.0475038b.js"><link rel="prefetch" href="/assets/js/232.a9e3d0d3.js"><link rel="prefetch" href="/assets/js/233.2ca519d3.js"><link rel="prefetch" href="/assets/js/234.c51411ea.js"><link rel="prefetch" href="/assets/js/235.35616a91.js"><link rel="prefetch" href="/assets/js/236.db361c9c.js"><link rel="prefetch" href="/assets/js/237.c8e14a33.js"><link rel="prefetch" href="/assets/js/238.0f2fcf3a.js"><link rel="prefetch" href="/assets/js/239.a33a3324.js"><link rel="prefetch" href="/assets/js/24.42946d79.js"><link rel="prefetch" href="/assets/js/240.7bf17817.js"><link rel="prefetch" href="/assets/js/241.613e82c4.js"><link rel="prefetch" href="/assets/js/242.5709b7c9.js"><link rel="prefetch" href="/assets/js/243.4f2501de.js"><link rel="prefetch" href="/assets/js/244.63720e3b.js"><link rel="prefetch" href="/assets/js/245.e756153f.js"><link rel="prefetch" href="/assets/js/246.b709da2f.js"><link rel="prefetch" href="/assets/js/247.e6f5dc6f.js"><link rel="prefetch" href="/assets/js/248.79fff09e.js"><link rel="prefetch" href="/assets/js/249.2c7ad7a2.js"><link rel="prefetch" href="/assets/js/25.a86a3678.js"><link rel="prefetch" href="/assets/js/250.29112339.js"><link rel="prefetch" href="/assets/js/251.634c259b.js"><link rel="prefetch" href="/assets/js/252.f020ace1.js"><link rel="prefetch" href="/assets/js/253.21843ce0.js"><link rel="prefetch" href="/assets/js/254.819b0d0b.js"><link rel="prefetch" href="/assets/js/255.a1a36af5.js"><link rel="prefetch" href="/assets/js/256.b331fde3.js"><link rel="prefetch" href="/assets/js/257.d496873b.js"><link rel="prefetch" href="/assets/js/258.d25fd3f2.js"><link rel="prefetch" href="/assets/js/259.e26ffb21.js"><link rel="prefetch" href="/assets/js/26.5fe90cb9.js"><link rel="prefetch" href="/assets/js/260.173d7a1c.js"><link rel="prefetch" href="/assets/js/261.fd2846ac.js"><link rel="prefetch" href="/assets/js/262.110b431f.js"><link rel="prefetch" href="/assets/js/263.deab1489.js"><link rel="prefetch" href="/assets/js/264.b38ed189.js"><link rel="prefetch" href="/assets/js/265.e50c8ee7.js"><link rel="prefetch" href="/assets/js/266.132d2f80.js"><link rel="prefetch" href="/assets/js/267.df5c367d.js"><link rel="prefetch" href="/assets/js/268.94eee0dd.js"><link rel="prefetch" href="/assets/js/269.27287f29.js"><link rel="prefetch" href="/assets/js/27.0ac244ef.js"><link rel="prefetch" href="/assets/js/270.82414c58.js"><link rel="prefetch" href="/assets/js/271.88d41d2f.js"><link rel="prefetch" href="/assets/js/272.fdea98e3.js"><link rel="prefetch" href="/assets/js/273.291d7bce.js"><link rel="prefetch" href="/assets/js/274.6470bef1.js"><link rel="prefetch" href="/assets/js/275.ae5aaecb.js"><link rel="prefetch" href="/assets/js/276.6249c409.js"><link rel="prefetch" href="/assets/js/277.c281acfe.js"><link rel="prefetch" href="/assets/js/278.4bf4de94.js"><link rel="prefetch" href="/assets/js/279.62240a5f.js"><link rel="prefetch" href="/assets/js/28.7b66a9da.js"><link rel="prefetch" href="/assets/js/280.a05c4459.js"><link rel="prefetch" href="/assets/js/281.f5dbccd2.js"><link rel="prefetch" href="/assets/js/282.3c8929fa.js"><link rel="prefetch" href="/assets/js/283.8632920a.js"><link rel="prefetch" href="/assets/js/284.74b2e3cc.js"><link rel="prefetch" href="/assets/js/285.1c3bb787.js"><link rel="prefetch" href="/assets/js/286.145be126.js"><link rel="prefetch" href="/assets/js/287.0a623eba.js"><link rel="prefetch" href="/assets/js/288.e07989da.js"><link rel="prefetch" href="/assets/js/289.2f5fdfd9.js"><link rel="prefetch" href="/assets/js/29.ebd9ba6b.js"><link rel="prefetch" href="/assets/js/290.0b12e49c.js"><link rel="prefetch" href="/assets/js/291.fc65a93a.js"><link rel="prefetch" href="/assets/js/292.2beb238d.js"><link rel="prefetch" href="/assets/js/293.87c822f4.js"><link rel="prefetch" href="/assets/js/294.450b6510.js"><link rel="prefetch" href="/assets/js/295.9191ddf7.js"><link rel="prefetch" href="/assets/js/296.92ec965c.js"><link rel="prefetch" href="/assets/js/297.68c01079.js"><link rel="prefetch" href="/assets/js/298.768b54ab.js"><link rel="prefetch" href="/assets/js/299.c2b57741.js"><link rel="prefetch" href="/assets/js/3.cd795b12.js"><link rel="prefetch" href="/assets/js/30.05a622e0.js"><link rel="prefetch" href="/assets/js/300.f409e502.js"><link rel="prefetch" href="/assets/js/301.56e89c78.js"><link rel="prefetch" href="/assets/js/302.942b0d94.js"><link rel="prefetch" href="/assets/js/303.d3946671.js"><link rel="prefetch" href="/assets/js/304.a346c8aa.js"><link rel="prefetch" href="/assets/js/305.46fa308b.js"><link rel="prefetch" href="/assets/js/306.c7f27ad8.js"><link rel="prefetch" href="/assets/js/307.0c8cf8c0.js"><link rel="prefetch" href="/assets/js/308.1d2db23d.js"><link rel="prefetch" href="/assets/js/309.2055654b.js"><link rel="prefetch" href="/assets/js/31.cc1bdb96.js"><link rel="prefetch" href="/assets/js/310.b6b7887a.js"><link rel="prefetch" href="/assets/js/311.e605ceab.js"><link rel="prefetch" href="/assets/js/312.52192bb7.js"><link rel="prefetch" href="/assets/js/313.2698bd8e.js"><link rel="prefetch" href="/assets/js/314.5d9073d0.js"><link rel="prefetch" href="/assets/js/315.c409cde3.js"><link rel="prefetch" href="/assets/js/316.2bf0e6df.js"><link rel="prefetch" href="/assets/js/317.5798e7df.js"><link rel="prefetch" href="/assets/js/318.95986f30.js"><link rel="prefetch" href="/assets/js/319.e6def8d6.js"><link rel="prefetch" href="/assets/js/32.ef0d5dba.js"><link rel="prefetch" href="/assets/js/320.8b3e8e94.js"><link rel="prefetch" href="/assets/js/321.f936d520.js"><link rel="prefetch" href="/assets/js/322.cdb3332c.js"><link rel="prefetch" href="/assets/js/323.4e193830.js"><link rel="prefetch" href="/assets/js/324.5da1850e.js"><link rel="prefetch" href="/assets/js/325.ff5ccde4.js"><link rel="prefetch" href="/assets/js/326.160651b1.js"><link rel="prefetch" href="/assets/js/327.db3feae7.js"><link rel="prefetch" href="/assets/js/328.4ebf4ae0.js"><link rel="prefetch" href="/assets/js/329.f71c3883.js"><link rel="prefetch" href="/assets/js/33.3c571bbe.js"><link rel="prefetch" href="/assets/js/330.885ae61c.js"><link rel="prefetch" href="/assets/js/331.b1a938ba.js"><link rel="prefetch" href="/assets/js/332.b142c42b.js"><link rel="prefetch" href="/assets/js/333.b8076f78.js"><link rel="prefetch" href="/assets/js/334.ad22ab75.js"><link rel="prefetch" href="/assets/js/335.88564fd2.js"><link rel="prefetch" href="/assets/js/336.39134b8d.js"><link rel="prefetch" href="/assets/js/337.df1ebc2c.js"><link rel="prefetch" href="/assets/js/34.270daa56.js"><link rel="prefetch" href="/assets/js/35.15d989c9.js"><link rel="prefetch" href="/assets/js/36.2e8aeb80.js"><link rel="prefetch" href="/assets/js/37.7b762275.js"><link rel="prefetch" href="/assets/js/38.22ec6ac7.js"><link rel="prefetch" href="/assets/js/39.bdb3430a.js"><link rel="prefetch" href="/assets/js/4.f7fa33d6.js"><link rel="prefetch" href="/assets/js/40.345f3c53.js"><link rel="prefetch" href="/assets/js/41.e93aab58.js"><link rel="prefetch" href="/assets/js/42.e7446174.js"><link rel="prefetch" href="/assets/js/43.3bbfdb12.js"><link rel="prefetch" href="/assets/js/44.4291d737.js"><link rel="prefetch" href="/assets/js/45.82f2cc4f.js"><link rel="prefetch" href="/assets/js/47.f2f6fe03.js"><link rel="prefetch" href="/assets/js/48.99fd0146.js"><link rel="prefetch" href="/assets/js/49.2634fd40.js"><link rel="prefetch" href="/assets/js/5.ae42b7a0.js"><link rel="prefetch" href="/assets/js/50.691080e8.js"><link rel="prefetch" href="/assets/js/51.292e4b2b.js"><link rel="prefetch" href="/assets/js/52.30b4a61b.js"><link rel="prefetch" href="/assets/js/53.2c6237df.js"><link rel="prefetch" href="/assets/js/54.49456bfa.js"><link rel="prefetch" href="/assets/js/55.4710856c.js"><link rel="prefetch" href="/assets/js/56.0334fc17.js"><link rel="prefetch" href="/assets/js/57.4f70d0ed.js"><link rel="prefetch" href="/assets/js/58.d9a970bf.js"><link rel="prefetch" href="/assets/js/59.7a4b31fd.js"><link rel="prefetch" href="/assets/js/6.d5889eef.js"><link rel="prefetch" href="/assets/js/60.417e05df.js"><link rel="prefetch" href="/assets/js/61.c7980332.js"><link rel="prefetch" href="/assets/js/62.58f3c155.js"><link rel="prefetch" href="/assets/js/63.7229815d.js"><link rel="prefetch" href="/assets/js/64.0d72da6d.js"><link rel="prefetch" href="/assets/js/65.2c623405.js"><link rel="prefetch" href="/assets/js/66.d170f5ad.js"><link rel="prefetch" href="/assets/js/67.01023aac.js"><link rel="prefetch" href="/assets/js/68.018604c5.js"><link rel="prefetch" href="/assets/js/69.abbe4df3.js"><link rel="prefetch" href="/assets/js/7.bf0f2b67.js"><link rel="prefetch" href="/assets/js/70.962b3ba0.js"><link rel="prefetch" href="/assets/js/71.a5606282.js"><link rel="prefetch" href="/assets/js/72.975041a5.js"><link rel="prefetch" href="/assets/js/73.d93206af.js"><link rel="prefetch" href="/assets/js/74.67fe0aca.js"><link rel="prefetch" href="/assets/js/75.a321d6fc.js"><link rel="prefetch" href="/assets/js/76.2a44bacd.js"><link rel="prefetch" href="/assets/js/77.90387be7.js"><link rel="prefetch" href="/assets/js/78.c6455938.js"><link rel="prefetch" href="/assets/js/79.8c3687c2.js"><link rel="prefetch" href="/assets/js/8.0b70db32.js"><link rel="prefetch" href="/assets/js/80.ed6a6259.js"><link rel="prefetch" href="/assets/js/81.9cc4a70c.js"><link rel="prefetch" href="/assets/js/82.4dfd22c3.js"><link rel="prefetch" href="/assets/js/83.bbe5ca26.js"><link rel="prefetch" href="/assets/js/84.0141f56f.js"><link rel="prefetch" href="/assets/js/85.26069038.js"><link rel="prefetch" href="/assets/js/86.c22416bd.js"><link rel="prefetch" href="/assets/js/87.ad2b839e.js"><link rel="prefetch" href="/assets/js/88.de3f91d8.js"><link rel="prefetch" href="/assets/js/89.abb8ff44.js"><link rel="prefetch" href="/assets/js/9.5282ef41.js"><link rel="prefetch" href="/assets/js/90.8dab802c.js"><link rel="prefetch" href="/assets/js/91.67430952.js"><link rel="prefetch" href="/assets/js/92.c95aaeec.js"><link rel="prefetch" href="/assets/js/93.d68e1554.js"><link rel="prefetch" href="/assets/js/94.f9fd1744.js"><link rel="prefetch" href="/assets/js/95.a4fd9b78.js"><link rel="prefetch" href="/assets/js/96.32d95188.js"><link rel="prefetch" href="/assets/js/97.3bf42546.js"><link rel="prefetch" href="/assets/js/98.a032b733.js"><link rel="prefetch" href="/assets/js/99.fc7abb0d.js">
    <link rel="stylesheet" href="/assets/css/0.styles.bc84ce4b.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu have-body-img"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/logo.png" alt="flokken's blog" class="logo"> <span class="site-name can-hide">flokken's blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><!----> <span class="title" style="display:;">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>web开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/javascript/" class="nav-link">JavaScript</a></li><li class="dropdown-subitem"><a href="/vue/" class="nav-link">Vue</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="后端开发" class="dropdown-title"><!----> <span class="title" style="display:;">后端开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/java/" class="nav-link">Java开发</a></li><li class="dropdown-item"><!----> <a href="/go/" class="nav-link">Go开发</a></li><li class="dropdown-item"><!----> <a href="/microservice/" class="nav-link">微服务开发</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/KG/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/DL/" class="nav-link">深度学习</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全与运维" class="dropdown-title"><!----> <span class="title" style="display:;">安全与运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/reverse/" class="nav-link">逆向</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><!----> <span class="title" style="display:;">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/spark/" class="nav-link">Spark</a></li><li class="dropdown-item"><!----> <a href="/spider/" class="nav-link">Spider</a></li><li class="dropdown-item"><!----> <a href="/mysql/" class="nav-link">MySQL</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法与数据结构" class="dropdown-title"><!----> <span class="title" style="display:;">算法与数据结构</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/algorithm/" class="nav-link">算法</a></li><li class="dropdown-item"><!----> <a href="/datastructure/" class="nav-link">数据结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><!----> <span class="title" style="display:;">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/tips/" class="nav-link">小知识</a></li></ul></div></div> <a href="https://github.com/flokken" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/avtar.png"> <div class="blogger-info"><h3>flokken</h3> <span>一个大水货</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="前端" class="dropdown-title"><!----> <span class="title" style="display:;">前端</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>web开发</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/javascript/" class="nav-link">JavaScript</a></li><li class="dropdown-subitem"><a href="/vue/" class="nav-link">Vue</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="后端开发" class="dropdown-title"><!----> <span class="title" style="display:;">后端开发</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/java/" class="nav-link">Java开发</a></li><li class="dropdown-item"><!----> <a href="/go/" class="nav-link">Go开发</a></li><li class="dropdown-item"><!----> <a href="/microservice/" class="nav-link">微服务开发</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="AI" class="dropdown-title"><!----> <span class="title" style="display:;">AI</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/KG/" class="nav-link">知识图谱</a></li><li class="dropdown-item"><!----> <a href="/DL/" class="nav-link">深度学习</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="安全与运维" class="dropdown-title"><!----> <span class="title" style="display:;">安全与运维</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/reverse/" class="nav-link">逆向</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="大数据" class="dropdown-title"><!----> <span class="title" style="display:;">大数据</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/spark/" class="nav-link">Spark</a></li><li class="dropdown-item"><!----> <a href="/spider/" class="nav-link">Spider</a></li><li class="dropdown-item"><!----> <a href="/mysql/" class="nav-link">MySQL</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法与数据结构" class="dropdown-title"><!----> <span class="title" style="display:;">算法与数据结构</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/algorithm/" class="nav-link">算法</a></li><li class="dropdown-item"><!----> <a href="/datastructure/" class="nav-link">数据结构</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="其他" class="dropdown-title"><!----> <span class="title" style="display:;">其他</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/tips/" class="nav-link">小知识</a></li></ul></div></div> <a href="https://github.com/flokken" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>知识图谱</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>Pytorch</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>深度学习-李宏毅2022</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading open"><span>鱼书-深度学习入门</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/435b55/" class="sidebar-link">numpy与matplotlib</a></li><li><a href="/pages/11726c/" class="sidebar-link">感知机与神经网络</a></li><li><a href="/pages/6c444b/" class="sidebar-link">损失函数</a></li><li><a href="/pages/1c6d13/" class="sidebar-link">数值微分与梯度</a></li><li><a href="/pages/81deb7/" class="sidebar-link">误差反向传播</a></li><li><a href="/pages/803669/" aria-current="page" class="active sidebar-link">神经网络的学习技巧</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/803669/#参数更新" class="sidebar-link">参数更新</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/803669/#sgd" class="sidebar-link">SGD</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#momentum" class="sidebar-link">Momentum</a></li><li class="sidebar-sub-header level4"><a href="/pages/803669/#sgd的缺点" class="sidebar-link">SGD的缺点</a></li><li class="sidebar-sub-header level4"><a href="/pages/803669/#momentum是啥" class="sidebar-link">Momentum是啥</a></li><li class="sidebar-sub-header level4"><a href="/pages/803669/#代码实现" class="sidebar-link">代码实现：</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#adagrad" class="sidebar-link">AdaGrad</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#adam" class="sidebar-link">Adam</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/803669/#权重的初始值" class="sidebar-link">权重的初始值</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/803669/#能不能将初始权重全部设置为0" class="sidebar-link">能不能将初始权重全部设置为0？</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#隐藏层激活值分布" class="sidebar-link">隐藏层激活值分布</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#梯度消失" class="sidebar-link">梯度消失</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#表现力受限" class="sidebar-link">表现力受限</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#xavier初始值" class="sidebar-link">Xavier初始值</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#relu的权重初始值" class="sidebar-link">ReLU的权重初始值</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#总结" class="sidebar-link">总结</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/803669/#batch-normalization" class="sidebar-link">Batch Normalization</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/803669/#结论" class="sidebar-link">结论</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/803669/#正则化" class="sidebar-link">正则化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/803669/#权值衰减" class="sidebar-link">权值衰减</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/803669/#dropout" class="sidebar-link">Dropout</a></li><li class="sidebar-sub-header level2"><a href="/pages/803669/#超参数的验证" class="sidebar-link">超参数的验证</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/803669/#验证集" class="sidebar-link">验证集</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#超参数最优化" class="sidebar-link">超参数最优化</a></li><li class="sidebar-sub-header level3"><a href="/pages/803669/#还是以minist为例" class="sidebar-link">还是以minist为例</a></li></ul></li></ul></li><li><a href="/pages/88621c/" class="sidebar-link">CNN介绍</a></li></ul></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>鱼书-自然语言处理</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable is-sub-group depth-1"><p class="sidebar-heading"><span>论文阅读</span> <span class="arrow right"></span></p> <!----></section></li></ul></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/categories/?category=AI" title="分类" data-v-06225672>AI</a></li><li data-v-06225672><a href="/categories/?category=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="分类" data-v-06225672>深度学习</a></li><li data-v-06225672><a href="/categories/?category=%E9%B1%BC%E4%B9%A6-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8" title="分类" data-v-06225672>鱼书-深度学习入门</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/Flokken" target="_blank" title="作者" class="beLink" data-v-06225672>flokken</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-03-30</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">神经网络的学习技巧<!----></h1>  <div class="theme-vdoing-content content__default"><h2 id="参数更新"><a href="#参数更新" class="header-anchor">#</a> 参数更新</h2> <p>神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。不过这个问题很困难，因为参数空间很复杂，无法轻易找到最优解（无法使用那种通过解数学式一下子就求得最小值的方法）。深度神经网络中，参数的数量非常庞大，导致最优化问题更加复杂。</p> <p>我们之前使用梯度来寻找最优的参数，也即是使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠</p> <p>近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称<strong>SGD</strong></p> <blockquote><p>注意这里的随机是每次随机选取一批数据</p></blockquote> <h3 id="sgd"><a href="#sgd" class="header-anchor">#</a> SGD</h3> <p>在复习一下SGD的公式：</p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></p> <p>SGD是朝着梯度方向只前,进一定距离的简单方法。将其实现为一个类：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SGD</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
    
    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>params<span class="token punctuation">,</span>grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这里默认lr为0.01，注意参数params和grads（与之前的神经网络的实现一样）是字典型变量，按params['W1']、grads['W1']的形式，分别保存了权重参数和它们的梯度。</p> <blockquote><p>一个key就是一层</p></blockquote> <p>这里可以用探险家的故事来理解寻找最优参数的困难</p> <blockquote><p>有一个性情古怪的探险家。他在广袤的干旱地带旅行，坚持寻找幽深的山谷。他的目标是要到达最深的谷底（他称之为“至深之地”）。这也是他旅行的目的。并且，他给自己制定了两个严格的“规定”：一个是不看地图；另一个是把眼睛蒙上。因此，他并不知道最深的谷底在这个广袤的大地的何处，而且什么也看不见。在这么严苛的条件下，这位探险家如何前往“至深之地”呢？他要如何迈步，才能迅速找到“至深之地”呢？</p> <p>寻找最优参数时，我们所处的状况和这位探险家一样，是一个漆黑的世界。我们必须在没有地图、不能睁眼的情况下，在广袤、复杂的地形中寻找“至深之地”</p></blockquote> <h3 id="momentum"><a href="#momentum" class="header-anchor">#</a> Momentum</h3> <h4 id="sgd的缺点"><a href="#sgd的缺点" class="header-anchor">#</a> SGD的缺点</h4> <p>思考下面这个函数最小值:</p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="f"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c="("></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-mi space="2" class="mjx-i"><mjx-c c="y"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c c=")"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c><mjx-c c="0"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c c="y"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container></p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330165020788.png" style="zoom:70%;"> <p>这个梯度的特征是，y轴方向上大，x轴方向上小。换句话说， 就是y轴方向的坡度大，而x轴方向的坡度小。这里需要注意的是，虽然式 （6.2）的最小值在(x, y) = (0, 0)处，但是图6-2中的<strong>梯度在很多地方并没有指 向(0, 0)</strong></p> <p>其梯度图如下：</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330165210917.png" style="zoom:70%;"> <p>我们来尝试对图6-1这种形状的函数应用SGD。</p> <p>从(x, y) = (−7.0, 2.0)处 （初始值）开始搜索，结果如图6-3所示。 在图6-3中，SGD呈“之”字形移动。这是一个相当低效的路径。也就是说， SGD的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索 的路径就会非常低效。因此，我们需要比单纯朝梯度方向前进的SGD更聪 明的方法。SGD低效的根本原因是，<strong>梯度的方向并没有指向最小值的方向</strong>。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330165330078.png" style="zoom:70%;"> <h4 id="momentum是啥"><a href="#momentum是啥" class="header-anchor">#</a> Momentum是啥</h4> <p>Momentum是“动量”的意思，和物理有关。用数学式表示Momentum方法，如下所示。</p> <p>$v \leftarrow \alpha v-\eta\frac{\partial L}{\partial W} $</p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mi space="3" class="mjx-i"><mjx-c c="v"></mjx-c></mjx-mi></mjx-math></mjx-container></p> <p>和前面的SGD一样，W表示要更新的权重参数， 表示损失函数关 于W的梯度，η表示学习率。<strong>这里新出现了一个变量v，对应物理上的速度</strong>。 表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增 加这一物理法则。</p> <p>Momentum方法给人的感觉就像是<strong>小球在地面上滚动</strong></p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330165921342.png" style="zoom:70%;"> <p>αv这一项是用来表示，<strong>在物体不受任何力时，该项承担使物体逐渐减速的任务</strong>（α设定为0.9之类的值），<strong>对应物理上的地面摩擦或空气阻力</strong></p> <blockquote><p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-utext variant="normal" style="font-family:serif;">称</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">为</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">动</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">量</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">参</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">数</mjx-utext></mjx-mo></mjx-math></mjx-container></p></blockquote> <p>为什么要这干呢？我们假设<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="3B1"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="="></mjx-c></mjx-mo><mjx-mn space="4" class="mjx-n"><mjx-c c="0"></mjx-c><mjx-c c="."></mjx-c><mjx-c c="9"></mjx-c></mjx-mn></mjx-math></mjx-container>，迭代过程如下</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330202247666.png" alt="image-20230330202247666"></p> <blockquote><p>我们可以看到之前的梯度会一直存在后面的迭代过程中，只是越靠前的梯度其权重越小。（说的数学一点，我们取的是这些梯度步长的指数平均）。</p></blockquote> <p>下图是实际情况下，只使用‘SGD的行进过程</p> <p><img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330202502064.png" alt="image-20230330202502064"></p> <blockquote><p>注意到大部分的梯度更新呈锯齿状。我们也注意到，<strong>每一步的梯度更新方向可以被进一步分解为 w1 和 w2 分量</strong>。如果我们单独的将这些向量求和，沿 w1 方向的的分量将抵消，沿 w2 方向的分量将得到加强。</p></blockquote> <p>更新路径就像小球在碗中滚动一样。和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，<strong>但是因为交互地受到正方向和反方向的力，它们会互相抵消</strong>，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330192817009.png" style="zoom:70%;"> <h4 id="代码实现"><a href="#代码实现" class="header-anchor">#</a> 代码实现：</h4> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Momentum</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
        self<span class="token punctuation">.</span>momentum <span class="token operator">=</span> momentum
        self<span class="token punctuation">.</span>v <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">def</span> <span class="token function">updtae</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>params<span class="token punctuation">,</span>grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>v <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>v <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            <span class="token keyword">for</span> key<span class="token punctuation">,</span>val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>
                
        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>key<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            slef<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>momentum<span class="token operator">*</span>self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>lr<span class="token operator">*</span>grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
            
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><blockquote><p>其实就是多了一个累积量，让以前的也会影响到下一步梯度的更新</p></blockquote> <h3 id="adagrad"><a href="#adagrad" class="header-anchor">#</a> AdaGrad</h3> <p>在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多” 学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。</p> <p>逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。 而AdaGrad 进一步发展了这个想法，针对“一个一个”的参数，赋予其“定 制”的值。 <strong>AdaGrad会为参数的每个元素适当地调整学习率</strong>，与此同时进行学习 （AdaGrad的Ada来自英文单词Adaptive，即“适当的”的意思）。下面，用数学式表示AdaGrad的更新方法。</p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi><mjx-mo space="3" class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo space="3" class="mjx-n"><mjx-c c="2217"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-mo space="4" class="mjx-n"><mjx-c c="2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B7"></mjx-c></mjx-mi><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.11em;"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="L"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c c="2202"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container></p> <blockquote><p>W表示要更新的权重参数， 表示损失函数关 于W的梯度，η表示学习率。这里新出现了变量h,<strong>它保 存了以前的所有梯度值的平方和</strong></p></blockquote> <p>在更新参数时，通过乘以 <mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.11em;"><mjx-mi class="mjx-i"><mjx-c c="h"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container>，就可以调整学习的尺度。这意味着， 参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说， 可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。</p> <blockquote><p>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新 的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0， 完全不再更新。为了改善这个问题，可以使用 RMSProp 方法。 RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐 地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。 这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小 过去的梯度的尺度</p></blockquote> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">AdaGrad</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span>lr
        self<span class="token punctuation">.</span>h <span class="token operator">=</span><span class="token boolean">None</span>
        
    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>params<span class="token punctuation">,</span>grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>h <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
            <span class="token keyword">for</span> key<span class="token punctuation">,</span>val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>key<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
        params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-7</span> <span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>这里需要注意的是，最后一行加上了微小值1e-7。这是为了防止当。self.h[key]中有0时，将0用作除数的情况。</p> <h3 id="adam"><a href="#adam" class="header-anchor">#</a> Adam</h3> <p>Momentum参照小球在碗中滚动的物理规则进行移动，AdaGrad为参 数的每个元素适当地调整更新步伐。如果将这两个方法融合在一起,,这就是Adam方法的基本思路.其证明过程很复杂，可以看论文。</p> <blockquote><p>Adam会设置 3个超参数。一个是学习率（论文中以α出现），另外两 个是一次momentum系数β1和二次momentum系数β2。根据论文， 标准的设定值是β1为 0.9，β2 为 0.999。设置了这些值后，大多数情 况下都能顺利运行。</p></blockquote> <p>根据使用的方法不同，参数更新的路径也不同。只看这个图的话，AdaGrad似乎是最好的，不过也要注意，结果会根据要解决的问</p> <p>题而变。并且，很显然，超参数（学习率等）的设定值不同，结果也会发生变化</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330205735504.png" style="zoom:70%;"> <h2 id="权重的初始值"><a href="#权重的初始值" class="header-anchor">#</a> 权重的初始值</h2> <p>设定什么样的权重初始值，经常关系到神经网络的学习能否成功。</p> <h3 id="能不能将初始权重全部设置为0"><a href="#能不能将初始权重全部设置为0" class="header-anchor">#</a> 能不能将初始权重全部设置为0？</h3> <p>不能。因为在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在2层神经网络中，假设第1层和第2层的权重为0。这样一来，正向传播时，因为输入层的权重为0，所以第2层的神经元全部会被传递相同的值。第2层的神经元中全部输入相同的值，这意味着反向传播时第2层的权重全部都会进行<strong>相同的更新</strong>(（回忆一下“乘法节点的反向传播”)。因此，权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），<strong>必须随机生成初始值</strong></p> <blockquote><p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上， 在这之前的权重初始值都是像0.01 * np.random.randn(10, 100)这样，使用 由高斯分布生成的值乘以0.01后得到的值</p></blockquote> <h3 id="隐藏层激活值分布"><a href="#隐藏层激活值分布" class="header-anchor">#</a> 隐藏层激活值分布</h3> <p>做一个简单的实验，观察权重初始值是如何影响隐藏层的激活值的分布的。这里要做的实验是，<strong>向一个5层神经网络（激活函数使用sigmoid函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布。</strong></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># coding: utf-8</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt


<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">ReLU</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">tanh</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    
input_data <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>  <span class="token comment"># 1000个数据</span>
node_num <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># 各隐藏层的节点（神经元）数</span>
hidden_layer_size <span class="token operator">=</span> <span class="token number">5</span>  <span class="token comment"># 隐藏层有5层</span>
activations <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>  <span class="token comment"># 激活值的结果保存在这里</span>

x <span class="token operator">=</span> input_data

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>hidden_layer_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> activations<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>

    <span class="token comment"># 改变初始值进行实验！</span>
    w <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>node_num<span class="token punctuation">,</span> node_num<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1</span>
    <span class="token comment"># w = np.random.randn(node_num, node_num) * 0.01</span>
    <span class="token comment"># w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)</span>
    <span class="token comment"># w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)</span>


    a <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span>


    <span class="token comment"># 将激活函数的种类也改变，来进行实验！</span>
    z <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    <span class="token comment"># z = ReLU(a)</span>
    <span class="token comment"># z = tanh(a)</span>

    activations<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> z

<span class="token comment"># 绘制直方图</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> a <span class="token keyword">in</span> activations<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>activations<span class="token punctuation">)</span><span class="token punctuation">,</span> i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">&quot;-layer&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span> plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># plt.xlim(0.1, 1)</span>
    <span class="token comment"># plt.ylim(0, 7000)</span>
    plt<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>a<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token builtin">range</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br></div></div><p>这里假设神经网络有5层，每层有100个神经元。然后，用高斯分布随机生成1000个数据作为输入数据，并把它们传给5层神经网络。激活函数使用sigmoid函数，各层的激活值的结果保存在activations变量中。这个代码段中需要注意的是<strong>权重的尺度</strong>。虽然这次我们使用的是标准差为1的高斯分布，但实验的目的是通过改变这个尺度（标准差），观察激活值的分布如何变化。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330215107111.png" style="zoom:70%;"> <h3 id="梯度消失"><a href="#梯度消失" class="header-anchor">#</a> 梯度消失</h3> <p>各层的激活值呈偏向0和1的分布。这里使用的sigmoid 函数是S型函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接 近0。因此，偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最 后消失。这个问题称为<strong>梯度消失</strong>（gradient vanishing）。层次加深的深度学习 中，梯度消失的问题可能会更加严重</p> <h3 id="表现力受限"><a href="#表现力受限" class="header-anchor">#</a> 表现力受限</h3> <p>如果仅仅把标准差改成0.01，也就是只改动这个</p> <p><code>w = np.random.randn(node_num, node_num) * 0.01</code></p> <p>结果如下图</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330215237651.png" style="zoom:70%;"> <p>这次呈集中在0.5附近的分布。<strong>因为不像刚才的例子那样偏向0和1，所以不会发生梯度消失的问题</strong>。但是，激活值的分布有所偏向，说明在表现力上会有很大问题。为什么这么说呢？因为<strong>如果有多个神经元都输出几乎相同的值，那它们就没有存在的意义了</strong>。比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。因此，<strong>激活值在分布上有所偏向会出现“表现力受限”的问题。</strong></p> <blockquote><p>各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。</p></blockquote> <h3 id="xavier初始值"><a href="#xavier初始值" class="header-anchor">#</a> Xavier初始值</h3> <p>Xavier Glorot等人的论文中推荐的权重初始值（俗称“Xavier初始值”)</p> <p>Xavier的论文中，为了使各层的激活值呈现出具有相同广度的分布，推 导了合适的权重尺度。推导出的结论是，如果前一层的节点数为n，则初始 值使用标准差为 的分布A 。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330215843327.png" style="zoom:70%;"> <p>使用Xavier初始值后，<strong>前一层的节点数越多，要设定为目标节点的初始值的权重尺度就越小</strong>。现在，我们使用Xavier初始值进行实验。进行实验的代码只需要将设定权重初始值的地方换成如下内容即可（因为此处所有层的节点数都是100，所以简化了实现）</p> <p><code>w = np.random.randn(node_num, node_num) / np.sqrt(node_num)</code></p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330220010465.png" style="zoom:70%;"> <p>从这个结果可知，越是后面的层，图像变得越歪斜，但是呈现了比之前更有广度的分布。因为各层间传递的数据有适当的广度，所以sigmoid函数的表现力不受限制，有望进行高效的学习</p> <blockquote><p>图 6-13的分布中，后面的层的分布呈稍微歪斜的形状。如果用tanh 函数（双曲线函数）代替sigmoid函数，这个稍微歪斜的问题就能得 到改善。实际上，使用tanh函数后，会呈漂亮的吊钟型分布。tanh 函数和sigmoid函数同是 S型曲线函数，但tanh函数是关于原点(0, 0) 对称的 S型曲线，而sigmoid函数是关于(x, y)=(0, 0.5)对称的S型曲 线。众所周知，<strong>用作激活函数的函数最好具有关于原点对称的性质。</strong></p></blockquote> <h3 id="relu的权重初始值"><a href="#relu的权重初始值" class="header-anchor">#</a> ReLU的权重初始值</h3> <p>Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid函数和tanh函数左右对称，且中央附近可以视作线性函数，所以适 合使用Xavier初始值。但<strong>当激活函数使用ReLU时，一般推荐使用ReLU专 用的初始值</strong>，也就是Kaiming He等人推荐的初始值，也称为**“He初始值”**</p> <p>当前一层的节点数为<em>n</em>时，He初始值使用标准差为<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-lop"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.254em;"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi size="s" class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container>的高斯分布。当Xavier的出啥子会是<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-lop"><mjx-c c="221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top:0.254em;"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi size="s" class="mjx-i"><mjx-c c="n"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math></mjx-container>时，可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数</p> <p>下面进行一下对比：</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330220723909.png" style="zoom:70%;"> <p>观察实验结果可知，当“std = 0.01”时，各层的激活值非常小 A。神经网 络上传递的是非常小的值，说明逆向传播时权重的梯度也同样很小。这是很 严重的问题，<strong>实际上学习基本上没有进展。</strong></p> <p>接下来是初始值为Xavier初始值时的结果。在这种情况下，随着层的加深， 偏向一点点变大。<strong>实际上，层加深后，激活值的偏向变大，学习时会出现梯 度消失的问题。</strong></p> <p>而当初始值为He初始值时，各层中分布的广度相同。由于 即便层加深，数据的广度也能保持不变，因此逆向传播时，也会传递合适的值。</p> <h3 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h3> <p>当激活函数使用ReLU时，权重初始值使用He初始值，当 激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。 这是目前的最佳实践。</p> <h2 id="batch-normalization"><a href="#batch-normalization" class="header-anchor">#</a> Batch Normalization</h2> <p>上面可以知道，如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。</p> <p>那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布 会怎样呢？实际上，Batch Normalization方法就是基于这个想法而产生的。</p> <p>Batch Norm有以下优点。</p> <ul><li>可以使学习快速进行（可以增大学习率）。</li> <li>不那么依赖初始值（对于初始值不用那么神经质）。</li> <li>抑制过拟合（降低Dropout等的必要性）。</li></ul> <p>Batch Norm的思路是调整各层的激活值分布使其拥有适当 的广度。为此，<strong>要向神经网络中插入对数据分布进行正规化的层，即Batch Normalization层</strong>（下文简称Batch Norm层）</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230330221110976.png" style="zoom:70%;"> <p>Batch Norm，<strong>顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化</strong>。具体而言，就是进行使数据分布的均值为0、方差为1的 正规化。用数学式表示的话，如下所示</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331095658139.png" style="zoom:70%;"> <p>这里对mini-batch的<em>m</em>个输入数据的集合<em>B</em> ={x1,x2...xm}求均值<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="u"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="b"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-utext variant="normal" style="font-family:serif;">和</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">方</mjx-utext><mjx-utext variant="normal" style="font-family:serif;">差</mjx-utext></mjx-mo><mjx-msubsup space="4"><mjx-mi noIC="true" class="mjx-i"><mjx-c c="3C3"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.3em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn><mjx-spacer style="margin-top:0.18em;"></mjx-spacer><mjx-mi size="s" class="mjx-i"><mjx-c c="B"></mjx-c></mjx-mi></mjx-script></mjx-msubsup></mjx-math></mjx-container>​,<strong>然后对输入数据进行均值为0，方差为1（标准正态分布）的正规化</strong>，式（6*.<em>7）中的</em>ε*是一个微小值（比如，10e-7等），<strong>它是为了防止出现除以0的情况</strong>。</p> <blockquote><p>若随机变量X服从一个数学期望为μ、方差为σ2的正态分布，记为N(μ，σ2)。其概率密度函数为正态分布的期望值μ决定了其位置，其标准差σ决定了分布的幅度。当μ = 0,σ = 1时的正态分布是标准正态分布。</p></blockquote> <p>经过这个处理，输入数据变成了，{<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.531em;"><mjx-mo class="mjx-n"><mjx-c c="^"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msub><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.531em;"><mjx-mo class="mjx-n"><mjx-c c="^"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msub><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.531em;"><mjx-mo class="mjx-n"><mjx-c c="^"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="3"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c=","></mjx-c></mjx-mo><mjx-msub><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.531em;"><mjx-mo class="mjx-n"><mjx-c c="^"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="m"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math></mjx-container>},这个处理插入到激活函数的前面（或者后面)，<strong>可以减小数据分布的偏向</strong></p> <p>接着，Batch Norm层会<strong>对正规化后的数据进行缩放和平移的变换</strong>，用数学式可以如下表示</p> <p><mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-msub><mjx-mi noIC="true" class="mjx-i"><mjx-c c="y"></mjx-c></mjx-mi><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo space="4" class="mjx-n"><mjx-c c="2190"></mjx-c></mjx-mo><mjx-mi space="4" class="mjx-i"><mjx-c c="3B3"></mjx-c></mjx-mi><mjx-msub><mjx-TeXAtom><mjx-mover><mjx-over style="padding-bottom:0.06em;padding-left:0.064em;margin-bottom:-0.531em;"><mjx-mo class="mjx-n"><mjx-c c="^"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c c="x"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-TeXAtom><mjx-script style="vertical-align:-0.15em;"><mjx-mi size="s" class="mjx-i"><mjx-c c="i"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c c="+"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c c="3B2"></mjx-c></mjx-mi></mjx-math></mjx-container></p> <p>一开始<em>γ</em> = 1，<em>β</em> = 0，然后再通过学习调整到合适的值</p> <p><strong>这个算法是神经网络上的正向传播，用计算图可以表示如下</strong></p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331100835339.png" style="zoom:70%;"> <blockquote><p>当然也有反向传播，不过有点复杂，不介绍</p></blockquote> <h3 id="结论"><a href="#结论" class="header-anchor">#</a> 结论</h3> <p>几乎所有的情况下都是使用Batch Norm时<strong>学习进行得更快</strong>。同时也可以发现，实际上，在不使用Batch Norm的情况下，如果不赋予一</p> <p>个尺度好的初始值，学习将完全无法进行。通过使用Batch Norm，可以推动学习的进行。并且，<strong>对权重初始值变得健壮</strong>（“对初始值健壮”表示不那么依赖初始值）</p> <h2 id="正则化"><a href="#正则化" class="header-anchor">#</a> 正则化</h2> <p>机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。</p> <blockquote><p>直观表现就是训练集正确率很高，测试集却低</p></blockquote> <p><strong>过拟合原因：</strong></p> <ul><li>模型拥有大量参数、表现力强。</li> <li>训练数据少。</li></ul> <p><strong>案例</strong></p> <p>我们故意满足这两个条件，制造过拟合现象。为此，要从</p> <p>MNIST数据集原本的60000个训练数据中<strong>只选定300个</strong>，并且，为了增加网</p> <p>络的复杂度，<strong>使用7层网络</strong>（每层有100个神经元，激活函数为ReLU）。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331101337890.png" style="zoom:70%;"> <p>过了 100 个 epoch 左右后，用训练数据测量到的识别精度几乎都为100%。但是，对于测试数据，离100%的识别精度还有较大的差距。如此大的识别精度差距，<strong>这样就是过拟合了。</strong></p> <h3 id="权值衰减"><a href="#权值衰减" class="header-anchor">#</a> 权值衰减</h3> <p>权值衰减是一直以来经常被使用的<strong>一种抑制过拟合的方法</strong>。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。<strong>很多过拟合原本就是因为权重参数取值过大才发生的</strong></p> <p>复习一下，<strong>神经网络的学习目的是减小损失函数的值</strong>。这里介绍L2正则化，其实就是为损失函数加上<strong>权重的平方范数</strong>（L2范数）。这样一来，就可以抑制权重变大。</p> <p>如果把权重记为<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi></mjx-math></mjx-container>，L2范数的权重衰减就是<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class="mjx-i"><mjx-c c="3BB"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;margin-left:0.071em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>,然后将这个东西加到损失函数上，就是L2正则化了。这里，<em>λ</em>是控制正则化强度的超参数。<em>λ</em>越大，<strong>对大的权重施加惩罚越重。</strong>。此外，<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class="mjx-i"><mjx-c c="3BB"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;margin-left:0.071em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container> 开头的<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math></mjx-container> 是用于将<mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class="mjx-i"><mjx-c c="3BB"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;margin-left:0.071em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>的求导结<strong>果变成λW的调整用常量</strong>。</p> <p>对于所有权重，权值衰减方法都会为损失函数加上 <mjx-container jax="CHTML" class="MathJax"><mjx-math class="MJX-TEX"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="1"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mi class="mjx-i"><mjx-c c="3BB"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c c="W"></mjx-c></mjx-mi><mjx-script style="vertical-align:0.363em;margin-left:0.071em;"><mjx-mn size="s" class="mjx-n"><mjx-c c="2"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math></mjx-container>。因此，在求权重梯度的计算中，<strong>要为之前的误差反向传播法的结果加上正则化项的导数λW</strong>。</p> <blockquote><p>这里详细摘抄一下L2范数是什么</p></blockquote> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331102844658.png" style="zoom:70%;"> <p>加入L2范数后的变化</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331103112772.png" style="zoom:70%;"> <p>虽然训练数据的识别精度和测试数据的识别精度之间有差距，但是与没有使用权值衰减的图6-20的结果相比，<strong>差距变小了</strong>。这说明</p> <p>过拟合受到了抑制。此外**，还要注意，训练数据的识别精度没有达到100%**</p> <h2 id="dropout"><a href="#dropout" class="header-anchor">#</a> Dropout</h2> <p>正则化可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。</p> <p>在这种情况下，我们经常会使用Dropout 方法</p> <p>Dropout是一种在学习的过程中随机删除神经元的方法。<strong>训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递</strong>。</p> <p>训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，</p> <p>要乘上训练时的删除比例后再输出。</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331103332755.png" style="zoom:70%;"> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment">#Dropout</span>
<span class="token keyword">class</span> <span class="token class-name">Dropout</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>dropout_ratio <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dropout_ratio <span class="token operator">=</span> dropout_ratio
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token boolean">None</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>train_flag<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> train_flg<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">&amp;</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">&gt;</span> self<span class="token punctuation">.</span>dropout_ratio
            <span class="token keyword">return</span> x <span class="token operator">*</span> self<span class="token punctuation">.</span>mask
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> x <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>dropout_ratio<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>mask
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>每次正向传播时，self.mask中都会以False的形式保存要删除的神经元。<strong>self.mask会随机生成和x形状相同的数组，并将值比</strong></p> <p><strong>dropout_ratio大的元素设为True</strong>。反向传播时的行为和ReLU相同。也就是说，正向传播时传递了信号的神经元，反向传播时按原样传递信号；正向传播时没有传递信号的神经元，反向传播时信号将停在那里。</p> <p>依然是对minist数据集进行实验</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331104704128.png" style="zoom:70%;"> <blockquote><p>通过使用Dropout，训练数据和测试数据的识别精度的<strong>差距变小了</strong>。并且，<strong>训练数据也没有到达100%的识别精度</strong>。像这样，通过使用Dropout，<strong>即便是表现力强的网络，也可以抑制过拟合</strong></p></blockquote> <h2 id="超参数的验证"><a href="#超参数的验证" class="header-anchor">#</a> 超参数的验证</h2> <p>神经网络中，除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。这里所说的超参数是指，比如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值，模型的性能就会很差。</p> <blockquote><p>人工设置的值就叫超参数</p></blockquote> <h3 id="验证集"><a href="#验证集" class="header-anchor">#</a> 验证集</h3> <p>之前我们使用的数据集分成了训练数据和测试数据，训练数据用于学习，测试数据用于评估泛化能力。由此，就可以评估是否只过度拟合了训练数据（是否发生了过拟合），以及泛化能力如何等。</p> <p>那能不能用测试数据评测超参数的性能呢？<strong>不能</strong></p> <p><strong>因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合</strong>。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。</p> <p>调整超参数时，必须使用超参数专用的确认数据。<strong>用于调整超参数的数据，一般称为验证数据（validation data）</strong>。我们使用这个验证数据来评估超参数的好坏</p> <blockquote><p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（<strong>比较理想的是只用一次）测试数据</strong>。</p></blockquote> <p><strong>怎么分割验证集</strong></p> <p>根据不同的数据集，有的会事先分成训练数据、验证数据、测试数据三部分，有的只分成训练数据和测试数据两部分，有的则不进行分割。在这种情况下，用户需要自行进行分割。如果是MNIST数据集，获得验证数据的最简单的方法就是从训练数据中事先分割20%作为验证数据，代码如下</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 打乱训练数据</span>
x_train<span class="token punctuation">,</span> t_train <span class="token operator">=</span> shuffle_dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span>
<span class="token comment"># 分割验证数据</span>
validation_rate <span class="token operator">=</span> <span class="token number">0.20</span>
validation_num <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> validation_rate<span class="token punctuation">)</span>
x_val <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token punctuation">:</span>validation_num<span class="token punctuation">]</span>
t_val <span class="token operator">=</span> t_train<span class="token punctuation">[</span><span class="token punctuation">:</span>validation_num<span class="token punctuation">]</span>
x_train <span class="token operator">=</span> x_train<span class="token punctuation">[</span>validation_num<span class="token punctuation">:</span><span class="token punctuation">]</span>
t_train <span class="token operator">=</span> t_train<span class="token punctuation">[</span>validation_num<span class="token punctuation">:</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p><strong>这里，分割训练数据前，先打乱了输入数据和教师标签。这是因为数据集的数据可能存在偏向</strong>（比如，数据从“0”到“10”按顺序排列等）。这里使用的shuffle_dataset函数利用了np.random.shuffle</p> <h3 id="超参数最优化"><a href="#超参数最优化" class="header-anchor">#</a> 超参数最优化</h3> <p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。</p> <p>所谓逐渐缩小范围，<strong>是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估</strong>；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。</p> <blockquote><p>可以说就是暴力求最优的解</p> <p>在进行神经网络的超参数的最优化时，与网格搜索等有规律的搜索相比，<strong>随机采样的搜索方式效果更好</strong>。这是因为在多个超参数中，各个超参数对最终的识别精度的影响程度不同</p></blockquote> <p>超参数的范围只要“大致地指定”就可以了。所谓“大致地指定”，是指像0*.*001（10^(-3) ）到1000（10^3 ）这样，以“10的阶乘”的尺度指定范围（也表述为“用对数尺度（log scale）指定”）。</p> <p><strong>总结步骤如下</strong>：</p> <p>步骤<strong>0</strong></p> <p>​	设定超参数的范围。</p> <p>步骤<strong>1</strong></p> <p>​	从设定的超参数范围中随机采样。</p> <p>步骤<strong>2</strong></p> <p>​	使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。</p> <p>步骤<strong>3</strong></p> <p>​	重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。</p> <p><strong>更精炼的优化方法</strong></p> <blockquote><p>如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine LearningAlgorithms”[16]等。</p></blockquote> <h3 id="还是以minist为例"><a href="#还是以minist为例" class="header-anchor">#</a> 还是以minist为例</h3> <p>我们使用MNIST数据集进行超参数的最优化。这里我们将<strong>学习率和控制权值衰减强度的系数</strong>（下文称为“权值衰减系数”）这两个超参数的搜索问题作为对象</p> <p>如前所述，通过从 0*.*001（10^<em>−</em>3 ）到 1000（10^3 ）<strong>这样的对数尺度的范围中随机采样进行超参数的验证</strong>。这在Python中可以写成10  np.random.uniform(-3, 3)。在该实验中，权值衰减系数的初始范围为10^−8 到10^−4 ，学习率的初始范围为10^-6 到10^−2 。此时，超参数的随机采样的代码如下所示。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>weight_decay <span class="token operator">=</span> <span class="token number">10</span> <span class="token operator">**</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>
lr <span class="token operator">=</span> <span class="token number">10</span> <span class="token operator">**</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>实验结果举例:</p> <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230331111137074.png" style="zoom:70%;"> <blockquote><p>显然，这里由高到低排列了结果</p></blockquote> <p>从上面可以发现，Best6之后效果不怎么好。<strong>直到“Best-5”左右，学习进行得都很顺利</strong>。因此，我们来观察一下“Best-5”之前的超参数的值（学习率和权值衰减系数），结果如下所示</p> <div class="language-shell line-numbers-mode"><pre class="language-shell"><code>Best-1 <span class="token punctuation">(</span>val acc:0.83<span class="token punctuation">)</span> <span class="token operator">|</span> lr:0.0092, weight decay:3.86e-07
Best-2 <span class="token punctuation">(</span>val acc:0.78<span class="token punctuation">)</span> <span class="token operator">|</span> lr:0.00956, weight decay:6.04e-07
Best-3 <span class="token punctuation">(</span>val acc:0.77<span class="token punctuation">)</span> <span class="token operator">|</span> lr:0.00571, weight decay:1.27e-06
Best-4 <span class="token punctuation">(</span>val acc:0.74<span class="token punctuation">)</span> <span class="token operator">|</span> lr:0.00626, weight decay:1.43e-05
Best-5 <span class="token punctuation">(</span>val acc:0.73<span class="token punctuation">)</span> <span class="token operator">|</span> lr:0.0052, weight decay:8.97e-06
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>从这个结果可以看出，学习率在0*.<em>001到0</em>.<em>01、权值衰减系数在10</em>−<em>8 到10</em>−*6 之间时，学习可以顺利进行**。像这样，观察可以使学习顺利进行的超参数的范围，从而缩小值的范围。然后，在这个缩小的范围中重复相同的操作。**这样就能缩小到合适的超参数的存在范围，然后在某个阶段，选择一个最终的超参数的值。</p></div></div>  <div class="page-edit"><div class="edit-link"><a href="https://github.com/flokken/edit/master/docs/AI/30.深度学习/03.鱼书-深度学习入门/06.神经网络的学习技巧.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2023/04/06, 11:58:54</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/81deb7/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">误差反向传播</div></a> <a href="/pages/88621c/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">CNN介绍</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/81deb7/" class="prev">误差反向传播</a></span> <span class="next"><a href="/pages/88621c/">CNN介绍</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/fd9dbf/"><div>
            线程池
            <!----></div></a> <span class="date">09-16</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/64ddd0/"><div>
            消息队列基础知识
            <!----></div></a> <span class="date">09-08</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/4b5092/"><div>
            树状数组和线段树
            <!----></div></a> <span class="date">08-22</span></dt></dl> <dl><dd></dd> <dt><a href="/archives" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="mailto:2878846959@qq.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/Flokken" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="https://music.163.com/#/my/m/music/playlist?id=807177837" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2023-2024
    <span>flokken | <a href="https://github.com/xugaoyi/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <div class="body-bg" style="background:url() center center / cover no-repeat;opacity:0.5;"></div> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.6c1ebbcf.js" defer></script><script src="/assets/js/2.28dcc766.js" defer></script><script src="/assets/js/46.342f85bd.js" defer></script>
  </body>
</html>
