---
title: 分布式共识
date: 2023-10-15
tags: 
  - 深入理解分布式系统
categories: 
  - 大数据
  - 深入理解分布式系统
---

> 这里内容主要摘录自《深入理解分布式系统》一书，并且只记录了个人认为最核心的部分
>
> [paxos英文论文](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)
>
> [paxos中文翻译](https://segmentfault.com/a/1190000037628341)
>
> [Paxos、Raft分布式一致性最佳实践](https://www.zhihu.com/column/paxos)
>
> [用 Raft 的方式理解 Multi-Paxos - 多颗糖的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/278054304)（此书作者，其专栏内容都非常好）

## 概念介绍

**什么是分布式共识 ？**  

分布式共识指的是分布式系统中的节点面对一些情况（比如leader选举，日志分歧）时如何达成共识的过程与方法。

> 比如raft，paxos算法也就是一种共识。

**为什么需要共识？**

我们知道分布式系统中存在着网络分区，节点故障和时钟不一致等问题。

那么我们有什么解决办法呢？在分布式系统领域，**状态机复制**（State Machine Replication ,SMR）是用于解决上面问题的常规方法。

> 状态机复制也称为**复制状态机**（Replicated State Machine），或者**多副本状态机**，常常混用这些称呼

#### 复制状态机

**状态机：**包括一组状态，一组输入，一组输出，一个转换函数，一个输出函数和一个“初始”状态。

**状态机最重要的是状态机具有确定性：**多个相同的状态机的副本，从同样的初始状态开始，经过相同的输入序列，会到达相同状态，输出同样结果。

可以定义状态机伪码：

~~~
//初始状态
state = init
log =[]
while true:
#客户输入,cmd就是输入
	on receiving cmd from a client:
		log.append(cmd)
		#生成新的状态和输出
		state,output = apply(cmd,state)
		send output to the client
~~~

**正是由于状态机的确定性，我们可以通过它来实现分布式系统中节点的一致性。**

> 状态机复制是理论上的东西，对应系统中其实就是多副本日志

具体来说：我们的分布式系统中，每个节点都有复制状态机，这些复制状态就对于相同的输入，产生相同的输出，到达一致的状态

**实现状态机复制常常需要实现一个多副本的日志（Replicated log）系统，**原因来自于工程经验：

- 如果日志的内容和顺序都相同，多个进程从同一状态开始，并且从相同的顺序读取日志，那么这些进程产生相同的输出，并且结束在相同的状态。

**共识算法常用来实现多副本日志。**

共识算法使得分布式系统中的每个副本（就是每个节点的日志副本），对日志的值和顺序达成共识，**也就是说每个节点存储相同的日志副本**，这样整个系统中的每个节点都能有一致的状态和输出。

> 这样这个分布式系统对外部来说就是一个单独的，高可用的状态机。

### 共识算法能解决的问题

之后会学习的raft以及paxos等算法，**除了可以解决一开始提到的：网络分区，时钟不一致，节点故障等问题。**

还可以解决一些经典问题：

- 互斥（Mutual Exclusion）：分布式系统中的进程到临界区访问资源，怎么保证访问顺序以及进程安全？其实就是分布式锁？
- 选主（Leader Election）：在单主复制的数据库系统中，如果master出现故障，那么剩下的节点就需要选一个新的master，那么如何让所有的节点在master节点选举上达成共识？
- 原子提交（Atomic Commit）:对于跨多节点或者多分区事务的数据库，一个事务可能在一些节点上成功，一些节点上失败。如果我们需要维护这种分布式事务的一致性，必须让所有节点对事务的结果达成共识：要么全部提交，要么全部中止。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           

### 异步系统中的共识

分布式系统可以分为同步，异步和部分同步系统，其中异步系统更接近现实

>复习一下
>
>基于时间或者是否同步（Synchronous),可以将系统分为同步系统模型和异步模型（Asynchronous）。
>
>同步系统模型（Synchronous）：一个消息的响应时间在有限且已知的范围内
>
>异步（Asyncchronous）系统模型指的是，一个消息的响应时间是无限的，无法知道一条消息什么时候到达。

**FPL不可能定理**

1985年的一篇论文证明了，一个完全异步的分布式系统中，即使只有一个系统出现故障，也不存在一个算法能达成共识。**这是一个经典结论。**

> 简单来说，在一个异步系统中，进程可以在任意时间返回响应，因此我们无法分辨一个节点是已经崩溃还是速度很慢。这导致我们无法在有限时间内达成共识

从这个定理出发，研究人员发现一个分布式共识算法应该具有两个属性：

- 安全性：所有正确的进程最终认同同一个值
- 活性：分布式系统最终会认同同一个值

FTP应该这么理解：在一个完全异步的系统中，不能同时得到容错性，安全性和活性（但可以同时得到其中两个）

理论上FTP定理虽然误解，但是工程上有解，可以绕过FTP定理。

> 主要是思路是异步系统不能达成共识，那就想办法把他变成同步的

- 故障屏蔽：如果一个进程（节点）有故障，那么他最终是会恢复的（人为或自动），并且会重新加入分布式系统。常见的一种方式是一个进程崩溃，那么他会重启，并且在故障之前；他已经持久化了足够的信息，那么其奔溃后重启也可以再正常工作。（已经广泛应用于各种系统）
- 使用故障检测器：进程可以通过某种检测器检测其是否故障。比如常见的实现是超时故障检测器，一个进程在一段时间内没有响应，就认为他已经故障了。

### 同步系统中共识

上面在异步系统中达成共识在工程上的解，都是想办法把异步转成同步系统（或者部分同步），那么肯定是因为同步系统中本来就可以实现共识算法。

> 这是有理论支撑的，Dolev-Strong算法证明了：在同步系统中，有不超过f个进程发生故障（其他节点正常），且错误进程数量f小于总进程数N，那么经过f+1次消息传递即可达成共识。

**实际生产活动中企业生产环境的共识算法，都依赖于部分同步系统的假设。同步系统中的共识算法也是重点**

共识算法分为拜占庭容错算法和非拜占庭容错算法。

大部分企业的系统运行在自己内部的安全的服务器上，所以大部分不需要考虑拜占庭容错。

> 拜占庭容错算法运行代价更高

**paxos和raft都是非拜占庭容错的算法，也是讨论的重点。**

## Paxos

Paxos算法时Leslie Lamport在1989年提出的共识算法

> 拜占庭问题也是他提出来的

### 基本概念（Basic Paxos）

**Paxos算法解决的问题是分布式一致性问题，即一个分布式系统中的各个进程如何就某个值（决议）达成一致。**

Paxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。

paxos通过一个故事展开。首先把paxos岛上的虚拟故事对应到分布式系统上，议员就是各个节点，制定的法律就是系统中的各个状态。每个节点（议员）都可能提出提案（Proposal）,Paxos用提案推动算法运行，使系统得出同一个提案。

提案包括提案编号(Proposal Number)和提案值(Proposal Value)。

>Paxos问世以来就持续垄断了分布式一致性算法，Paxos这个名词几乎等同于分布式一致性。Google的很多大型分布式系统都采用了Paxos算法来解决分布式一致性问题，如Chubby、Megastore以及Spanner等。开源的ZooKeeper，以及MySQL 5.7推出的用来取代传统的主从复制的MySQL Group Replication等纷纷采用Paxos算法解决分布式一致性问题。

#### **大多数原则**

在一个分布式系统中，需要对某种事达成共识时，需要超过半数的节点赞成。

在paxos中，对于一个提案，需要集群中的超过半数的节点都同意这个提案

> 对于有奇数个n节点的分布式系统，至少要n/2+1的节点赞成，也就是向上取整

**注意，一旦一个提案被接受，提议者都必须将该值作为后续提案值**

#### 角色

Paxos算法中分布式系统节点的角色。

- 客户端：客户端向分布式系统中发送一个请求，并等待响应。例如对于一个分布式数据库，客户端请求执行写操作
- 提议者：提议者收到客户端的请求，提出相关的提案。
- 接受者：也叫投票（Acceptor）：也叫投票者（Voters）,即投票接受或拒绝提议者的提案，若超过半数的接受者接受提案，则提案被批准。
- 学习者：学习者只能“学习”被批准的提案，不参与决议提案。一旦客户端的请求得到接受者的同意，学习者就可以学习到提案者，执行其中的请求并向客户端发送响应。

#### 时钟

Basic paxos强调：一旦一个值被批准，未来的提案必须使用相同的提案值。也就是说paxos只是批准某个提案值（达成一致），但是还需要设计一个两阶段（2-phase）协议，将已经批准的值告诉后续请求，以便使用相同的提案值。

因此，提案需要一个顺序，但是分布式系统中，直接用现实时间来定义提案的先后是不行的（因为每个服务器的时间可能有误差）。

paxos中通过给每一个提案附加一个提案编号，来唯一确定一个提案，如`<n,serverId>`,其中`n`是轮次（**保证其单增**），`serverId`是服务器编号。通过n就可以判断提案先后。

> 当然，为了保证容错性，这个提案编号必须保证持久化存储

### 问题描述

我们在分布式存储系统中经常使用多副本的方式实现容错，每一份数据都保存多个副本，这样部分副本的失效不会导致数据的丢失。每次更新操作都需要更新数据的所有副本，使多个副本的数据保持一致。那么问题来了，如何在一个可能出现各种故障的异步分布式系统中保证同一数据的多个副本的一致性 (Consistency) 呢？

常见的一种方式是主从复制来提高可用性和实现高性能（当然是为了一致性），即用一个主节点来写，然后复制到各个从节点。但是主从复制有一个问题就是单点故障，一旦主节点出现故障，需要手动进行故障处理，此时服务不可用；如果自动处理故障，可能导致出现多个主节点而导致数据不一致。

**因此，主从复制是不能同时保证数据的一致性和可用性的。**

> CAP定理，不能同时满足可用性，一致性和容错

而Paxos可以在一致性和可用性之间取得了较好的平衡。

### Paxos算法流程

>这里先讨论Basic Paxos，其只讨论如何决议出一个共识的值，并且这个值之后都使用。

Basic Paxos 主要分为第两个阶段，第一阶段（分为a,b）和     第二阶段（分为a,b）分别对应两轮RPC调用。而每个阶段的a和b则分别对应RPC的请求和响应阶段

> **有些文章里还有第三个阶段，即学习者学习提案，但显然不是核心。**
>
> **远程过程调用**（Remote Procedure Call ,RPC）是一个计算机通信协议，常用与分布式计算。该协议运行一台计算机程序时调用另一个地址空间（比如另一个计算机）的程序，就像调用本地程序一样，无需额外关注网络等细节。
>
> RPC是一种CS（Client/Server）架构，通过发送请求和接收响应进行交互。

![image-20231017155034842](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231017155034842.png)

<center> paxos算法流程</center>

#### 第一阶段

第一阶段的发送RPC请求阶段称为（phase 1a）,**也叫prepare阶段**。

该阶段提议者（promiser）接收到客户端的请求后，选择以一个最新的提案编号n，**向超过半数的接受者（acceptor）广播prepare，请求接受者对提案进行投票**。

第一阶段响应RPC请求阶段称为（phase 1b）,**也叫promise阶段**。接受者（acceptor）两种返回情况：

- 如果prepare消息中的提案编号n大于之前接收的所有提案编号，则返回promise进行响应，**并且承诺不会再接收编号小于n的提案。同时还要返回上一次接收的提案编号和值（如果有）**
- 如果n小于接受者之前接收的n，忽略请求或者恢复拒绝

同时，为例故障恢复，**接受者需要持久化存储已接受的最大提案编号（记为max_n）、已接受的提案编号（记为acceptd_n）、已接受的提案值（accepted_Value）**。

#### 第二阶段

第二阶段的发送RPC请求阶段称为（phase 2a）,**也叫Propose或者Accept阶段**。

如果提案者接收到超过半数的接受者的Promise（）响应，**那他就要向接受者发送Accept(n,value)请求，也就是带上了提案编号和值**

**注意：**

- 这里如果之前接收者的Promise（）返回了accepted_Value，**那么使用提案编号最大的值作为提案值**
- 如果没有，提案者自由决定提案值（也就是使用客户端发来的值）

- 提案者发送的Accept（）请求不是只能发给之前有回应的接受者，也可以发给没有回应的接受者。实际上这里提议者的Accept（）是广播给多数派接受者
- **接受者处理Acceept（）请求时，需要更新提案编号max_n的值，否则将导致集群可能接受不同的提案**

第二阶段的响应RPC请求阶段称为（phase 2b）,**也叫Accepted阶段**。接受者收到Accept（）请求后：

- 如果接受者没有另外 承诺（promise）提案编号比n大的提案，则更新承诺的提案编号，保存已经接受的提案。

总结来说如下：

1. 取一个Proposal ID n，为了保证Proposal ID唯一，可采用编号n（单增）+Server ID生成；
2. Proposer向所有Acceptors广播Prepare(n)请求；
3. Acceptor比较n和minProposal，如果n>minProposal，minProposal=n，并且将 acceptedProposal 和 acceptedValue 返回；
4. Proposer接收到过半数回复后，如果发现有acceptedValue返回，将所有回复中acceptedProposal最大的acceptedValue作为本次提案的value，否则可以任意决定本次提案的value；
5. 到这里可以进入第二阶段，广播Accept (n,value) 到所有节点（**这里可能也不是全部，暂不清楚**）；
6. Acceptor比较n和minProposal，如果n>=minProposal，则acceptedProposal=minProposal=n，acceptedValue=value，本地持久化后，返回；否则，返回minProposal。
7. 提议者接收到过半数请求后，如果发现有返回值result >n，表示有更新的提议，跳转到1；否则value达成一致。

### 案例

下面是具体的例子

情况一：提案已被批准

**这个情况是提案已经被批准后，后续的提案会继续使用上一次的提案值**

> 我的困惑，这里X是先来的请求，Y是后来的请求，正常来说应该是Y是系统最终的值，但是根据Paxos却是X是哦最终的共识，这样对吗？

![image-20231017164941255](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231017164941255.png)

**图中P代表Prepare阶段，A代表Accept阶段。3.1代表Proposal ID为3.1，其中3为提案编号，1为Server ID。X和Y代表提议Value。**

上图具体流程是：

- S1收到客户端的X写请求，于是S1向S1，S2，S3发起Prepare(3.1)请求，请求接受者对提案编号为3.1的提案进行投票。接受者们直接没有接受提案，所以只返回了Promise()。
- S1继续向S1,S2,S3发送Accept（3.1,X）请求，**三个节点都接受，因为超过一半的节点接受，所以该提案被批准。**
- 之后S5收到客户端Y写请求，并向S3,S4,S5发送Prepare(4.5)请求。这里注意提案编号4>3,并且上个（3.1 X）意见被批准，接受者S3还会返回包含提案值X的Promise()响应
- S4根据S3的响应，将提案值Y换成X，然后S5继续向S3,S4,S5发送Accept（4.5,X）请求，**三个节点都接受，因为超过一半的节点接受，所以该提案被批准。**

> <font color="red" size="3">注意接受是单个接受者对提案的状态，批准是整个系统通过了这个提案（超过半数接受）。</font>

情况二：提案被接受，提议者可见

**这个情况是提案已经只是被S3接受，但是还没被批准**

P 3.1没有被多数派Accept（只有S3 Accept），但是接受者S3还会返回包含提案值X的Promise()响应（因为接受了）

所以S5将提案的Value由Y替换为X，继续向S3,S4,S5发送Accept（4.5,Y），**最终所有接受者对X达成共识**。

> 该情况最终状态和情况一没有区别

![image-20231017171641595](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231017171641595.png)

情况三：提案被接受，提议者不可见

**这个情况是提案只被S1接受，但是S1没有返回响应，因为S5没向他请求**

所以S5没有收到上一次的提案值和编号（Prepare阶段），因此S5自行决定提案值为Y，并且第二阶端发送Accept(4.5,Y)请求。

![image-20231017172157598](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231017172157598.png)

**这个时候，S3承诺的提案编号最大值由3变成了4，并且4>3，所以S3没有接受S1后续的Accept(3.1 X),而是接受了S5的Accept(4.5,Y)请求**

最终S1,S2的提案值为X，但是S3,S4,S5都为Y，**根据大多数原则，最终系统批准的是Y**

### 活锁

Paxos算法可能形成活锁而永远不会结束，如下图实例所示：

![image-20231017173541347](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231017173541347.png)

具体来说，提议者在phase 1a阶段发出Prepare(),还没发送phase 2a的Accept消息时；第二个提议者又发出编号更大的Prepare();

因为接受者不再应答Proposal ID小于等于当前请求的Prepare请求，如果一直是上面两种情况交替出现，就会一直锁下去。

> 就是一直有提议者在上一个提议者发出Accept()请求前，提出编号更大的Prepare

## Multi-Paxos

共识算法通过一个复制的日志实现状态机复制，而Paxos算法就是用来进行状态机复制的。但是Basic Paxos算法只能选出一个提案，而日志里显然不会只有一个值。

因此，我们可以设想**对日志的每一条记录单独运行一次Paxos算法来决议得出其中的值，重复运行Paxos算法即可创建一个日志的多份副本，从而实现状态机复制。**

> **每条日志在此书中有多种称呼：一条日志条目（Log Entry），一条日志记录（Record）或者一条日志。他们都是一个意思**

一个Paxos算法的实例可以决议一条目录，多个Paxos可以并行运行来实现日志复制。但是如果每一条日志都通过一个Paxos实例达成共识，那么每次都需要至少两轮通信（paxos的两阶段）。这样会产生大量的网络开销。

**因此实际上Basic Paxos会进行一系列的优化来提升性能，称为Multi-Paxos。Multi-Paxos的目标就是高效地实现状态机复制。**

> 注意高效两个字，Basic算法虽然可以实现状态机复制，但是效率低。

接下来看怎么从Paxos一步步到Multi-Paxos算法。

### 确定日志索引

日志中有多条记录，如果要通过一个Paxos实例确定这一条条记录，那么Paxos实例必须知道当前在写哪条记录(第几条日志)。

**因此Multi-Paxos的第一个调整就是添加提供日志索引index参数到Basic Paxos的第一阶段和第二阶段，用来表示某一轮Paxos正在决策哪一条日志目录。**

具体流程是，首先是当提议者收到**客户端带有提案值**的请求时：

1. 找到第一个没有被批准的日志条目的索引，记为index。

2. 运行Basic Paxos算法，对index位置处的日志用客户端请求的提案值作为提案。

3. 第一阶段接受者返回的响应是否包含已接受的值`acceptedValue`？如果有已经接受的值，则用`acceptedValue`作为本来Paxos的提案值，然后回到1寻找下一个未被批准的日志条目的索引；否则继续运行Paxos算法，继续尝试批准客户端的提案值。

> 其实相比之下就是多了一个找index。
>
> 不过这里可以思考，如果使用了`acceptedValue`,那么客户端传来的值岂不是就没了？
>
> 实际上，提议者会把客户端给的提案值往后移，当没有收到接受者发来`acceptedValue`时，才把这个提案值写入。

**举例：**

对于状态机来说，提案值就是一条命令（日志的每条记录就是一个个命令）。服务器上得的每个日志条目可能存在三种状态：

- 已经保存并已知被批准的日志条目，例如S1中边框加粗的1,2,6
- 已经保存但不知道批准没有的日志条目,例如S1中的3索引处的cmp。（这里虽然系统中有两台都是cmp，但是S1不知道，所以还不算被批准）
- 空的记录，比如S1的4,5日志

![image-20231026164620541](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231026164620541.png)

这里顺便考虑一下容错，我们知道是三个节点的系统最多支持一个节点出错，**这里假设S3已经宕机或者由于网络分区与集群隔离。**

**当S1收到客户端发来提案值为jmp的请求时(S1是提议者)**，流程是：

1. 服务器先去找第一个还没有被批准的日志目录，所以S1找到了第3个日志条目（该体制被接受，但没有被批准）
2. S1提议将jmp作为第3条目录的值来运行Paxos算法
3. 但是S1已经接受了cmp，因此S1在phase 1b消息中返回`acceptedValue`为cmp，所以S1不能用jmp当提案值。**而是用cmp当提案值跑完这次的paxos。**此时S2的第3条目录也会是cmp。所以根据大多数原则，**S1的cmp变为已经批准。**
4. S1上的提议者继续找下一个没有被批准的位置，所以找到了第4个日志条目
5. 虽然S1的4是空的，但是S2的4已经接受了sub,所以同样的phase 1b 阶段，返回了sub，所以最后批准的是sub，然后S1继续找index，找到了5.
6. 因为S3已经出了故障，所以S1这次没有收到`acceptedValue`，**所以成功运行paxos算法后，5位置被批准的命令是jmp，S1向客户端返回成功的消息**。

> 提议者同时也是接受者，也会有`acceptedValue`，这里注意

**注意：**

这个系统可以并行处理多个客户端请求，比如服务器S1知道第3,4,5,7这几个位置的日志是未批准的，可以直接尝试把收到的4个命令并行写入这四个位置。

但是状态机执行日志的命令时，还是得按照日志的输入顺序逐一串行输入，所以日志里的每条命令都必须保证是已批准。

**问题：**

提议者很可能需要多次执行Paxos算法才能成功批准一条命令。当所有提议者并行工作时，由于提议者的断断续续的提交提案会有更多的冲突，需要更多轮的RPC才能达成共识。并且最优情况下，也就是只执行一次Paxos，也需要两轮RPC。

针对提案冲突和消息轮次过多的问题，Multi-Paxos有以下两种解决方案：

1. 领导者（Leader）选举。从多个提议者中选择一个领导者，**任意时刻只有一个领导者来提交提案，这样可以避免提案冲突。并且leader故障了可以马上再选一个，所以也不存在单点故障。**
2. 减少一阶段的请求。有了领导者之后，**由于提案值都是从领导者这里提出，实际上可以保证提案编号是单调递增的，因此只需要对整个日志发送一次一阶段的请求，后续可以直接通过二阶段发送提案值，使得日志被批准。**

### 领导者选举

领导者选举在分布式系统中经常使用，不仅仅是paxos里。

领导者选举有很多方法，**lamport选用一种很简单的方法：让服务器id最大的节点当领导者。**

> 提案编号用轮次n和服务器编号server_id组成，这种选举方法就是选server_id最大的节点

具体流程：

1. 首先每台服务器要知道其他服务器的server_id,才能知道谁的最大
2. 每隔T毫秒，每台服务器其他所有服务器发送心跳信息，交换彼此的`server_id`
3. 如果一个节点在2T毫秒之后没有收到比自己server_id更大的心跳，他就成为leader。同时意味着他需要处理客户端请求并同时也当
4. 如果一个节点收到比自己的server_id大的心跳，他就会拒绝客户端请求或者把他重定向到领导者。

注意，这种方式下，小概率有两个节点server_id一样，那么他们会同时成为领导者，但是不影响正确性。

> 此时就是普通的paxos算法

该方法问题很明显：

如果领导者（server_id）的日志落后其他节点很多，那么就会导致可能需要很长的时间来补齐日志，期间导致服务暂停较长时间

### 减少请求

Multi-Paxos想去掉第一阶段，那我们首先来看paxos的第一阶段的作用和改进想法。

> 其实就是想压缩prepare请求，来减少rpc调用的次数。

- 屏蔽过期提案。这一步里paxos**需要保证当前的提案编号是最新的**。但是由于每个日志条目的paxos实例是相互独立的，所以每次请求只能屏蔽一个日志条目的提案，对于后面位置的日志信息并不清楚。

  - 改进： 改变提案编号的意思。现在的提案编号被全局化，表示所有的日志（**因为整个日志使用同一个单调递增的编号**），而不是为每个日志记录都保留独立的提案编号。这样的话就只需要一次prepare请求就可以prepare所有的日志了

- 用已经接受的提案值来替代原本的提案值。当多个提议者并发提案时，需要保证新提案的提案值与已接受的提案值相同。

  - 改进：增加接受者的响应信息来表示接受者没有要返回的已接受提案，那么leader可以直接发起第二阶段的请求。不过如果接受者有，对当前的preapre(n)中的提案编号，acceptor会返回其收到过的n以下最大编号的提案。

    其次，acceptor还会检查n以后的日志，如果之后的日志都没有通过任何提案，说明之后的日志都是空的，即可用的，acceptor就会在回复prepare(n)时携带一个noMoreAccepted


 如果使用了选举leader的机制，最终leader会收到大部分acceptor返回的noMoreAccepted。**这表示这些acceptor在n号日志之后的内容都是空的，leader不需要再发送多余的prepare请求了。**

> 参考https://zhuanlan.zhihu.com/p/341122718

### 日志全复制

到现在为止，日志并没有完整的复制到所有的服务器：

- 只有大多数的服务器中有通过的日志，但应该让所有的节点的日志具有一致性
- 只有领导者知道哪些日志是已经批准了的

优化方案：

1. 领导者在收到超过半数的接受者回复后可以继续处理请求，但是会一直重复accpet给没有收到回复的接受者

> 但是可能存在问题，比如领导者在重试时宕机

2. 追踪已经批准的日志

需要增加两个变量：

- `acceptedProposal`是一个数组，`acceptedProposal[i]`代表第i条日志的提案编号，如果被批准，则`acceptedProposal[i]=∞`。（因为被批准的提案的位置不会接受另外的提案。）

- 每个节点都维持一个`firstUnchosenIndex`变量，表示第一个没有被批准的日志索引，也就是第一个`acceptedProposal[i]!=∞`的位置。

举例：

下图表示同一个 Acceptor 节点 Accept 请求前后的 `acceptedProposal`。该 Acceptor 在 Accept 请求之前的第 6 位的提案编号为 3.4，这时它收到一个提案编号也为 3.4 的 Accept 请求，并且请求的 firstUnchosenIndex = 7，大于之前 3.4 所在的 6，所以**将选中第 6 位，同时因为该请求的 index = 8，acceptedProposal[8] == 3.4。**

![image-20231027112752424](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231027112752424.png)

通过前面的两步，基本所有的接受者已经能最终知道已经批准的日志目录。但是还需要考虑领导者切换的情况。

3. 接受者的目录中可能有前任领导者留下的提案，但是前任领导者没有完成日志复制或者批准就宕机了。

- 此时Acceptor 将其 firstUnchosenIndex 作为 Accept 请求的响应返回给 Proposer

- Proposer （也是Leader）判断如果 Acceptor.firstUnChosenIndex < Proposer.firstUnChosenIndex，则在后台（异步）发送 Success(index, v) RPC

- Acceptor 收到 Success RPC 后，更新已经被 chosen 的日志记录：

  - acceptedValue[index] = v

    acceptedProposal[index] = 无穷大

    return firstUnchosenIndex

    如果需要(可能存在多个不确定的状态)，Proposer 发送额外的 Success RPC

**这三个优化步骤就可以确保所有的 Acceptor 都最终知道 chosen （批准）的日志记录。在一般的情况，并不需要额外的第 3步，只有在 Leader 切换时才可能需要第 3 步。**

> 参考：Paxos 的变种（一）：Multi-Paxos 是如何劝退大家去选择 Raft 的 - 多颗糖的文章 - 知乎 https://zhuanlan.zhihu.com/p/266580107

### **客户端请求**

接下来需要考虑客户端如何与系统交互。

首先，当客户端第一次请求时，并不知道谁是 Leader，**它任意请求一台服务器，如果该服务器不是 Leader，重定向给 Leader。**

Leader 直到日志记录被 chosen 并且被 Leader 的状态机执行才返回响应给客户端。

**客户端会一直和 Leader 交互，直到无法再联系上它（例如请求超时）。在这种情况下，客户端会联系任何其它服务器，这些服务器又在重定向到实际的 Leader。**

但这存在一个问题，如果请求提案被 chosen 后，Leader 在回复客户端之前宕机了。客户端会认为请求失败了，并重试请求。这相当于一个命令会被状态机执行两次，这是不可接受的。

**解决办法是客户端为每个请求增加一个唯一 id，服务器将该 id 与命令一起保存到日志记录中。状态机在执行命令之前，会根据 id 检查该命令是否被执行过。**

### **集群管理（配置变更）**

最后一个非常棘手的问题，因为集群中的节点是会变更的，包括：服务器的 id、网络地址变更和节点数量等。集群节点数量改变会影响多数派数量的判断，我们必须保证不会出现两个重叠的多数派。

![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/v2-1f21160c3016167833ef89ff32952fb7_b.jpg)

Lamport 在 Paxos 论文中的建议解决方案是：使用日志来管理这些变更。当前的集群配置被当作一条日志记录存储起来，并与其它的日志记录一起被复制同步。

![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/v2-a15bd4dc1ce39652b89f27f5bfaf047d_b.jpg)

这里看起来会比较奇怪，如图所示，第 1、3 个位置存储了两个不同的系统配置，其它位置的日志存储了状态机要执行的命令。增加一个系统参数  α 去控制当配置变更时什么时候去应用它，  **表示配置多少条记录后才能生效。**

这里假设  α = 3，意味着 C1 在 3 条记录内不生效，也就是 C1 在第 4 条才会生效， C2 在第 6 条开始生效。

  是在系统启动的时候就指定的参数。这个参数的大小会限制我们在系统可以同时 chosen 的日志条数：在 i 这个位置的值被 chosen 之前，我们不能 chosen i+α 这个位置的值——因为我们不知道中间是否有配置变更。

所以，如果 α 值很小，假设是 1，那整个系统就是串行工作了；如果 α = 3，意味着我们可以同时 chosen 3 个位置的值；如果 α 非常大， α = 1000，那事情就会变得复杂，如果我们要变更配置，可能要等配置所在的 1000 条记录都被 chosen 以后才会生效，那要等好一阵子。这时候为了让配置更快生效，我们可以写入一些 no-op 指令来填充日志，使得迅速达到需要的条数，而不用一直等待客户端请求进来。

## 完整实现

> [用 Raft 的方式理解 Multi-Paxos - 多颗糖的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/278054304)