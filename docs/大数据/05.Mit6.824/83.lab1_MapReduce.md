---
title: lab1-Mapreduce
date: 2023-10-09
tags: 
  - null
categories: 
  - 大数据
  - Mit6.824
---

> 这里做2022版的
>
> [6.824 Lab 1: MapReduce (mit.edu)](http://nil.csail.mit.edu/6.824/2022/labs/lab-mr.html)
>
> 参考：
>
> [呆呆的分布式系统杂谈](https://www.zhihu.com/column/robert-ds-talks)
>
> [MIT 6.824 分布式系统](https://www.zhihu.com/column/c_1294335950039019520)

**环境准备：**

Ubuntun20.04

首先我是本来就安装了一个1.14的go，但是要求是1.17，所以需要升级一下Go的版本.Go的升级需要自己先删除原来安装的版本，再重新下新的版本。[官方地址][https://golang.org/dl/]

> 参考:http://www.yinzhongnet.com/1114.html 这里不赘述怎么安装了。

## 实验导读

首先这个lab提供了一个`src/main/mrsequential.go`,实现了单机版的mapReduce计算。具体来说就是先去根据文件名获取文本，然后调用map得到key-value的中间文件，然后直接`sort`（单机直接sort），然后再进行Reduce进行合并。最后得到`Word_count`的输出如下：

![image-20231031210344689](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031210344689.png)

>这里稍微回忆MapReduce的核心操作。
>
>Map：一个映射函数    Reduce：合并函数   Shuffle：搬数据

总体对我们的要求就是要实现一个分布式的mapreduce计算框架，补充完成这 `mr/coordinator.go`, `mr/worker.go`, and `mr/rpc.go`三个文件。

> 这个lab里`coordinator`也是指`master`
>
> **注意实验用的RPC是基于本地Socket的，因为实验目标是在一个机器上运行，但是如果想在多个机器上运行，应该用TCP/IP协议。**

完成之后，通过`main/test-mr.sh`.可以进行测试，这样才算全部通过。

![image-20231031211406956](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031211406956.png)

###  一些规则

这里直接照搬原文的要求：

![image-20231031211525060](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031211525060.png)

### 提示

当然，作为新手，完全不会做。所以这个lab也给了一些提示（Hints）

> 通用的作业提示，包括花费时间，难度，[Lab guidance][http://nil.csail.mit.edu/6.824/2022/labs/guidance.html]

所以我们先看我们接下来就是一步步跟着提示走的。

**第一步是worker写RPC请求给master，然后master会回复一些东西。**

怎么请求呢，Go里面有RPC的包，代码里也直接给出了

`worker.go`代码里有一个`callExample()`,而`coordinator()`则有`Example()`用于回复这个RPC请求。

`worker.go`

~~~go

//
// main/mrworker.go calls this function.
//
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {

	// Your worker implementation here.

	// uncomment to send the Example RPC to the coordinator.
	// CallExample()
	CallExample()

}

func CallExample() {
   /////......................
	ok := call("Coordinator.Example", &args, &reply)
	if ok {
		// reply.Y should be 100.
		fmt.Printf("reply.Y %v\n", reply.Y)
	} else {
		fmt.Printf("call failed!\n")
	}
}
//
// send an RPC request to the coordinator, wait for the response.
// usually returns true.
// returns false if something goes wrong.
//
func call(rpcname string, args interface{}, reply interface{}) bool {
	// c, err := rpc.DialHTTP("tcp", "127.0.0.1"+":1234")
	sockname := coordinatorSock()
	c, err := rpc.DialHTTP("unix", sockname)
	if err != nil {
		log.Fatal("dialing:", err)
	}
	defer c.Close()

	err = c.Call(rpcname, args, reply)
	if err == nil {
		return true
	}

	fmt.Println(err)
	return false
}
~~~

`coordinator().go` :注意先把`ret:=true`,也就是自动重连改成`false`

~~~java
//
// main/mrcoordinator.go calls Done() periodically to find out
// if the entire job has finished.
//
func (c *Coordinator) Done() bool {
	ret := false
	//ret是重连
	// Your code here.
	return ret
}

func (c *Coordinator) Example(args *ExampleArgs, reply *ExampleReply) error {
	reply.Y = args.X + 1
	return nil
}
~~~

然后在`main`里运行`bash test-mr.sh`,就开始了测试并且看到了结果！

![image-20231102134845660](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231102134845660.png)

根据上面，我们就知道了`worker`和`master`怎么通过RPC进行通信的了。之后的内容不会这么细的记录，只记录精要的内容。

> 注意这个lab里coordinator和master混用，两者实际是一个东西

### 分析

由于之前学过paxos代码的简易实现，所以这里类比类比。MapReduce是主从架构，一个master多worker。master通过RPC和worker通信并分配任务。具体要干什么可以看论文干了什么以及rules。

怎么实现呢，无论什么程序无非数据结构加算法（也即是函数）。

所以我们分别从这两个角度分析`worker`和`master`

#### **算法/功能分析**

先复习一下`MapReduce`流程

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/fe9a03016b995b0c0581ce23d2b4c98d.jpg" alt="img" style="zoom: 50%;" />

根据上面的图，我们再来分析`Coordinator`和`worker`的功能。

**Coordinator**

在理论时我们学过，`Coordinator`需要给worker分配任务，那至少得知道worker在哪，这里可以用ID表示worker；

MapReduce是先进行Map，再Reduce。所以`Coordinator`需要能给`Worker`分配map和reduce任务。

并且还需要知道任务完成了没有，因为Map全部完成了，才能进行Reduce；Reduce完成了，说明整个作业完成了。

**总结来说，Coordinator功能有：**

- 在启动时根据指定的输入文件数及 Reduce Task 数，生成 Map Task 及 Reduce Task
  - 这个也是因为代码中定义了`MakeCoordinator`函数，所以才想到这个
- 响应 Worker 的 Task 申请 RPC 请求，分配可用的 Task（包括map/Reduce） 给到 Worker 处理
- 追踪 Task 的完成情况，在所有 Map Task 完成后进入 Reduce 阶段，开始派发 Reduce Task；在所有 Reduce Task 完成后标记作业已完成并退出。

**Worker**

work需要完成任务，并且还要返回任务的结果。

首先，**worker需要自己申请任务，只要它是空闲的，那么他就发RPC请求来请求任务，所以他要不断的向master请求任务。**

- 如果请求到了`map`任务，那么他需要实现map，并且存储`map`到本地磁盘。还要返回存储的位置（应该就是文件名）给`master`
- 如果请求到了`reduce`任务，那么他需要去读取中间结果，然后进行`reduce`操作。

**注意，这里还要求实现`woker`失败了的情况，后面补充**

#### 数据结构

**Task**

我们知道`worker`就是一个工人，只需要完成`master`分配的任务就行了。因此我们要定义`worker`和`coordinator`的之间通信的消息的数据结构，**这个lab里称为`task`。**

我们之前知道，`worker`要从`coordianator`那里拿到的信息有：

- 输入的文件名（对于论文的master告诉worker去哪找文件）
- 我要干啥，是map还是reduce，所以需要**变量表示任务类型**
  - 这里也可以用定义两个类分别表示两种任务，感觉耦合性更低
- 任务结果，任务失败了可以返回false，成功了要返回对应的东西

- 还需要向`master`返回消息以及中间文件名

**Coordinator**

我们根据上面的分析，需要维护的信息有：

- 任务信息：包括 总 MAP Task 数量、总 Reduce Task 数量
- 调度任务需要的信息，所以需要维护一个`task`池

`MapTaskState`

```go
type MapTaskState struct {
	beginSecond int64
	workerId int
	fileId int
}
```

`MapTaskArgs`

```go
type MapTaskArgs struct{
	WorkerId int
}
```

`MapTaskReply`

```go
type MapTaskReply struct{
	// worker passes this to the os package
	FileName string
	
	// marks a unique file for mapping
	// -1代表未分配或者其他状态（错误，执行完成，如果为0-n的整数时才有意义
	FileId int

	// for reduce tasks
    //因为reduce任务需要知道最终输出多少个文件
	NReduce int

	// assign worker id as this reply is the first sent to workers
	WorkerId int

	//标记任务是否全部完成
    //Map任务全部完成后进入Reduce阶段
	AllDone bool
}
```

`MapTaskJoinArgs`

```go
type MapTaskJoinArgs struct {
	FileId int
	WorkerId int
}
```

`MapTaskJoinReply`

```go
type MapTaskJoinReply struct {
	Accept bool
}
```

#### Reduce任务

`ReduceTaskArgs`

```go
type ReduceTaskArgs struct{
	WorkerId int
}
```

`ReduceTaskReply`

```go
type ReduceTaskReply struct{
	RIndex int
	NReduce int
	FileCount int
	AllDone bool
}
```

`ReduceTaskJoinArgs`

```go
type ReduceTaskJoinArgs struct {
	WorkerId int
	RIndex int
}
```

`ReduceTaskJoinReply `

```go
type ReduceTaskJoinReply struct {
	Accept bool
}
```

`Coordinator`数据结构

`Coordinator`

```go
type Coordinator struct {
	// Your definitions here.
	fileNames []string
	nReduce int
	//记录当前有多少个worker

	//因为master不知道有多少个worker，所以这里需要一个计数的变量
	curWorkerId int
	//没有分配的maptask队列，
	unIssuedMapTask *BlockQueue
	//已经分配的maptask队列，如果有task超时，需要拿出来放回unIssuedMapTask重新分配
	issuedMapTask *MapSet
	issuedMapMutex sync.Mutex

	
	unIssuedReduceTask *BlockQueue
	issuedReduceTask *MapSet
	issuedReduceMutex sync.Mutex

	mapTask []MapTaskState
	reduceTask []ReduceTaskState
	//标记任务是否完成
	mapDone bool
	allDone bool


}
```

