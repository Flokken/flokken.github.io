---
title: lab1-Mapreduce
date: 2023-10-09
tags: 
  - null
categories: 
  - 大数据
  - Mit6.824
---

> 这里做2022版的
>
> [6.824 Lab 1: MapReduce (mit.edu)](http://nil.csail.mit.edu/6.824/2022/labs/lab-mr.html)
>
> 参考：
>
> [呆呆的分布式系统杂谈](https://www.zhihu.com/column/robert-ds-talks)
>
> [MIT 6.824 分布式系统](https://www.zhihu.com/column/c_1294335950039019520)
>
> 最终代码实现：
>
>  [mit6.824](https://gitee.com/flokken/mit6.824/tree/master/src)

**环境准备：**

Ubuntun20.04

首先我是本来就安装了一个1.14的go，但是要求是1.17，所以需要升级一下Go的版本.Go的升级需要自己先删除原来安装的版本，再重新下新的版本。[官方地址][https://golang.org/dl/]

> 参考:http://www.yinzhongnet.com/1114.html 这里不赘述怎么安装了。

## 实验导读

首先这个lab提供了一个`src/main/mrsequential.go`,实现了单机版的mapReduce计算。具体来说就是先去根据文件名获取文本，然后调用map得到key-value的中间文件，然后直接`sort`（单机直接sort），然后再进行Reduce进行合并。最后得到`Word_count`的输出如下：

![image-20231031210344689](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031210344689.png)

总体对我们的要求就是要实现一个分布式的mapreduce计算框架，补充完成这 `mr/coordinator.go`, `mr/worker.go`, and `mr/rpc.go`三个文件。

> 这个lab里`coordinator`也是指`master`
>
> **注意实验用的RPC是基于本地Socket的，因为实验目标是在一个机器上运行，但是如果想在多个机器上运行，应该用TCP/IP协议。**

完成之后，通过`main/test-mr.sh`.可以进行测试，这样才算全部通过。

![image-20231031211406956](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031211406956.png)

###  一些规则

这里直接照搬原文的要求：

![image-20231031211525060](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231031211525060.png)

### 提示

当然，作为新手，完全不会做。所以这个lab也给了一些提示（Hints）

> 通用的作业提示，包括花费时间，难度，[Lab guidance][http://nil.csail.mit.edu/6.824/2022/labs/guidance.html]所以我们先看我们接下来就是一步步跟着提示走的。

**第一步是worker写RPC请求给master，然后master会回复一些东西。**

怎么请求呢，Go里面有RPC的包，代码里也直接给出了示例

具体来说就是`worker.go`代码里有一个`callExample()`,而`coordinator()`则有`Example()`用于回复这个RPC请求。这样实际上就完成了RPC通信，具体的建立连接，监听等过程已经给出,也就是`call`函数。

`worker.go`

~~~go

//
// main/mrworker.go calls this function.
//
func Worker(mapf func(string, string) []KeyValue,
	reducef func(string, []string) string) {

	// Your worker implementation here.

	// uncomment to send the Example RPC to the coordinator.
	// CallExample()
	CallExample()

}

func CallExample() {
   /////......................
	ok := call("Coordinator.Example", &args, &reply)
	if ok {
		// reply.Y should be 100.
		fmt.Printf("reply.Y %v\n", reply.Y)
	} else {
		fmt.Printf("call failed!\n")
	}
}
//
// send an RPC request to the coordinator, wait for the response.
// usually returns true.
// returns false if something goes wrong.
//意思就是这个call方法就可以建立与master的
func call(rpcname string, args interface{}, reply interface{}) bool {
	// c, err := rpc.DialHTTP("tcp", "127.0.0.1"+":1234")
	sockname := coordinatorSock()
	c, err := rpc.DialHTTP("unix", sockname)
	if err != nil {
		log.Fatal("dialing:", err)
	}
	defer c.Close()

	err = c.Call(rpcname, args, reply)
	if err == nil {
		return true
	}

	fmt.Println(err)
	return false
}
~~~

`coordinator().go` :注意先把`ret:=true`,也就是自动重连改成`false`

~~~java
//
// main/mrcoordinator.go calls Done() periodically to find out
// if the entire job has finished.
//
func (c *Coordinator) Done() bool {
	ret := false
	//ret是重连
	// Your code here.
	return ret
}

func (c *Coordinator) Example(args *ExampleArgs, reply *ExampleReply) error {
	reply.Y = args.X + 1
	return nil
}
~~~

然后在`main`里运行`bash test-mr.sh`,就开始了测试并且看到了结果！

![image-20231102134845660](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231102134845660.png)

根据上面，我们就知道了`worker`和`master`怎么通过RPC进行通信的了。

> 注意这个lab里coordinator和master混用，两者实际是一个东西

### 分析

怎么实现呢，无论什么程序无非数据结构加算法（也即是函数）。

所以我们分别从这两个角度分析`worker`和`master`

#### **算法/功能分析**

先复习一下`MapReduce`流程

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/fe9a03016b995b0c0581ce23d2b4c98d.jpg" alt="img" style="zoom: 50%;" />

根据上面的图，我们再来分析`Coordinator`和`worker`的功能。

>Shuffle操作在MapReduce中起到了将Map阶段的输出数据按键进行分组、排序和传输到Reduce阶段的对应任务节点的作用,当然我们的实现中，不涉及网络传输，因为只会让对应数据的worker去进行Reduce

**Coordinator**

在理论时我们学过，`Coordinator`需要给worker分配任务，那至少得知道worker在哪，这里可以用ID表示worker；

MapReduce是先进行Map，再Reduce。所以`Coordinator`需要能给`Worker`分配map和reduce任务。

并且还需要知道任务完成了没有，因为Map全部完成了，才能进行Reduce；Reduce完成了，说明整个作业完成了。

**总结来说，Coordinator功能有：**

- 在启动时根据指定的输入文件数及 Reduce Task 数，生成 Map Task 及 Reduce Task
  - 这个也是因为代码中定义了`MakeCoordinator`函数，所以才想到这个
- 响应 Worker 的 Task 申请 RPC 请求，分配可用的 Task（包括map/Reduce） 给到 Worker 处理
- 追踪 Task 的完成情况，在所有 Map Task 完成后进入 Reduce 阶段，开始派发 Reduce Task；在所有 Reduce Task 完成后标记作业已完成并退出。

**Worker**

work需要完成任务，并且还要返回任务的结果。

首先，**worker需要自己申请任务，只要它是空闲的，那么他就发RPC请求来请求任务，所以他要不断的向master请求任务。**

- 如果请求到了`map`任务，那么他需要实现map，并且存储`map`到本地磁盘。还要返回存储的位置（应该就是文件名）给`master`
- 如果请求到了`reduce`任务，那么他需要去读取中间结果，然后进行`reduce`操作。

**错误处理**

这个实验中要求我们对于超时的worker处理，也就是一个worker领取了任务，如果10s之内没有返回结果，可以任务是宕机了，需要把`task`重新分配给其他`worker`。

#### 数据结构

##### **Coordinator**

我们根据上面的分析，需要维护的信息有：

- 任务信息：包括 总 MAP Task 数量、总 Reduce Task 数量
- 调度任务需要的信息，所以需要维护一个`task`池

其中`mapset`就是一个map集合，`BlockQueue`就是一个带锁的双端队列。

详细可见`mr/mapset.go`和`mr/queue.go`

```go
type Coordinator struct {
	// Your definitions here.
	fileNames []string
	nReduce int
	//记录当前有多少个worker

	//因为master不知道有多少个worker，所以这里需要一个计数的变量，并且worker的id由master来定
	curWorkerId int
	//没有分配的maptask队列，
	unIssuedMapTask *BlockQueue
	//已经分配的maptask队列，如果有task超时，需要拿出来放回unIssuedMapTask重新分配
	issuedMapTask *MapSet
	issuedMapMutex sync.Mutex

	
	unIssuedReduceTask *BlockQueue
	issuedReduceTask *MapSet
	issuedReduceMutex sync.Mutex

	mapTask []MapTaskState
	reduceTask []ReduceTaskState
	//标记Map任务是否完成
	mapDone bool
    //全部任务是否完成，包括map和reduce
	allDone bool


}
```

##### **Worker**

```go
type AWorker struct {
	mapf    func(string, string) []KeyValue
	reducef func(string, []string) string

	// false on map
	// true on reduce
	//表明worker接到了哪个阶段的任务
	mapOrReduce bool

	// must exit if true
	allDone bool

	workerId int
}
```

##### **Task**

- 输入的文件名（对于论文的master告诉worker去哪找文件）
- 任务的执行时间
  - 因为我们要判断超时的任务

- 任务结果，任务失败了可以返回false

Task任务可以分为Map任务和Reduce任务，所以我们定义的时候也应该分别定义两种类型的任务。

所以我们可以分别对`MapTask`和`ReduceTask`分别定义相关的数据类型。

更详细的，我们把Map任务细分为了`JoinMapTask`的子任务,Reduce任务也细分为一个个的`JoinReduceTask`子任务。

**Map任务**

`MapTaskState`当前的Maptask的状态

```go
type MapTaskState struct {
	beginSecond int64 //开始时间
	workerId int //那个worker执行
	fileId int//我们构建一个映射关系，每个文件名对应一个唯一的id，方便使用
}
```

`MapTaskArgs`Worker发送MapTask的结构体

```go
type MapTaskArgs struct{
	WorkerId int//只需要告诉master是哪个worker即可
}
```

`MapTaskReply`代表整个大阶段的`Map`任务的数据结构

```go
type MapTaskReply struct{
	// 文件名，即实际要统计的文本的文件名称，如pg-*等等
	FileName string
	
	// 文件编号，用来区分每个文件
	// -1代表未分配或者其他状态（错误，执行完成，如果为0-n的整数时才有意义
	FileId int

	// for reduce tasks
    //因为reduce任务需要知道最终输出多少个文件
	NReduce int

	// 对于一个worker的id，是由master分配给他的，在worker第一次与master进行通信时
	WorkerId int

	//标记任务是否全部完成
    //Map任务全部完成后进入Reduce阶段
	AllDone bool
}
```

`MapTaskJoinArgs` 如果worker领取了一个map任务，这个map任务的相关信息如下

> 为什么这里还需要一个join呢？因为master发送了任务，但是对于worker来说，他完成了任务之后，还需要告知master是否成功。并且这个函数里还可以做task超时判断,如果超时了或者文件不在这个worker上，我们都应该要做出对应处理。
>
> 所以这里join的意思就是这个worker接的task是否成功的完成对应的任务。，Reduce同理

```go
type MapTaskJoinArgs struct {
    //这两个值都是master来赋给worker的
	FileId int
	WorkerId int
}
```

`MapTaskJoinReply` worker需要返回一个map任务是否成功

```go
type MapTaskJoinReply struct {
	Accept bool
}
```

**Reduce任务**

`ReduceTaskState` 当前的ReduceTask任务状态

```go
type ReduceTaskState struct {
	beginSecond int64
	workerId int
	fileId int
}
```

`ReduceTaskArgs`Worker请求ReduceTask的结构体

```go
type ReduceTaskArgs struct{
	WorkerId int
}
```

`ReduceTaskReply`代表整个大阶段的Reduce任务

```go
type ReduceTaskReply struct{
	RIndex int //ReduceTask的索引，因为我们需要找某个任务
	NReduce int //最终输出的结果文件数量
	FileCount int //文件数量
	AllDone bool
}
```

`ReduceTaskJoinArgs`一个Reducetask的相关参数

```go
type ReduceTaskJoinArgs struct {
    //这两个值也是master给worker的
	WorkerId int
	RIndex int
}
```

`ReduceTaskJoinReply `Worker回复Master的一个ReduceTask是否成功完成

```go
type ReduceTaskJoinReply struct {
	Accept bool
}
```

#### 具体代码实现

##### **Coordinator**

对于`Coordinator`来说，只是起分配作用，因此对于Map阶段和Reduce阶段得到逻辑区别不大。

首先我们需要启动一个`Coordinator`,并且开启一个server来监听`worker.go`。然后`worker`会不断的发送给`master`发送请求，然后`master`做出回复对应响应。

```go
func MakeCoordinator(files []string, nReduce int) *Coordinator {
	c := Coordinator{}

	// Your code here.

	//log.SetPrefix("coordinator: ")
	Println("making coordinator")

	c.fileNames = files
	c.nReduce = nReduce
	c.curWorkerId = 0
	c.mapTask = make([]MapTaskState, len(files))
	c.reduceTask = make([]ReduceTaskState, nReduce)
	c.unIssuedMapTask = NewBlockQueue()
	c.issuedMapTask = NewMapSet()
	c.unIssuedReduceTask = NewBlockQueue()
	c.issuedReduceTask = NewMapSet()
	c.allDone = false
	c.mapDone = false

	// start a thread that listens for RPCs from worker.go
	c.server()
	Println("listening started...")
	// 这里开启一个协程一直监督有没有超时任务
	go c.loopRemoveTimeoutMapTasks()

	// all are unissued map tasks
	// send to channel after everything else initializes
	Printf("file count %d\n", len(files))
	for i := 0; i < len(files); i++ {
		Printf("sending %vth file map task to channel\n", i)
		c.unIssuedMapTask.PutFront(i)
	}
	
	return &c
}
//
// start a thread that listens for RPCs from worker.go
//
func (c *Coordinator) server() {
	// Println("111...")
	rpc.Register(c)
	// Println("222...")
	rpc.HandleHTTP()
	//l, e := net.Listen("tcp", ":1234")
	// Println("333...")
	sockname := coordinatorSock()
	// Println("444...")
	os.Remove(sockname)
	l, e := net.Listen("unix", sockname)
	if e != nil {
		Println("listen error:", e)
	}
	go http.Serve(l, nil)
	// Println("listen started...")
}
```

然后开始将`Coordiantor`的相关任务。这里我们只讲Map阶段的逻辑，Reduce阶段逻辑类比即可。

`GiveMapTask()`是对于整个大阶段的MapTask的分配。主要逻辑是：

- 判断发送请求的worker是不是第一次请求任务，是的话赋予`id`

- 检查是否还有maptask没有做完，做完了并且一旦做完了maptask，调用`prepareAllReduceTasks()`进行Reduce的准备，并且退出。
- 如果有未分配的任务，也就是`unIssuedMapTask`不为空，就回复一个任务

```go
func mapDoneProcess(reply* MapTaskReply){
	Println("all map tasks complete, telling workers to switch to reduce mode")
	reply.FileId = -1
	reply.AllDone = true
}
func(c* Coordinator)GiveMapTask(args* MapTaskArgs, reply*MapTaskReply) error{
	//这里实现的逻辑是最后一次map任务完成时更新标记，并且立刻让master切换到Reduce任务
	if args.WorkerId == -1 {
		// simply allocate
		reply.WorkerId = c.curWorkerId
		c.curWorkerId++
	} else {
		reply.WorkerId = args.WorkerId
	}
	Printf("worker %v asks for a map task\n", reply.WorkerId)
	c.issuedMapMutex.Lock()

	//如果map任务已经全部执行完了应该直接返回，这里是为了防止什么情况呢？
	if c.mapDone {
		c.issuedMapMutex.Unlock()
		mapDoneProcess(reply)
		return nil
	}
	//切换到Reduce，所有任务完成之后。同时更新标记
	if c.unIssuedMapTask.Size() == 0 && c.issuedMapTask.Size() == 0 {
		c.issuedMapMutex.Unlock()
		mapDoneProcess(reply)
		c.prepareAllReduceTasks()
		c.mapDone = true
		return nil
	}
	//到这里，说明有未执行完的任务
	Printf("%v unissued map tasks %v issued map tasks at hand\n", c.unIssuedMapTask.Size(), c.issuedMapTask.Size())

	//下面开始map任务的分配，所以把issue的锁打开了
	c.issuedMapMutex.Unlock()  // release lock to allow unissued update
	curTime := getNowTimeSecond()
	ret, err := c.unIssuedMapTask.PopBack()//这里我们自己在PopBack中定义了队列为空时，会返回错误
	var fileId int
	if err != nil {
		Println("no map task yet, let worker wait...")
		fileId = -1
	} else {
		fileId = ret.(int)
		c.issuedMapMutex.Lock()
		reply.FileName = c.fileNames[fileId]
		c.mapTask[fileId].beginSecond = curTime
		c.mapTask[fileId].workerId = reply.WorkerId
		c.issuedMapTask.Insert(fileId)
		c.issuedMapMutex.Unlock()
		Printf("giving map task %v on file %v at second %v\n", fileId, reply.FileName, curTime)
	}
	
	reply.FileId = fileId
	reply.AllDone = false
	reply.NReduce = c.nReduce
	return nil

}

```

**JoinMapTask**

我们前面说过，worker接到任务后还会调用join函数，一个是确认是否成功执行，还需要判断超时等情况

> 这里由于文件都在一个服务器上，所以其实是偷懒了的，具体看代码。所以如果是最后的Challenge真正部署在多个服务器中，这部分逻辑应该是要改的

```go
func (c *Coordinator) JoinMapTask(args *MapTaskJoinArgs, reply *MapTaskJoinReply) error {
	// check current time for whether the worker has timed out
	Printf("got join request from worker %v on file %v %v\n", args.WorkerId, args.FileId, c.fileNames[args.FileId])

	// Println("locking issuedMutex...")
	c.issuedMapMutex.Lock()

	curTime := getNowTimeSecond()
	taskTime := c.mapTask[args.FileId].beginSecond
	if !c.issuedMapTask.Has(args.FileId) {
		//因为我们把丢弃的task，没有分配的task都设置FileId=-1
		Println("task abandoned or does not exists, ignoring...")
		// Println("unlocking issuedMutex...")
		c.issuedMapMutex.Unlock()
		reply.Accept = false
		return nil
	}
	//map任务需要在对应的worker上处理，所以对不上的话就忽略
    //这里因为文件都在一个服务器上，所以可以把这些判断都放在一个函数里判断了，不然这个可能还需要独立出去再判断一次，比如分配的时候worker检测到不是自己的，就直接返回false了。
	if c.mapTask[args.FileId].workerId != args.WorkerId {
		Printf("map task belongs to worker %v not this %v, ignoring...", c.mapTask[args.FileId].workerId, args.WorkerId)
		c.issuedMapMutex.Unlock()
		reply.Accept = false
		return nil
	}
	//从超时的任务先放入unIssuedMapTasks，之后再来处理
	if curTime - taskTime > maxTaskTime {
		Println("task exceeds max wait time, abadoning...")
		reply.Accept = false
		c.unIssuedMapTask.PutFront(args.FileId)
	} else {
		Println("task within max wait time, accepting...")
		reply.Accept = true
		c.issuedMapTask.Remove(args.FileId)
	}

	// Println("unlocking issuedMutex...")
	c.issuedMapMutex.Unlock()
	return nil
}
```

**超时处理**

我们之前记录了任务开始的时间，因此我们在直接对`issuedMapTask`和`issuedReduceTask`这两个集合不断的进行查询是否超过10s，以此来判断任务是否超时即可。

`removeTimeoutTasks()`

~~~go
func (m *MapSet) removeTimeoutMapTasks(mapTasks []MapTaskState, unIssuedMapTasks *BlockQueue) {
	for fileId, issued := range m.mapbool {
		now := getNowTimeSecond()
		if issued {
			if now - mapTasks[fileId.(int)].beginSecond > maxTaskTime {
				Printf("Map Task worker %v on file %v abandoned due to timeout\n", mapTasks[fileId.(int)].workerId, fileId)
				m.mapbool[fileId.(int)] = false
				m.count--
				unIssuedMapTasks.PutFront(fileId.(int))
			}
		}
	}
}

func (m *MapSet) removeTimeoutReduceTasks(reduceTasks []ReduceTaskState, unIssuedReduceTasks *BlockQueue) {
	for fileId, issued := range m.mapbool {
		now := getNowTimeSecond()
		if issued {
			if now - reduceTasks[fileId.(int)].beginSecond > maxTaskTime {
				Printf("Reduce Task worker %v on file %v abandoned due to timeout\n", reduceTasks[fileId.(int)].workerId, fileId)
				m.mapbool[fileId.(int)] = false
				m.count--
				unIssuedReduceTasks.PutFront(fileId.(int))
			}
		}
	}
}

func (c *Coordinator) removeTimeoutTasks() {
	Println("removing timeout maptasks...")
	c.issuedMapMutex.Lock()
	c.issuedMapTask.removeTimeoutMapTasks(c.mapTask, c.unIssuedMapTask)
	c.issuedMapMutex.Unlock()
	c.issuedReduceMutex.Lock()
	c.issuedReduceTask.removeTimeoutReduceTasks(c.reduceTask, c.unIssuedReduceTask)
	c.issuedReduceMutex.Unlock()
}
//每秒查询一次
func (c *Coordinator) loopRemoveTimeoutMapTasks() {
	for true {
		time.Sleep(1 * 1000 * time.Millisecond)
		c.removeTimeoutTasks()
	}
}
~~~

执行完Map阶段任务后，我们需要切换到Reduce阶段的调用。因此需要做一些准确，其实就是生成reduce任务。

~~~go
//生成reduce任务，因为nReduce就代表要最终输出几个文件
func (c*Coordinator) prepareAllReduceTasks(){
	for i:=0;i<c.nReduce;i++{
		Printf("putting %vth reduce task into channel\n", i)
		c.unIssuedReduceTask.PutFront(i)
	}
}
~~~

上面就是`Coordinator`在Map阶段需要进行的工作，对于Reduce阶段的任务，我们也是直接类比即可。 

##### Worker

worker就是实际干活，所以其实也就是不断的请求map/reduce任务即可。

注意我们最后是直接执行的`test-mr.sh`，所以我们最好先去看看他干了些什么。

> 包括删除mr-tmp下属文件，创建worker进程，编译（包括插入插件）等等

首先我们看`worker`的构造函数，最终的测试脚本会自动调用这个构造函数。我们构造的这个worker，会一直调用`process`来不断请求任务，直到全部完成

```go

//
// main/mrworker.go calls this function.
//
func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) {

	// time.Sleep(1000 * time.Millisecond)

	// Your worker implementation here.

	worker := AWorker{}
	worker.mapf = mapf
	worker.reducef = reducef
	worker.mapOrReduce = false
	worker.allDone = false
	worker.workerId = -1

	worker.logPrintln("initialized!")

	// uncomment to send the Example RPC to the coordinator.
	// CallExample()
	// main process
	for !worker.allDone {
		worker.process()
	}
	worker.logPrintln("no more tasks, all done, exiting...")
}

func (worker *AWorker) process() {
	if worker.allDone {
		// must exit
		// checked by caller
	}
	if !worker.mapOrReduce {
		// process map task
		reply := worker.askMapTask()
		if reply == nil {
			// must switch to reduce mode
			worker.mapOrReduce = true
		} else {
			if reply.FileId == -1 {
				// no available tasks for now
				time.Sleep(2 * 1000 * time.Millisecond)
			} else {
				// must execute
				worker.executeMap(reply)
			}
		}
	}
	if worker.mapOrReduce {
		// process reduce task
		reply := worker.askReduceTask()
		if reply == nil {
			// all done
			// must exit
			worker.allDone = true
		} else {
			if reply.RIndex == -1 {
				// noavailable tasks for now
				time.Sleep(2 * 1000 * time.Millisecond)
			} else {
				// must execute
				worker.executeReduce(reply)
			}
		}
	}
}
```

**任务的输入输出**

每个`map`任务接受一个`txt`文件，产生一系列**键值对**，并将这些**键值对**分配到特定的`reduce`任务。相同的单词应分配到相同的`reduce`任务去。

第`m`个`txt`文件，对应的第`m`个`map`任务，将产生`nReduce`个文件，其中`nReduce`给定的为`reduce`任务个数。第`m`个`map`任务分配给第`n`个`reduce`任务的所有**键值对**放在单独的文件中，命名为`mr-m-n`。（**这也就是中间文件**）

第`n`个`reduce`任务，读取所有命名为`mr-*-n`的文件，获得所有的`map`任务产生给第`n`个`reduce`任务的键值对，将它们合并，生成最终输出`mr-out-n`。**注意lab要求`Reduce`的输出必须是`mr-out-n`格式的。**

> 测试脚本将所有命名为`mr-out-*`的文件再整合，可看作一次更进一步的`reduce`任务。

最终产生的输出文件应都具有命名形式`mr-m-n`。有8个电子书、指定`nReduce=10`，则取值范围m∈[0,7],n∈[0,9]，一共产生80个中间文件。

**执行Map任务**

```go
func (worker *AWorker) executeMap(reply *MapTaskReply) {
	intermediate := makeIntermediateFromFile(reply.FileName, worker.mapf)
	worker.logPrintln("writing map results to file")
	worker.writeToFiles(reply.FileId, reply.NReduce, intermediate)
	worker.joinMapTask(reply.FileId)
}
```

其中我们生成了中间文件`intermediate`，当然后面reduce也得去读取`intermediate`

```go
func (worker *AWorker) writeToFiles(fileId int, nReduce int, intermediate []KeyValue) {
	kvas := make([][]KeyValue, nReduce)
	for i := 0; i < nReduce; i++ {
		kvas[i] = make([]KeyValue, 0)
	}
	for _, kv := range intermediate {
		index := keyReduceIndex(kv.Key, nReduce)
		//内置函数，在切片末尾添加元素
		kvas[index] = append(kvas[index], kv)
	}
	for i := 0; i < nReduce; i++ {
		// 创建临时文件
		// use this as equivalent
		tempfile, err := os.CreateTemp(".", "mrtemp")
		if err != nil {
			log.Fatal(err)
		}
		enc := json.NewEncoder(tempfile)
		for _, kv := range kvas[i] {
			err := enc.Encode(&kv)
			if err != nil {
				log.Fatal(err)
			}
		}
		outname := fmt.Sprintf("mr-%v-%v", fileId, i)
		err = os.Rename(tempfile.Name(), outname)
		if err != nil {
			worker.logPrintf("rename tempfile failed for $v\n", outname)
		}
	}
}
//中间文件内容示例
/////{"Key":"Apple","Value":"1"}
```

然后还需要告诉`master`是否执行成功。当然是通过`joinMapTask`来完成

> 这里偷懒了，是先执行map任务，再去回复，正确来说应该先判断这个worker能不能执行这个任务，这里由于是一个服务器上，所以肯定能读取到对应文件，所以这里直接先执行了，然后报告和询问master并进行各种情况的判读。

```go
func (worker *AWorker) joinMapTask(fileId int) {
	args := MapTaskJoinArgs{}
	args.FileId = fileId
	args.WorkerId = worker.workerId

	reply := MapTaskJoinReply{}
	call("Coordinator.JoinMapTask", &args, &reply)

	if reply.Accept {
		worker.logPrintln("accepted")
	} else {
		worker.logPrintln("not accepted")
	}
}
```

值得一提的是，这里的`mapf`,以及之后的`reducef`已经lab提供了，我们需要写一个函数型参数接受并使用（必须用）

```go
//
// a word-count application "plugin" for MapReduce.
//
// go build -buildmode=plugin wc.go
//

import "6.824/mr"
import "unicode"
import "strings"
import "strconv"

//
// The map function is called once for each file of input. The first
// argument is the name of the input file, and the second is the
// file's complete contents. You should ignore the input file name,
// and look only at the contents argument. The return value is a slice
// of key/value pairs.
//
func Map(filename string, contents string) []mr.KeyValue {
	// function to detect word separators.
	ff := func(r rune) bool { return !unicode.IsLetter(r) }

	// split contents into an array of words.
	words := strings.FieldsFunc(contents, ff)

	kva := []mr.KeyValue{}
	for _, w := range words {
		kv := mr.KeyValue{w, "1"}
		kva = append(kva, kv)
	}
	return kva
}

//
// The reduce function is called once for each key generated by the
// map tasks, with a list of all the values created for that key by
// any map task.
//
func Reduce(key string, values []string) string {
	// return the number of occurrences of this word.
	return strconv.Itoa(len(values))
}

```

执行worker任务示例图。

##### 函数调用关系图





最后`cd src/main`,执行`bash mr-test`可以通过所有`bash test-mr.sh`

![image-20231111101228162](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231111101228162.png)

### 结语

这个代码学习下来还是有很大收获，包括数据结构设计，各个模块功能的封装等等。

这个代码其实还有改进空间，比如后面的`Challenge`,要求改成几个服务器上部署，而不是开进程来执行模拟。以后有时间再说吧。

拖了太久才完成，快去做lab2把！

