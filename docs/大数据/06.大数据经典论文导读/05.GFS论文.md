---
title: GFS论文
date: 2023-06-04
tags: 
  - null
categories: 
  - 大数据
  - 大数据经典论文导读
---

> [The Google File System](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/035fc972c796d33122033a0614bc94cff1527999.pdf)
>
> 听mit的课之前，可以先看一遍这个，再看论文，在听课，就会豁然开朗

**GFS 的设计决策**

GFS 定了三个非常重要的设计原则，这三个原则带来了很多和传统的分布式系统研究大相径庭的设计决策。但是这三个原则又带来了大量工程上的实用性，使得 GFS 的设计思路后续被 Hadoop 这样的系统快速借鉴并予以实现。

**第一个是以工程上“简单”作为设计原则**。GFS 直接使用了 Linux 服务上的普通文件作为基础存储层，并且选择了最简单的单 Master 设计。单 Master 让 GFS 的架构变得非常简单，避免了需要管理复杂的一致性问题。不过**它也带来了很多限制，比如一旦 Master 出现故障，整个集群就无法写入数据**，而恢复 Master 则需要运维人员手动操作，所以 GFS 其实算不上一个高可用的系统。

但另外一方面，GFS 还是采用了 **Checkpoints、操作日志（Operation Logs）、影子 Master（Shadow Master）等一系列的工程手段，来尽可能地保障整个系统的“可恢复（Recoverable）”，以及读层面的“可用性（Availability）”**。

**第二个是根据硬件特性来进行设计取舍。**

道，2003 年的数据中心，各台机器的网卡带宽只有 100MB，网络带宽常常是系统瓶颈。所以 GFS 在写数据的时候，选择了流水线式的数据传输，而没有选择树形的数据传输方式。更进一步地，**GFS 专门设计了一个 Snapshot 的文件复制操作，在文件复制的时候避免了数据在网络上传输。这些设计都是为了减少数据在网络上的传输，避免我们有限的网络带宽成为瓶颈。**

**第三个是根据实际应用的特性，放宽了数据一致性（consistency）的选择。**最后，论文里也提到，GFS 是为了在廉价硬件上进行大规模数据处理而设计的。所以 GFS 的一致性相当宽松。GFS 本身对于随机写入的一致性没有任何保障，而是把这个任务交给了客户端。对于追加写入（Append），GFS 也只是作出了“至少一次（At Least Once）”这样宽松的保障。

可以说，GFS 是一个基本没有什么一致性保障的文件系统。但即使是这样，通过在客户端库里面加上校验、去重这样的处理机制，GFS 在大规模数据处理上已经算是足够好用了。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/158149b378d1d5b383078b3ee3440915.jpg" style="zoom:70%">

<center>GFS设计原则 </center>

我们会看到 GFS 是一个非常简单的单 Master 架构，但是这个 Master 其实有三种不同的身份，分别是：

- 相对于存储数据的 Chunkserver，Master 是一个目录服务；
- 相对于为了灾难恢复的 Backup Master，它是一个同步复制的主从架构下的主节点；
- 相对于为了保障读数据的可用性而设立的 Shadow Master，它是一个异步复制的主从架构下的主节点。

## GFS的Master

### 身份一：一个目录服务

作为一个分布式文件系统，一个有几千台服务器跑在线上的真实系统，GFS 的设计可以说是非常简单。

举个例子。要把一个文件存在 GFS 上，其实和在 Linux 操作系统上很像，GFS 一样会通过“命名空间 + 文件名”来定义一个文件。比如，我们可以把这一讲的录音文件存储在 /data/geektime/bigdata/gfs01 这样一个路径下。这样，所有 GFS 的客户端，都可以通过这个 /data/geektime/bigdata 命名空间加上 gfs01 这个文件名，去读或者写这个文件。

那么，当我们的客户端，实际上要去读或者写这个 gfs01 文件的时候，这个文件是实际存放在哪个物理服务器上呢？以及我们的客户端，具体是怎么读到这个文件的呢？**首先你要知道，在整个 GFS 中，有两种服务器，一种是 master，也就是整个 GFS 中有且仅有一个的主控节点；第二种是 chunkserver，也就是实际存储数据的节点。**

而既然 GFS 是叫做分布式文件系统，那么这个文件，其实就可以不存储在同一个服务器上。

因此**，在 GFS 里面，会把每一个文件按照 64MB 一块的大小，切分成一个个 chunk。每个 chunk 都会有一个在 GFS 上的唯一的 handle，这个 handle 其实就是一个编号，能够唯一标识出具体的 chunk。然后每一个 chunk，都会以一个文件的形式，放在 chunkserver 上。**

> handle起到ID的作用

而 chunkserver，其实就是一台普通的 Linux 服务器，上面跑了一个用户态的 GFS 的 chunkserver 程序。这个程序，会负责和 master 以及 GFS 的客户端进行 RPC 通信，完成实际的数据读写操作。

当然，为了确保数据不会因为某一个 chunkserver 坏了就丢失了，**每个 chunk 都会存上整整三份副本（replica）。其中一份是主数据（primary），两份是副数据（secondary），当三份数据出现不一致的时候，就以主数据为准。**有了三个副本，不仅可以防止因为各种原因丢数据，还可以在有很多并发读取的时候，分摊系统读取的压力。

> 这里在mit的课堂提问里也提到了

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/4ccb89f66276af2ce19c1fc83fdb432e.jpg" style="zoom:70%">

<center> 文件会拆分成chunk放在不同的chunkserver上</center>

如此一来，文件就被拆分成了一个个的 chunk 存在了 chunkserver 上。**那么你可能还要问：GFS 的客户端，怎么知道该去哪个 chunkserver 找自己要的文件呢？**

当然是问 master 啦。（Master与所有chunk server通信）首先，master 里面会存放三种主要的元数据（metadata）：

- 文件和 chunk 的命名空间信息，也就是类似前面 /data/geektime/bigdata/gfs01 这样的路径和文件名；

- 这些文件被拆分成了哪几个 chunk，也就是这个全路径文件名（file name）到多个 chunk handle 的映射关系；
- 这些 chunk 实际被存储在了哪些 chunkserver 上，也就是 chunk handle 到 chunkserver 的映射关系。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/440494242af83f78909bf836bbe1c0e2.jpg" style="zoom:70%">

#### clinet读取GFS数据流程

三个步骤：

- 客户端先去问 master，我们想要读取的数据在哪里。这里，客户端会发出两部分信息，一个是文件名，另一个则是要读取哪一段数据，也就是读取文件的 offset 及 length。因为所有文件都被切成 64MB 大小的一个 chunk 了，所以根据 offset 和 length，我们可以很容易地算出客户端要读取的数据在哪几个 chunk 里面。于是，客户端就会告诉 master，我要哪个文件的
- +第几个 chunk。
- master 拿到了这个请求之后，就会把这个 chunk 对应的所有副本所在的 chunkserver，告诉客户端。
- 等客户端拿到 chunk 所在的 chunkserver 信息后，客户端就可以直接去找其中任意的一个 chunkserver 读取自己所要的数据。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/7124aa76c1ec715b2a29613b5f065d95.jpg" style="zoom:70%">

<center> GFS里客户端读取数据的整个过程指令流向</center>

这整个过程抽象一下，其实和 Linux 文件系统差不多。master节点和 chunkserver 这样两种节点的设计，其实和操作系统中的文件系统一脉相承。**master 就好像存储了所有 inode 信息的 super block，而 chunk 就是文件系统中的一个个 block。只不过 chunk 比 block 的尺寸大了一些**，并且放在了很多台不同的机器上而已。我们通过 master 找到 chunk 的位置来读取数据，就好像操作系统里通过 inode 到 block 的位置，再从 block 里面读取数据。

所以，**这个时候的 master，其实就是一个“目录服务”，master 本身不存储数据，而是只是存储目录这样的元数据。这个和我们的单机系统的设计思想是一样的。其实在计算机这个行业中，所有的系统都是从最简单、最底层的系统演化而来的。**

### 身份二：快速恢复（Backup Master的同步复制）

只有一个Master的确会简单一些，但是也有 相应代价

GFS 里面的 master 节点压力很大。在一个 1000 台服务器的集群里面，chunkserver 有上千个，但 master 只有一个。几百个客户端并发读取的数据，虽然可以分摊到那 1000 个 chunkserver 的节点上，但是找到要读的文件的数据存放在哪里，都要去 master 节点里面去找。

**master 节点的所有数据，都是保存在内存里的。这样，master 的性能才能跟得上几百个客户端的并发访问。**

但是数据放在内存里带来的问题，就是一旦 master 挂掉，数据就会都丢了。所以，master 会通过记录操作日志和定期生成对应的 Checkpoints 进行持久化，也就是写到硬盘上。

但是数据放在内存里带来的问题，**就是一旦 master 挂掉，数据就会都丢了。所以，master 会通过记录操作日志和定期生成对应的 Checkpoints 进行持久化，也就是写到硬盘上。**

这是为了确保在 master 里的这些数据，不会因为一次机器故障就丢失掉。当 master 节点重启的时候，**就会先读取最新的 Checkpoints，然后重放（replay）Checkpoints 之后的操作日志，把 master 节点的状态恢复到之前最新的状态。这是最常见的存储系统会用到的可恢复机制。**

**master 节点的硬件彻底故障了呢？**

去数据中心重新更换硬件可不是几分钟的事情**，所以 GFS 还为 master 准备好了几个“备胎”，也就是另外几台 Backup Master。**所有针对 master 的数据操作，都需要同样写到另外准备的这几台服务器上。只有当数据在 master 上操作成功，对应的操作记录刷新到硬盘上，并且这几个 Backup Master 的数据也写入成功，并把操作记录刷新到硬盘上，整个操作才会被视为操作成功。

> 总结一下，就是为了防止一台Master暴毙了，集群挂掉，准备几台Backup Master，对于所有对Master的操作同步复制到Backup Master上，出问题了，就切换到备胎

这种方式，**叫做数据的“同步复制**”，是分布式数据系统里的一种典型模式。假如你需要一个高可用的 MySQL 集群，一样也可以采用同步复制的方式，在主从服务器之间同步数据。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/199f3ddb59c4d0a0233f9b71549a85c1.jpg" style="zoom:70%">

>在t1时间点上，客户端得到写入成功的返回，此时Master和Backup Master的数据是一致的，但是Shadow Master的数据，要到t2时间点才会追上来

**不过要实现master挂了，切换到备胎。在集群外部需要有监控 master 的服务在运行。**如果只是 master 的进程挂掉了，那么这个监控程序会立刻重启 master 进程。而如果 master 所在的硬件或者硬盘出现损坏，那么这个监控程序就会在前面说的 Backup Master 里面找一个出来，启动对应的 master 进程，让它“备胎转正”，变成新的 master。

> 在mit的课里提到，认为这个监控程序时完全可信的，不会奔溃

**快速恢复具体实现**

为了让集群中的其他 chunkserver 以及客户端不用感知这个变化，GFS 通过一个规范名称（Canonical Name）来指定 master，而不是通过 IP 地址或者 Mac 地址。这样，**一旦要切换 master，这个监控程序只需要修改 DNS 的别名，就能达到目的。有了这个机制，GFS 的 master 就从之前的可恢复（Recoverable），进化成了能够快速恢复（Fast Recovery）。**

不过，就算做到了快速恢复，我们还是不满足。毕竟，从监控程序发现 master 节点故障、启动备份节点上的 master 进程、读取 Checkpoints 和操作日志，仍然是一个几秒级别乃至分钟级别的过程。在这个时间段里，我们可能仍然有几百个客户端程序“嗷嗷待哺”，希望能够在 GFS 上读写数据。虽然作为单个 master 的设计，这个时候的确是没有办法去写入数据的。

### 身份三 shadow Masater（异步复制）

如上所述，监控程序发现 master 节点故障、启动备份节点上的 master 进程、读取 Checkpoints 和操作日志，仍然是一个几秒级别乃至分钟级别的过程。

为了让我们这个时候还能够从 GFS 上读取数据，Google 的工程师还是想了一个办法。

这个办法就是加入一系列只读的“影子 Master”，这些影子 Master 和前面的备胎不同，master 写入数据并不需要等到影子 Master 也写入完成才返回成功**。而是影子 Master 不断同步 master 输入的写入，尽可能保持追上 master 的最新状态。**

**影子 Master 会不断同步 master 里的数据，不过当 master 出现问题的时候，客户端们就可以从这些影子 Master 里找到自己想要的信息**。当然，因为小小的延时，**客户端有很小的概率，会读到一些过时的 master 里面的信息，比如命名空间、文件名等这些元数据。**但你也要知道，这种情况其实只会发生在以下三个条件都满足的情况下：

- 第一个，是 master 挂掉了；
- 第二个，是挂掉的 master 或者 Backup Master 上的 Checkpoints 和操作日志，还没有被影子 Master 同步完；
- 第三个，则是我们要读取的内容，恰恰是在没有同步完的那部分操作上；

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/7a312ed6bda66ce6e8b112yyfb77c82d.jpg" style="zoom:70%">

**影子 Master 让整个 GFS 在 master 快速恢复的过程中，虽然不能写数据，但仍然是完全可读的**。至少在集群的读取操作上，GFS 可以算得上是“高可用（High Availability）”的了。

### 总结

GFS 采用了单 Master 架构，但是这个 Master 有着三重不同的含义。

首先，作为和 chunkserver 相对的 master，**它是一个目录服务。所有的元数据都会保留在我们的 master 里，而所有的数据会保存在 chunkserver 里。master 存的就好比是 inode 信息，而 chunkserver 存的则是实际的 block。**

其次，为了保障系统在故障下的快速恢复能力和读数据的高可用性，GFS 又对这个 master 添加了各种工程保障措施。这些措施，**包括 Checkpoints 和操作日志、Backup Master、外部的监控程序，以及只读的影子 Master**。在这个高可用场景下，Master 是唯一可以写入数据的节点。Backup Master 通过同步复制的方式从 master 同步数据，而影子 Master 则通过异步复制的方式来同步 master 的数据。

在这个场景下，Backup Master 和影子 Master 其实是 Master 的 Slave 节点。**如果我们把 Master 当成是一个 MySQL 数据库的主节点，那么 Backup Master 就是配置了高可用的同步复制的 HA 节点，而影子 Master 就是只配置了异步复制分担数据读取压力的 readonly 节点。**

### Q&A

1如果你读了论文原文，你应该看到 GFS 的客户端会在读取数据的过程中，把一些数据缓存了下来，那么它究竟缓存了哪些数据来减少频繁的网络往来？在这个缓存机制上，客户端有可能会读到过时的数据吗？

A: **GFS客户端会在读取数据时，把文件其余块的元数据缓存在本地**，因为是追加写已写入成功的元数据短期不会变化，再者从空间局部性的原理出发，读部分块后，大概率会要接下来读这个文件余下块，所以提前装载元数据也算是一种优化预测吧；

2前面说了，master 的数据都会通过操作日志和 Checkpoints 持久化在硬盘上。但这句话其实不完全正确，每个 chunk 存放在什么 chunkserver 上的这些元数据，master 并不会持久化。读完论文之后，你能说说当 master 重启的时候，怎么重新拿到这个数据吗？

A：mit课里提到到，这就是handle->chunk server的信息，不用持久化存储，因为master重启时会通信每个chunk server，让他们告诉自己他们有哪些chunk handle

## GFS应对网络瓶颈（依据硬件进行设计）

GFS 论文中第二个重要的设计决策，也就是根据实际的硬件情况来进行系统设计。如图GFS设计原则的“依据硬件进行设计”

在单台服务器下，**我们的硬件瓶颈常常是硬盘。而到了一个分布式集群里，我们又有了一个新的瓶颈，那就是网络。**

### GFS 的硬件配置

2003 年的 GFS 是跑在什么样的硬件服务器上的呢？论文的第 6 部分还真的透露了一些信息给我们。Google 拿来做微基准测试（Micro-Benchmark）的服务器集群的配置是这样的：

- 19 台服务器、1 台 master、2 台 master 的只读副本、16 台 chunkserver，以及另外 16 台 GFS 的客户端；
- 所有服务器的硬件配置完全相同，都是双核 1.45 GHz 的奔腾 3 处理器 + 2GB 内存 + 两块 80GB 的 5400rpm 的机械硬盘 + 100 Mbps 的全双工网卡。
- 然后把所有的 19 台 GFS 集群的机器放在一台交换机上，所有的 16 台 GFS 的客户端放在另外一台交换机上，两台交换机之间通过带宽是 1Gbps 的网线连接起来。

而 Google 跑在内部实际使用的真实集群，虽然论文里没有给出具体的硬件配置，但我们也可以反向推算一下。论文第 6 部分有一个 Table 2，里面有 A 和 B 两个集群。根据表格里面的数据可以计算得出，里面的 A 集群平均每台 chunkserver 大约有 200GB 的硬盘，每台 chunkserver 需要的 Metadata（元数据）大约是 38MB。而里面的 B 集群则是 800GB 的硬盘，以及 93MB 的 Metadata。这样看起来，除了可以多插几块硬盘增加一些存储空间之外，前面测试集群的硬件配置完全够用了。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/ce97bd552029ab8e6a506a120049749a.jpg" style="zoom:70%">

<center>推算容量 </center>

**在这个硬件配置中，5400 转（rpm）的硬盘，读写数据的吞吐量通常是在 60MB/s~90MB/s 左右。而且我们通常会插入多块硬盘，比如集群 B，就需要 10 块 80GB 的硬盘，这样就意味着整体硬盘的吞吐量可以达到 500MB/s 以上**。但是，100Mbps 的网卡的极限吞吐率只有 12.5MB/s，这个也就意味着，**当我们从 GFS 读写数据的时候，瓶颈就在网络上。**

### GFS 文件写入

实际上，读数据简单，是因为不管我们有多少个客户端并发去读一个文件，读到的内容都不会有区别，即使它们读同一个 chunk 是分布在不同 chunkserver。**我们不是靠在读取中做什么特殊的动作，来保障客户端读到的数据都一样。“保障读到的数据一样”这件事情，其实是在数据写入的过程中来保障的**。

考虑一种情况，取只需要读一个 chunkserver，最坏的情况无非是读不到重试。**而写入，则是同时要写三份副本，如果一个写失败，两个写成功了，数据就已经不一致了。**

首先看GFS如何写入数据？

- 第一步，客户端会去问 master 要写入的数据，应该在哪些 chunkserver 上。

- 第二步，和读数据一样，master 会告诉客户端所有的次副本（secondary replica）所在的 chunkserver。这还不够，master 还会告诉客户端哪个 replica 是“老大”，**也就是主副本（primary replica），数据此时以它为准。**

  - **如同mit课程里提到的，多个chunk server中，会选一个当Primat，其他的当secondary**,这里返回的是要写入数据的chunk server（显然不是全部）

- 第三步，拿到数据应该写到哪些 chunkserver 里之后，客户端会把要写的数据发给所有的 replica。不过此时**，chunkserver 拿到发过来的数据后还不会真的写下来，只会把数据放在一个 LRU 的缓冲区里。**

  >所谓的LRU(Least recently used)算法的基本概念是当内存的剩余的可用空间不够时,缓冲区尽可能的先保留使用者最常使用的数据,换句话说就是优先清除”较不常使用的数据”,并释放其空间.

第四步，等到所有次副本都接收完数据后，客户端就会发送一个写请求给到主副本。GFS 面对的是几百个并发的客户端，所以主副本可能会收到很多个客户端的写入请求。**主副本自己会给这些请求排一个顺序，确保所有的数据写入是有一个固定顺序的**。接下来，主副本就开始按照这个顺序，把刚才 LRU 的缓冲区里的数据写到实际的 chunk 里去。

第五步，主副本会把对应的写请求转发给所有的次副本，所有次副本会和主副本以同样的数据写入顺序，把数据写入到硬盘上。

第六步，次副本的数据写入完成之后，会回复主副本，我也把数据和你一样写完了。

第七步，主副本再去告诉客户端，这个数据写入成功了。而如果在任何一个副本写入数据的过程中出错了，**这个出错都会告诉客户端，也就意味着这次写入其实失败了。**

**因此，GFS写入过程中，是会出现有些副本写入成功，有些写入失败的情况。**

> 放宽数据一致性的要求讲解如何处理

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/cd111d95dde55f57eb7cecf23da4e7e6.jpg" style="zoom:70%">

<center> GFS数据写入（数据流和控制流分离）</center>

从上图可以看到。GFS 的数据写入使用了两个很有意思的模式（分离控制流和流水线传输），**来解决这节课一开始提到的网络带宽的瓶颈问题。**

### 分离控制流和数据流

和之前从 GFS 上读数据一样，**GFS 客户端只从 master 拿到了 chunk data 在哪个 chunkserver 的元数据，实际的数据读写都不再需要通过 master。**另外，不仅具体的数据传输不经过 master，**后续的数据在多个 chunkserver 上同时写入的协调工作，也不需要经过 master。**

**控制流和数据流的分离，不仅仅是数据写入不需要通过 master，更重要的是实际的数据传输过程，和提供写入指令的动作是完全分离的。**

### 流水线式的网络数据传输

GFS采用了流水线（pipeline）式的网络传输。数据不一定是先给到主副本，**而是看网络上离哪个 chunkserver 近，就给哪个 chunkserver，数据会先在 chunkserver 的缓冲区里存起来**，就是前面提到的第 3 步。但是写入操作的指令，也就是上面的第 4~7 步，则都是由客户端发送给主副本，再由主副本统一协调写入顺序、拿到操作结果，再给到客户端的。

> 比如上面数据先发给了次副本A，同时，次副本A也传输数据给最近的副本

之所以要这么做，还是因为 GFS 最大的瓶颈就在网络。如果用一个最直观的想法来进行数据传输，我们可以把所有数据直接都从客户端发给三个 chunkserver。

但是这种方法的问题在于，**客户端的出口网络会立刻成为瓶颈。**

#### 举例

比如，我们要发送 1GB 的数据给 GFS，客户端的出口网络带宽有 100MB/ 秒，那么我们只需要 10 秒就能把数据发送完。但是因为三个 chunkserver 的数据都要从客户端发出，所以要 30s 才能把所有的数据都发送完，而且这个时候，三个 chunkserver 的网络带宽都没有用满，各自只用了 1/3，网络并没有被有效地利用起来。

而在流水线式的传输方式下，**客户端可以先把所有数据，传输给到网络里离自己最近的次副本 A，然后次副本 A 一边接收数据，一边把对应的数据传输给到离自己最近的另一个副本，也就是主副本。**

同样的，主副本可以如法炮制，把数据也同时传输给次副本 B。**在这样的流水线式的数据传输方式下，只要网络上没有拥堵的情况，只需要 10 秒多一点点**，就可以把所有的数据从客户端，**传输到三个副本所在的 chunkserver 上。**

> 所以传写入数据的时候，每个chunk server都传了所有数据

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/410ac2b91de26ecedfc4b2382decced0.jpg" style="zoom:70%">

<center> 流水线传输</center>

#### 为什么不能直接发给主副本A？

> 主要是由数据中心实际网络拓扑结构决定的

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/34417d1ee235d2876e2ab3987e57cb9f.jpg" style="zoom:70%">

<center> 传统的三层交换的数据中心拓扑图</center>

一般来说，我们几百台服务器所在的数据中心，一般都是通过三层交换机连通起来的：

- 同一个机架（Rack）上的服务器，都会接入到一台接入层交换机（Access Switch）上；
- 各个机架上的接入层交换机，都会连接到某一台汇聚层交换机（Aggregation Switch）上；
- 而汇聚层交换机，再会连接到多台核心交换机（Core Switch）上。

那么根据这个网络拓扑图，你会发现，两台服务器如果在同一个机架上，它们之间的网络传输只需要通过接入层的交换机即可。在这种情况下，除了两台服务器本身的网络带宽之外，它们只会占用所在的接入层交换机的带宽。

但是，如果两台服务器不在一个机架，乃至不在一个 VLAN 的情况下**，数据传输就要通过汇聚层交换机，甚至是核心交换机了。而如果大量的数据传输，都是在多个不同的 VLAN 之间进行的，那么汇聚层交换机乃至核心交换机的带宽，就会成为瓶颈。**

所以我们再回到之前的链式传输的场景，GFS 最大利用网络带宽，同时又减少网络瓶颈的选择就是这样的：

- 首先，客户端把数据传输给离自己“最近”的，也就是在同一个机架上的次副本 A 服务器；
- 然后，次副本 A 服务器再把数据传输给离自己“最近”的，在不同机架，但是处于同一个汇聚层交换机下的主副本服务器上；
- 最后，主副本服务器，再把数据传输给在另一个汇聚层交换机下的次副本 B 服务器。

**这样的传输顺序（传最近，然后流水线），就最大化地利用了每台服务器的带宽，并且减少了交换机的带宽瓶颈**。而如果我们非要先把数据从客户端传输给主副本，再从主副本传输到次副本 A，那么同样的数据就需要多通过汇聚层交换机一次，从而就占用了更多的汇聚层交换机的资源。

### Snapshot操作

除了分离控制流和数据流，以及使用流水线式的数据传输方式之后，对于 GFS 的网络传输上，还有什么其他的优化空间吗？

**那就是为常见的文件复制操作单独设计一个指令。**

复制文件，相信这个是你用自己的电脑的时候，会常常做的事儿。**在 GFS 上，如果我们用笨一点的办法，自然是通过客户端把文件从 chunkserver 读回来，再通过客户端把数据写回去。这样的话，读数据也经过一次网络传输，写回三个副本服务器，即使是流水线式的传输，也要三次传输，一共需要把数据在网络上搬运四次。**

> 仔细想想，如果是复制数据，还不是chunk server来存储数据，所以完全可以省去这部分的数据传输，让chunk server自己在本地备份

所以，GFS 就专门为文件复制设计了一个 Snapshot 指令，当客户端通过这个指令进行文件复制的时候，这个指令会通过控制流，下达到主副本服务器，主副本服务器再把这个指令下达到次副本服务器。**不过接下来，客户端并不需要去读取或者写入数据，而是各个 chunkserver 会直接在本地把对应的 chunk 复制一份。**

### 总结

 GFS 的最大的硬件瓶颈就是在网络，于是，在 GFS 设计数据写入机制的时候，就是经过仔细分析后针对这个问题来设计的。

- 第一，和读数据一样，GFS 采用了控制流和数据流分离的方式。在写入数据的时候，客户端只是从 master 拿到 chunk 所在位置的元数据，而在实际的数据传输过程中，并不需要 master 参与，从而就避免了 master 成为瓶颈。
- 第二，在客户端向多个 chunkserver 写入数据的时候，采用了“就近”的流水线式传输的方案。这种方式，就尽可能有效地利用了客户端、chunkserver 乃至于交换机的带宽。
- 第三，对于常见的文件复制这个操作，GFS 专门设计了一个 Snapshot 的指令，针对文件复制，会直接在 chunkserver 本地进行，完全避免了网络传输。

### Q&A

在你接触过的系统和代码中，有没有什么设计，也是深度考虑了实际的硬件性能和瓶颈的呢？

## 放宽数据一致性的要求（依据应用进行设计）

2003 年的 GFS，对于一致性的要求，是非常宽松的。一方面，这是为了遵循第一个设计原则，就是“保持简单”，简单的设计使得做到很强的一致性变得困难。

另一方面，则是要考虑“硬件特性”，GFS 希望在机械硬盘上尽量有比较高的写入性能，所以它只对顺序写入考虑了一致性，这就自然带来了宽松的一致性。

> 随机写入无一致性保障
>
> 追加写入仅保障“至少一次”（At Least Once）

### 随机写入只是“确定”的

通过上一讲的学习，我们知道了在 GFS 中，客户端是怎么把数据写入到文件系统里的。不过，我们并没有探讨一个非常重要的问题，就是数据写入的一致性（Consistency）问题。这个一致性，也是我们常常听说的 CAP 理论中的 C，即一致性、可用性、分区容错性在分布式系统中，三者不能兼得中的一致性问题。这个 C，也正来自于一致性的英文 Consitency 的首字母。

我们先来看看，一致性到底指的是什么东西。在 GFS 里面，主要定义了对一致性的**两个层级的**概念：

1. 第一个，就叫做“一致的（Consistent）”。这个就是指，多个客户端无论是从主副本读取数据，还是从次副本读取数据，读到的数据都是一样的。
2. 第二个，叫做“确定的（Defined）”。这个要求会高一些，指的是对于客户端写入到 GFS 的数据，能够完整地被读到。

![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/f08de0776c1338980a22f5e57014b976.jpg)

<center> GFS论文Table1</center>

**首先，如果数据写入失败，GFS 里的数据就是不一致的。**

> 即上面提到的问题，如果部分副本写入失败就会不一致

其次，如果客户端的数据写入是顺序的（**串行**），并且写入成功了，那**么文件里面的内容就是确定的。**

>比如，你先往一个文件里，写入一部电影《星球大战》，这个时候，客户端无论从哪个副本读数据，读到的都是星球大战。然后再写入《星际迷航》，那么客户端再读数据，读到的也一定是《星际迷航》。

但是**，如果由多个客户端并发写入数据，即使写入成功了，GFS 里的数据也可能会进入一个一致但是非确定的状态。**

> 也就是说，两个客户端并发往一个文件里面写数据，一个想要写入《星球大战》，一个想要写入《星际迷航》，两个写入都成功了。这个时候，GFS 里面三份副本的数据是一样的，客户端读到的数据无论是从哪个副本里读，都是一样的。

但是呢，客户端可能读出来的数据里，前一小时是《星球大战》，后一小时是《星际迷航》。**无论哪个时间节点去读数据，客户端都不能读到一部完整的《星球大战》，或者是《星际迷航》。**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/16097c4fd15e13f3810401a5491d5b20.jpg" style="zoom:70%">

**原因**

- 第一种因素是在 GFS 的数据读写中，为了减轻 Master 的负载，数据的写入顺序并不需要通过 Master 来进行协调，而是直接由存储了主副本的 chunkserver，来管理同一个 chunk 下数据写入的操作顺序。
- 第二种因素是随机的数据写入极有可能要跨越多个 chunk。

> 在写入《星球大战》和《星际迷航》的时候，前一个小时的电影是在 chunk 1，对应的主副本在 server A，后一个小时的电影是在 chunk 2，对应的主副本在 server B。然后写入请求到 server A 的时候，《星际迷航》在前，《星球大战》在后，那么《星球大战》的数据就覆盖了《星际迷航》。而到 server B 的时候则是反过来，《星际迷航》又覆盖了《星球大战》。于是，就会出现客户端读数据，前半段是《星球大战》，后半段是《星际迷航》的奇怪现象了。

这个一致但是非确定的状态，**是因为随机的数据写入，没有原子性（Atomic）或者事务性（Transactional）**。如果想要随机修改 GFS 上的数据，一般会建议使用方在客户端的应用层面，保障数据写入是顺序的，从而可以避免并发写入的出现。

### 追加写入的“至少一次”的保障（记录追加）

GFS 不是支持几百个客户端并发读写吗？现在怎么又说，写入要客户端自己保障是“顺序”的呢？

这是因为随机写入并不是 GFS 设计的主要的数据写入模式，**GFS 设计了一个专门的操作，叫做记录追加（Record Appends）。这是 GFS 希望我们主要使用的数据写入的方式，而且它是原子性（Atomic）的，能够做到在并发写入时候是基本确定的。**

GFS 的记录追加的写入过程，和上一讲的数据写入几乎一样。它们之间的差别主要在于，**GFS 并不会指定在 chunk 的哪个位置上写入数据，而是告诉最后一个 chunk 所在的主副本服务器，“我”要进行记录追加。**

这个时候，主副本所在的 chunkserver 会做这样几件事情：

- 检查当前的 chunk 是不是可以写得下现在要追加的记录。**如果写得下，那么就把当前的追加记录写进去（一个chunk 64MB）**，同时，这个数据写入也会发送给其他次副本，在次副本上也写一遍。
- 如果当前 chunk 已经放不下了，**那么它先会把当前 chunk 填满空数据，并且让次副本也一样填满空数据**。然后，主副本会告诉客户端，让它在下一个 chunk 上重新试验。这时候，客户端就会去一个新的 chunk 所在的 chunkserver 进行记录追加。
- 因为主副本所在的 chunkserver 控制了数据写入的操作顺序，并且数据只会往后追加，**所以即使在有并发写入的情况下，请求也都会到主副本所在的同一个 chunkserver 上排队(类似锁的概念，控制了并发)，也就不会有数据写入到同一块区域，覆盖掉已经被追加写入的数据的情况了。**
- 而为了保障 chunk 里能存的下需要追加的数据，**GFS 限制了一次记录追加的数据量是 16MB，**而 chunkserver 里的一个 chunk 的大小是 64MB。所以，在记录追加**需要在 chunk 里填空数据的时候，最多也就是填入 16MB，也就是 chunkserver 的存储空间最多会浪费 1/4。**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/44440eb6b88c073763efdcb58e6f410c.jpg" style="zoom:70%">

<center> 追加记录</center>

**如果在主副本上写入成功了，但是在次副本上写入失败了怎么办呢？这样不是还会出现数据不一致的情况吗？**

其实在这个时候，主副本会告诉客户端数据写入失败，然后让客户端重试。不过客户端发起的重试，并不是在原来的位置去写入数据，而是发起一个新的记录追加操作。这个时候，可能已经有其他的并发追加写入请求成功了，那么这次重试会写入到更后面。

我们可以一起来看这样一个例子：有三个客户端 X、Y、Z 并发向同一个文件进行记录追加，写入数据 A、B、C，对应的三个副本的 chunkserver 分别是 Q、P、R。

主副本先收到数据 A 的记录追加，在主副本和次副本上进行数据写入。在 A 写入的同时，B，C 的记录追加请求也来了，**这个时候写入会并行进行，追加在 A 的后面。**

这个时候，A 的写入在某个次副本 R 上失败了，于是主副本告诉客户端去重试；同时，客户端再次发起记录追加的重试，这次的数据写入，不在 A 原来的位置，**而会是在 C 后面。**

如此一来，在 B 和 C 的写入，以及 A 的重试完成之后，我们可以看到：

- 在 Q 和 P 上，chunkserver 里面的数据顺序是 A-B-C-A；
- 但是在 R 上，chunkserver 里面的数据顺序是 N/A-B-C-A；
- **也就是 Q 和 P 上，A 的数据被写入了两次，而在 R 上，数据里面有一段是有不可用的脏数据**。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/d36e354c23d0312df6e06eb31d8de0d9.jpg" style="zoom:70%">

所以在这个记录追加的场景下**，GFS 承诺的一致性，叫做“至少一次（At Least Once）”。**也就是写入一份数据 A，**在重试的情况下，至少会完整地在三个副本的同一个位置写入一次**。**但是也可能会因为失败，在某些副本里面写入多次。**那么，在不断追加数据的情况下，你会看到大部分数据都是一致的，并且是确定的，但是整个文件中，会夹杂着少数不一致也不确定的数据。

### 解决一致性的机制

GFS 的写入数据的一致性保障是相当低的。它只是保障了所有数据追加至少被写入一次，并且还保障不了数据追加的顺序。这使得客户端读取到的副本中，可能也会存在重复的数据或者空的填充数据

**不过，这个“至少一次”的机制，其实很适合 Google 的应用场景**。你想像一下，如果你是一个搜索引擎，不断抓取网页然后存到 GFS 上。其实你并不会太在意这个网页信息是不是被重复存了两次，你也不太会在意不同的两个网页存储的顺序。而且即使你在意这两点，比如你存的不是网页，而是用户的搜索日志或广告展示和点击的日志数据。或者你担心数据写入失败，带来的是部分不完整的数据，也有很多简单的解决办法。

事实上，**GFS 的客户端里面自带了对写入的数据去添加校验和（checksum），并在读取的时候计算来验证数据完整性的功能**。

> 而对于数据可能重复写入多次的问题，你也可以对每一条要写入的数据生成一个唯一的 ID，并且在里面带上当时的时间戳。这样，即使这些日志顺序不对、有重复，你也可以很容易地在你后续的数据处理程序中，通过这个 ID 进行排序和去重。思考题·，也可以用这样的办法

“至少一次”的写入模型好处：

- 第一是高并发和高性能，这个设计使得我们可以有很多个客户端并发向同一个 GFS 上的文件进行追加写入，而高性能本身也是我们需要分布式系统的起点。
- 第二是简单，GFS 采用了一个非常简单的单 master server，多个 chunkserver 架构，所有的协调动作都由 master 来做，而不需要复杂的一致性模型。毕竟，2003 年我们只有极其难以读懂的 Paxos 论文，Raft 这样的分布式共识算法要在 10 年之后的 2013 年才会诞生。而简单的架构设计，使得系统不容易出 Bug，出了各种 Bug 也更容易维护。

>而即使 GFS 里的数据随机写入能够保障确定性，在那个年代的实用价值也不高。因为那个时候，大家用的还都是 5400 转，或者 7200 转的机械硬盘。如果你读过我的《深入浅出计算机组成原理》，**你一定还记得机械硬盘的 IOPS 也就是在 70 左右，**几百个客户端要是并发写入一个位置，硬盘根本抗不住。

对于随机数据写入的一致性,**后面 Bigtable 的论文里，** Google 是如何在这个不可靠的 GFS 上，架起一个**高可用性、可以随机读写的 KV 数据库。**

### 总结

。对于随机数据写入，GFS 针对成功的并发写入，只保障一致性，但是不保障确定性。而对于记录追加这样专门设计出来的操作，GFS 也只保障“至少一次”的写入，避免不了确定的数据中，夹杂着不一致和不确定的脏数据。

>而这些数据，都需要你通过校验和乃至应用层自行去重来进行处理。这些设计和机制，都是和 GFS 面临的应用场景所匹配的，即高并发大量追加写入新的日志、网页、生成的索引等等的应用场景。

### Q&A

1 GFS 的文件，只能保障“至少一次”的追加写入。那么，如果我们通过 GFS 的客户端要写入一部电影到 GFS，然后过一阵再读出来，我们都可以有哪些方式，来保障这个电影读取之后能够正常播放呢？

A: 给每一条要写入的数据生成一个唯一的ID，并且在里面带上当时的时间戳，读取的时候再按ID的顺序去重读取即可。

## 推荐

[Hadoop](https://ieeexplore.ieee.org/document/5496972/)
