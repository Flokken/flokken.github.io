---
title: Spark基础
date: 2023-01-14 00:00:00
tags: 
  - Spark
categories: 
  - 大数据
  - Spark
permalink: /pages/d8fb92/
---

## 1 Hadoop框架

Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。

![image-20230122172205779](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172205779.png)

**其中核心技术是MapReduce（分布式计算框架）和HDFS（分布式存储系统）。**

![image-20230122172236349](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172236349.png)

### 1.1 MapReduce

Map和Reduce是大规模集群并行计算过程的高度抽象。

**核心思想**：

Map: 将数据进行**拆分**，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。

Reduce:对数据进行**汇总**,即对map阶段的结果进行全局汇总。

**抽象模型**：

**Map**: 对一组数据元素进行某种重复式的处理；

**Reduce:** 对Map的中间结果进行某种进一步的结果整理。

MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现，可以看到，**MapReduce处理的数据类型是<key,value>键值对:** 

- map: [k1,v1] → [(k2,v2)] 
- reduce: [k2, {v2,…}] → [k3, v3]

**工作流程**

![image-20230122172246932](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172246932.png)

### 1.2 HDFS

设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统（Distributed File System）

**设计结构：**

HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件的访问操作；集群中的DataNode管理存储的数据。

HDFS架构图

![image-20230122172256602](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172256602.png)



### 1.3 Yarn

yarn（yet another resource negotiator）是一个**通用分布式资源管理系统和调度平台**，为上层应用提供统一的资源管理和调度。在集群利用率、资源统一管理和数据共享等方面带来巨大好处。

- 资源管理系统：集群的硬件资源，如内存、CPU等。
- 调度平台：多个程序同时申请计算资源如何分配，调度的规则(算法)。
- 通用：不仅仅支持mapreduce程序，理论上支持各种计算程序。yarn不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。

可以把yarn理解为—个分布式的操作系统平台，而mapreduce等计算程序则相当于运行于操作系统之上的应用程序**，yarn为这些程序提供运算所需的资源(内存、CPU等)**。hadoop能有今天这个地位，yarn可以说是功不可没。因为有了yarn，更多计算框架可以接入到hdfs中，而不单单是mapreduce，正是因为yarn的包容，使得其他计算框架能专注于计算性能的提升。架构图：

![image-20230122172303532](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172303532.png)

YARN 是一个资源管理、任务调度框架，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）、Container

- ResourceManager # 负责所有应用程序（整个集群）资源分配与管理以及接收作业的提交；
- NodeManager # 负责每一个节点的维护；
- ApplicationMaster # 负责每一个应用程序的资源分配（资源二次分配）以及监控所有任务的运行状态；

## 2 spark

### 2.1 spark 相较于Hadoop

以spark2.x为例

![image-20230122172309679](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172309679.png)





|          | Hadoop                         | Spark                                   |
| -------- | ------------------------------ | --------------------------------------- |
| 场景     | 大数据数据集的批处理           | 迭代计算、流计算                        |
| 编程范式 | Map+Reduce API较低层，适应性差 | RDD组成DAG有向无环图，API顶层，方便使用 |
| 存储     | 中间结果在磁盘，延迟大         | RDD结果在内存，延迟小                   |
| 运行方式 | Task以进程方式维护，启动任务慢 | Task以线程方式维护，启动快              |

#### 2.1.1 原理比较

Hadoop和[Spark](https://so.csdn.net/so/search?q=Spark&spm=1001.2101.3001.7020)都是并行计算，

**Hadoop**一个作业称为一个Job，Job里面分为Map Task和Reduce Task阶段，每个MapTask/ReduceTesk都是**进程**级别的！当Task结束时，**进程**也会随之结束；
好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，
但是问题在于**task之间不方便共享数据，执行效率比较低**。比如多个MapTask读取不同数据源文件需要将数据源加载到每个MapTask中，造成重复加载和浪费内存。

**Spark**的任务称为application，一个SparkContext对应一个application；
application中存在多个job，每触发一次行动算子就会产生一个job；
每个job中有多个stage，stage是shuffle过程中DAGScheduler通过RDD之间的依赖关系划分job而来的，stage数量=宽依赖（shuffle）数量+1 （默认有一个ResultStage）；
每个stage里面有多个task，组成taskset，由TaskScheduler分发到各个executor中执行；
executor的生命周期是和application一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。
Spark基于**线程**的方式计算是为了数据共享和提高执行效率！
Spark采用了**线程**的最小的执行单位，但缺点是线程之间会有资源竞争。

#### 2.1.2 应用场景

**Hadoop MapReduce** 其设计初衷是**一次性数据计算**（一个job中 只有一次map和reduce），并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题(使用磁盘交互，进度非常慢)。

**Spark** 应运而生，Spark 就是在传统的MapReduce计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的**RDD** 计算模型。
**Spark**将job的结果放到了**内存**当中，为下一次计算提供了更加便利的处理方式，所以Spark做迭代效率更高。

**Hadoop**适合处理静态数据，对于迭代式流式数据的处理能力差；
**Spark**通过在内存中缓存处理的数据，提高了处理流式数据和迭代式数据的性能；

#### 2.1.3 处理速度

**Hadoop**是磁盘级计算，**计算时需要在磁盘中读取数据**；其采用的是MapReduce的逻辑，把数据进行切片计算用这种方式来处理大量的离线数据.；

**Spark**它会在内存中以接近“实时”的时间完成所有的数据分析。Spark的批处理速度比MapReduce快近10倍，【内存中】的数据分析速度则快近100倍。

#### 2.1.4 启动速度

Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。

#### 2.1.5 中间结果存储

**Hadoop**中 ，中间结果存放在**HDFS**中，每次MR都需要刷写-调用
**Spark**中间结果存放优先存放在**内存**中，内存不够再存放在磁盘中，不放入HDFS，避免了大量的IO和刷写读取操作；

#### 2.1.6 根本差异

Spark 和Hadoop 的**根本差异**是**多个作业之间的数据通信问题** : Spark 多个作业之间数据通信是基于**内存**，而 Hadoop 是基于**磁盘**。
Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互

![image-20230122172316174](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172316174.png)

### 2.2 概念介绍

- RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布 式内存的一个抽象概念，提供了一种高度受限的共享内存模型 
- DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依 赖关系 
- Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task
- 应用（Application）：用户编写的Spark应用程序
- 任务（ Task ）：运行在Executor上的工作单元 
- 作业（ Job ）：一个作业包含多个RDD及作用于相应RDD上的各种操作 
- 阶段（ Stage ）：是作业的基本调度单位，一个作业会分为多组任务，每 组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间 没有Shuffle依赖关系的任务组成的任务集

Spark任务运行流程图（Spark对象代表对集群的一个连接）

![image-20230122172321304](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172321304.png)

（1）首先为应用构建起基本的 运行环境，即由Driver创建一个 SparkContext，进行资源的申请 、任务的分配和监控

（2）资源管理器为Executor分 配资源，并启动Executor进程 

（3）SparkContext根据RDD的 依赖关系构建DAG图，DAG图 提交给DAGScheduler解析成 Stage，然后把一个个TaskSet提 交给底层调度器TaskScheduler 处理；Executor向SparkContext 申请Task，Task Scheduler将 Task发放给Executor运行，并提 供应用程序代码 （4）Task在Executor上运行， 把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完毕后写 入数据并释放所有资源

#### 2.2.1 RDD

RDD典型的执行过程如下： 

（1）创建RDD对象

（2）SparkContext负责计算RDD之间的依赖关系，构建DAG； 

（3）DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个 Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。

![image-20230122172329803](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172329803.png)

RDD特性

Spark采用RDD以后能够实现高效计算的原因主要在 于： 

（1）高效的容错性 •现有容错机制：数据复制或者记录日志 •RDD：血缘关系、重新计算丢失分区、无需回滚 系统、重算过程在不同节点之间并行、只记录粗 粒度的操作 

（2）中间结果持久化到内存，数据在内存中的多个 RDD操作之间进行传递，避免了不必要的读写磁盘开销 

（3）存放的数据可以是Java对象，避免了不必要的对 象序列化和反序列化

RDD之间的依赖关系

**Shuffle**操作 

是否包含Shuffle操作是区分窄依赖和宽依赖的根据

![image-20230122172334327](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172334327.png)

**宽依赖与窄依赖**

![image-20230122175248603](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175248603.png)

#### 2.2.2 spark on yarn

架构如下：![image-20230122172344429](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172344429.png)

#### 2.2.3 spark submit

spark提交任务可以在本地运行，也可以在集群中运行，这里介绍spark提交任务到yarn集群。

**前提**：

假设集群资源：1台master，4个slave，每个机器40个Vcores，总共可用内存290G

举例：

spark-submit --master yarn --deploy-mode cluster --num-executors 30 --driver-memory 4g --executor-memory 9g --executor-cores 4 --conf spark.kryoserializer.buffer.max=512m --conf spark.executor.memoryOverhead=1024 /root/wj/word_count/pre_save_result.py 2>&1 | tee pre_save_result.output.logs



具体提交参数 

- executor-cores 每个 executor 的最大核数。根据经验实践，设定在 3~6 之间比较合理。 **当然具体也要根据集群机器的核心数和内存大小确定**
- num-executors 该参数值=每个节点的 executor 数 * work 节点数 每个 node 的 executor 数 = 单节点 yarn 总核数 / 每个 executor 的最大 cpu 核数 考虑到系统基础服务和 HDFS 等组件的余量，yarn.nodemanager.resource.cpu-vcores 配 置为：，参数 executor-cores 的值为：4，那么每个 node 的 executor 数 = 40/4 = 10,集 群节点为5，那么 num-executors = 5* 10 = 50
  - 这里必须注意每个节点如果可用资源不一样，出的executor就不一样
- executor-memory 该参数值=yarn-nodemanager.resource.memory-mb / 每个节点的 executor 数量 如果 yarn 的参数配置为 100G，那么每个 Executor 大概就是 100G/10≈10G,**同时要注意 yarn 配置中每个容器允许的最大内存是否匹配。**
- conf spark.executor.memoryOverhead，堆外内存

#### 2.2.4常见问题

##### 堆外内存设置

1堆外内存参数 讲到堆外内存，就必须去提一个东西，那就是去 yarn 申请资源的单位，容器。Spark on yarn 模式，一个容器到底申请多少内存资源。 一个容器最多可以申请多大资源，是由 yarn 参数 yarn.scheduler.maximum-allocationmb 决定， 需要满足： spark.executor.memoryOverhead + spark.executor.memory + spark.memory.offHeap.size ≤ yarn.scheduler.maximum-allocation-mb 参数解释：

 ➢ spark.executor.memory：提交任务时指定的堆内内存。 

➢ spark.executor.memoryOverhead：堆外内存参数，内存额外开销。 默认开启，默认值为 spark.executor.memory*0.1 并且会与最小值 384mb 做对比， 取最大值。所以 spark on yarn 任务堆内内存申请 1 个 g，而实际去 yarn 申请的内 存大于 1 个 g 的原因。 ➢ spark.memory.offHeap.size ： 堆 外 内 存 参 数 ， spark 中 默 认 关 闭 ， 需 要 将 spark.memory.enable.offheap.enable 参数设置为 true。 注意：很多网上资料说 spark.executor.memoryOverhead 包含 spark.memory.offHeap.size，

##### **故障排除一：控制 reduce 端缓冲大小以避免 OOM** 

在 Shuffle 过程，reduce 端 task 并不是等到 map 端 task 将其数据全部写入磁盘后再去 拉取，而是 map 端写一点数据，reduce 端 task 就会拉取一小部分数据，然后立即进行后面 的聚合、算子函数的使用等操作。 reduce 端 task 能够拉取多少数据，由 reduce 拉取数据的缓冲区 buffer 来决定，因为拉 取过来的数据都是先放在 buffer 中，然后再进行后续的处理，buffer 的默认大小为 48MB。 reduce 端 task 会一边拉取一边计算，不一定每次都会拉满 48MB 的数据，可能大多数 时候拉取一部分数据就处理掉了。 虽然说增大 reduce 端缓冲区大小可以减少拉取次数，提升 Shuffle 性能，但是有时 map 端的数据量非常大，写出的速度非常快，此时 reduce 端的所有 task 在拉取的时候，有 可能全部达到自己缓冲的最大极限值，即 48MB，此时，再加上 reduce 端执行的聚合函数 的代码，可能会创建大量的对象，这可难会导致内存溢出，即 OOM。 如果一旦出现 reduce 端内存溢出的问题，我们可以考虑减小 reduce 端拉取数据缓冲 区的大小，例如减少为 12MB。 在实际生产环境中是出现过这种问题的，这是典型的以性能换执行的原理。reduce 端 拉取数据的缓冲区减小，不容易导致 OOM，但是相应的，reudce 端的拉取次数增加，造成 更多的网络传输开销，造成性能的下降。 注意，要保证任务能够运行，再考虑性能的优化。

##### 故障排除二：JVM GC 导致的 shuffle 文件拉取失败

 在 Spark 作业中，有时会出现 shuffle file not found 的错误，这是非常常见的一个报错， 有时出现这种错误以后，选择重新执行一遍，就不再报出这种错误。 出现上述问题可能的原因是 Shuffle 操作中，后面 stage 的 task 想要去上一个 stage 的 task 所在的 Executor 拉取数据，结果对方正在执行 GC，执行 GC 会导致 Executor 内所有的 工作现场全部停止，比如 BlockManager、基于 netty 的网络通信等，这就会导致后面的 task 拉取数据拉取了半天都没有拉取到，就会报出 shuffle file not found 的错误，而第二次 再次执行就不会再出现这种错误。

可以通过调整 reduce 端拉取数据重试次数和 reduce 端拉取数据时间间隔这两个参数 来对 Shuffle 性能进行调整，增大参数值，使得 reduce 端拉取数据的重试次数增加，并且 每次失败后等待的时间间隔加长。

` val conf = new SparkConf() .set("spark.shuffle.io.maxRetries", "60") .set("spark.shuffle.io.retryWait", "60s")`

### 2.3 RDD编程基础

Spark采用textFile()方法来从文件系统中加载数据创建RDD

- 该方法把文件的URI作为参数，这个URI可以是： 
- 本地文件系统的地址 
- 或者是分布式文件系统HDFS的地址 •或者是Amazon S3的地址等             

**EX：**

**读入**：lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")

![image-20230122172353867](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172353867.png)

#### 2.3.1 创建RDD

![image-20230122175318394](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122175318394.png)

|       操作        |                             含义                             |
| :---------------: | :----------------------------------------------------------: |
|   filter(func)    |       筛选出满足函数func的元素，并返回一个新的数据 集        |
|     map(func)     |  将每个元素传递到函数func中，并将结果返回为一 个新的数据集   |
|   flatMap(func)   |   与map()相似，但每个输入元素都可以映射到0或多 个输出结果    |
|   groupByKey()    | 应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集 |
| reduceByKey(func) | 应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中每个值是将每个key传递到 函数func中进行聚合后的结果 |

图示：

**Map**

![image-20230122172359791](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172359791.png)

**flatMap**

![image-20230122172403966](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172403966.png)

**groupByKey**

![image-20230122172408695](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172408695.png)

**reduceByKey：**

![image-20230122174903653](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122174903653.png)

**综合案例**

~~~shell
>>> lines = sc. \
... textFile("file:///usr/local/spark/mycode/rdd/word.txt")
>>> wordCount = lines.flatMap(lambda line:line.split(" ")). \
... map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)
>>> print(wordCount.collect())
[('good', 1), ('Spark', 2), ('is', 3), ('better', 1), ('Hadoop', 1), ('fast', 1)]
~~~

![image-20230122172413103](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172413103.png)

#### 2.3.2 分区

**分区原则**：

RDD分区的一个原则是使得分区的个数尽量等于集群中的CPU核心 （core）数目

对于不同的Spark部署模式而言（本地模式、Standalone模式、 YARN模式、Mesos模式），都可以通过设置 spark.default.parallelism这个参数的值，来配置默认的分区数目，一 般而言：

本地模式：默认为本地机器的CPU数目，若设置了local[N],则默认 为N

*Apache Mesos：默认的分区数为8 

*Standalone或YARN：在“集群中所有CPU核心数目总和”和“2” 二者中取较大值作为默认值

**设置分区个数**：

（1）创建RDD时手动指定分区个数 在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法 格式如下： sc.textFile(path, partitionNum) 其中，path参数用于指定要加载的文件的地址，partitionNum参数用于 指定分区个数。

~~~python
>>> list = [1,2,3,4,5]
>>> rdd = sc.parallelize(list,2) //设置两个分区
~~~

（2）使用reparititon方法重新设置分区个数 通过转换操作得到新 RDD 时，直接调用 repartition 方法即可。例如：

~~~python
>>> data = sc.parallelize([1,2,3,4,5],2)
>>> len(data.glom().collect()) #显示data这个RDD的分区数量
2
>>> rdd = data.repartition(1) #对data这个RDD进行重新分区
>>> len(rdd.glom().collect()) #显示rdd这个RDD的分区数量
1
~~~



实例：

根据key值的最后一位数字，写到不同的文件 例如： 10写入到part-00000 11写入到part-00001 . . . 19写入到part-00009

~~~python
from pyspark import SparkConf, SparkContext
def MyPartitioner(key):
 print("MyPartitioner is running")
 print('The key is %d' % key)
 return key%10
def main():
 print("The main function is running")
 conf = SparkConf().setMaster("local").setAppName("MyApp")
 sc = SparkContext(conf = conf)
 data = sc.parallelize(range(10),5)
 data.map(lambda x:(x,1)) \
 .partitionBy(10,MyPartitioner) \
 .map(lambda x:x[0]) \
 .saveAsTextFile("file:///usr/local/spark/mycode/rdd/partitioner")
if __name__ == '__main__':
 main()
~~~

#### 2.3.3 数据读写

- 从本地文件读入

  - ~~~python
    >>> textFile = sc.\
    ... textFile("file:///usr/local/spark/mycode/rdd/word.txt")
    >>> textFile.first()
    'Hadoop is good'
    //读入
    >>> textFile = sc.\
    ... textFile("file:///usr/local/spark/mycode/rdd/word.txt")
    //保存
    >>> textFile.\
    ... saveAsTextFile("file:///usr/local/spark/mycode/rdd/writeback")
    ~~~

- 从分布式文件系统读写（文本文件格式）

  - ~~~python
    >>> textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")
    >>> textFile.first()
    
    >>> textFile.saveAsTextFile("writeback")
    ~~~

- 从HBase读入

  - HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、 列族、列限定符和时间戳 • 每个值是一个未经解释的字符串，没有数据类型 • 用户在表中存储数据，每一行都有一个可排序的行键和任意多的列 • 表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多 个列，同一个列族里面的数据存储在一起 • 列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义 列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行 数据类型转换
  - 表：HBase采用表来组织数据，表由行 和列组成，列划分为若干个列族 • 行：每个HBase表都由若干行组成，每 个行由行键（row key）来标识。 
  - 列族：一个HBase表被分组成许多“列 族”（Column Family）的集合，它是 基本的访问控制单元 
  - 列限定符：列族里的数据通过列限定符 （或列）来定位 
  - 单元格：在HBase表中，通过行、列族 和列限定符确定一个“单元格”（cell ），单元格中存储的数据没有数据类型 ，总被视为字节数组byte[] 
  - 时间戳：每个单元格都保存着同一份数 据的多个版本，这些版本采用时间戳进行索引
  - ![image-20230122172418980](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172418980.png)

### 2.4 Spark SQL

#### 2.4.1 shark 到spark SQL

​	Shark即Hive on Spark，为了实现与Hive兼容，Shark在 HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、 执行计划优化等逻辑，可以近似认为仅将物理执行计划从 MapReduce作业替换成了Spark作业，通过Hive的HiveQL解 析，把HiveQL翻译成Spark上的RDD操作

​	2014年6月1日Shark项目和Spark SQL项目的主持人Reynold Xin宣布： 停止对Shark的开发，团队将所有资源放在Spark SQL项目上，至此， Shark的发展画上了句号，但也因此发展出两个分支：Spark SQL和 Hive on Spar

- Spark SQL作为Spark生态的一 员继续发展，而不再受限于Hive， 只是兼容Hive 
- Hive on Spark是一个Hive的发 展计划，该计划将Spark作为 Hive的底层引擎之一，也就是说， Hive将不再受限于一个引擎，可 以采用Map-Reduce、Tez、 Spark等引擎

#### 2.4.2 Dataframe

DataFrame的推出，让Spark具备了处理大规模结构化数据的能力，不仅比原 有的RDD转化方式更加简单易用，而且获得了更高的计算性能 

Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询

**RDD与Dataframe比较**

![image-20230122172427794](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172427794.png)



- RDD是分布式的 Java对象的集合，但是，对象内部结构对于RDD而言却 是不可知的 
- DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息

从Spark2.0以上版本开始，Spark使用全新的SparkSession接口替代 Spark1.6中的SQLContext及HiveContext接口来实现其对数据加载、转 换、处理等功能。SparkSession实现了SQLContext及HiveContext所有 功能 

SparkSession支持从不同的数据源加载数据，并把数据转换成 DataFrame，并且支持把DataFrame转换成SQLContext自身中的表， 然后使用SQL语句来操作数据。

SparkSession亦提供了HiveQL以及其 他依赖于Hive的功能的支持 可以通过如下语句创建一个SparkSession对象：

~~~python
from pyspark import SparkContext,SparkConf
from pyspark.sql import SparkSession
spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()
~~~

创建Dataframe

在创建DataFrame时，可以使用spark.read操作，从不同类型的文 件中加载数据创建DataFrame，例如： 

•spark.read.format("text").load("people.txt")：读取文本文件 people.json创建DataFrame； •spark.read.format("json").load("people.json")：读取JSON文件 people.json创建DataFrame； •spark.read.format("parquet").load("people.parquet")：读取 Parquet文件people.parquet创建DataFrame。

**数据**：

假设有数据在“/usr/local/spark/examples/src/main/resources/”这个目录下，这个目录下有 两个样例数据people.json和people.txt。 people.json文件的内容如下： 

`{"name":"Michael"}`

`{"name":"Andy", "age":30}`

`{"name":"Justin", "age":19}`

people.txt文件的内容如下： 

`Michael, 29` 

`Andy, 30` 

`Justin, 19`

保存Dataframe

下面从示例文件people.json中创建一个DataFrame，名称为 peopleDF，把peopleDF保存到另外一个JSON文件中，然后，再从 peopleDF中选取一个列（即name列），把该列数据保存到一个文本 文件中

~~~python
>>> peopleDF = spark.read.format("json").\
... load("file:///usr/local/spark/examples/src/main/resources/people.json")
>>> peopleDF.select("name", "age").write.format("json").\
... save("file:///usr/local/spark/mycode/sparksql/newpeople.json")
>>> peopleDF.select("name").write.format("text").\
... save("file:///usr/local/spark/mycode/sparksql/newpeople.txt")
~~~

会新生成一个名称为newpeople.json的目录（不是文件）和一个名称 为newpeople.txt的目录（不是文件） part-00000-3db90180-ec7c-4291-ad05-df8e45c77f4d.json _SUCCESS

常用函数：

printSchema()，select()，filter()，groupBy等等

#### 2.4.3 RDD转换为Dataframe

利用反射机制推断RDD模式（schema）

示例：

~~~python
>>> from pyspark.sql import Row
>>> people = spark.sparkContext.\
... textFile("file:///usr/local/spark/examples/src/main/resources/people.txt").\
... map(lambda line: line.split(",")).\
... map(lambda p: Row(name=p[0], age=int(p[1])))
>>> schemaPeople = spark.createDataFrame(people)
#必须注册为临时表才能供下面的查询使用
>>> schemaPeople.createOrReplaceTempView("people")
>>> personsDF = spark.sql("select name,age from people where age > 20")
#DataFrame中的每个元素都是一行记录，包含name和age两个字段，分别用
p.name和p.age来获取值
>>> personsRDD=personsDF.rdd.map(lambda p:"Name: "+p.name+ ","+"Age:
"+str(p.age))
>>> personsRDD.foreach(print)
Name: Michael,Age: 29
Name: Andy,Age: 30
~~~

自动推断过程：

![image-20230122172434466](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172434466.png)

**使用编程方式定义RDD模式**

当无法提前获知数据结构时，就需要采用编程方式定义RDD模式。 比如，现在需要通过编程方式把people.txt加载进来生成 DataFrame，并完成SQL查询。

![image-20230122172438728](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172438728.png)

~~~python
>>> from pyspark.sql.types import *
>>> from pyspark.sql import Row
#下面生成“表头”
>>> schemaString = "name age"
>>> fields = [StructField(field_name, StringType(), True) for field_name in
schemaString.split(" ")]
>>> schema = StructType(fields)
#下面生成“表中的记录”
>>> lines = spark.sparkContext.\
... textFile("file:///usr/local/spark/examples/src/main/resources/people.txt")
>>> parts = lines.map(lambda x: x.split(","))
>>> people = parts.map(lambda p: Row(p[0], p[1].strip()))
#下面把“表头”和“表中的记录”拼装在一起
>>> schemaPeople = spark.createDataFrame(people, schema)
~~~

![image-20230122172444117](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230122172444117.png)

## 3 Spark Streaming

### 3.1 静态数据和流数据

**静态数据**：

很多企业为了支持决策分析而构建的数据仓库系统，其中 存放的大量历史数据就是静态数据。技术人员可以利用数 据挖掘和OLAP（On-Line Analytical Processing）分析工 具从静态数据中找到对企业有价值的信息

**流数据**：

近年来，在Web应用、网络监控、传感监测等领域，兴 起了一种新的数据密集型应用——流数据，即数据以大量 、快速、时变的流形式持续到达 。实例：PM2.5检测、电子商务网站用户点击流 

静态数据和流数据 流数据具有如下特征： 

- 数据快速持续到达，潜在大小也许是无穷无尽的 
- 数据来源众多，格式复杂 
- 数据量大，但是不十分关注存储，一旦经过处理，要 么被丢弃，要么被归档存储 
- 注重数据的整体价值，不过分关注个别数据 
- 数据顺序颠倒，或者不完整，系统无法控制将要处理 的新到达的数据元素的顺序

对静态数据和流数据的处理，对应着两种截然不同的计算模式：**批量 计算和实时计算**

**批量计算**

充裕时间处理静态数据， 如Hadoop

**实时计算:**

**流数据不适合采用批量计算，**因为流数据不适合用传统的关系模型建模 

流数据必须采用实时计算，响应时间为秒级 

数据量少时，不是问题，但是，在大数据时代，数据格式复杂、来源众多、 数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计 算——流计算，应运而生

**流计算**：

**针对流数据的实时计算**。**实时获取来自不同数据源的海量数据**，经过实时 分析处理，获得有价值的信息

流计算秉承一个基本理念，**即数据的价值随着时间的流逝 而降低**，如用户点击流。因此，当事件出现时就应该立即 进行处理，而不是缓存起来进行批量处理。为了及时处理 流数据，就需要一个低延迟、可扩展、高可靠的处理引擎

特点：

对于一个流计算系统来说，它应达到如下需求：

- 高性能：处理大数据的基本要求，如每秒处理几十万 条数据 
- 海量式：支持TB级甚至是PB级的数据规模 
- 实时性：保证较低的延迟时间，达到秒级别，甚至是 毫秒级别 
- 分布式：支持大数据的基本架构，必须能够平滑扩展 
- 易用性：能够快速进行开发和部署 
- 可靠性：能可靠地处理流数据

举例：

较为常见的是开源流计算框架，代表如下：

– Twitter Storm：免费、开源的分布式实时计算系统，可简单、高 效、可靠地处理大量的流数据

– Yahoo! S4（Simple Scalable Streaming System）：开源流计算 平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的 流式系统

### 3.2 流计算处理过程

流计算的处理流程一般包含三个阶段：数据实时采集、数据实时计算 、实时查询服务![image-20230114172510455](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114172510455.png)

### 3.3 Spark Streaming简介

Spark Streaming的基本原理是将实时输入数据流以时 间片（秒级）为单位进行拆分，然后经Spark引擎以类 似批处理的方式处理每个时间片数据![image-20230114173002042](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173002042.png)

Spark Streaming最主要的抽象是**DStream（Discretized Stream，离散化数据 流）**，表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按 照时间片（如1秒）分成一段一段**，每一段数据转换为Spark中的RDD，这些分 段就是Dstream，**并且对DStream的操作都最终转变为对相应的RDD的操作

![image-20230114173047816](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173047816.png)

Spark Streaming和Storm最大的区别在于，**Spark Streaming无法实现毫秒级的流计算，而Storm可以实 现毫秒级响应**

Spark Streaming采用的小批量处理的方式使得它可 以同时兼容批量和实时数据处理的逻辑和算法，因此， 方便了一些需要历史数据和实时数据联合分析的特定 应用场合

**案例**：

![image-20230114173616774](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173616774.png)

- 在Spark Streaming中，会有一个组件Receiver，作为一个长期运 行的task跑在一个Executor上

- 每个Receiver都会负责一个input DStream（比如从文件中读取数据 的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等） 

- Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据

  ![image-20230114173657107](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114173657107.png)

编写Stream数据基本步骤：

1. 通过创建输入DStream来定义输入源 
2. 通过对DStream应用转换操作和输出操作来定义流计算 
3. 用streamingContext.start()来开始接收数据和处理流程 
4. 通过streamingContext.awaitTermination()方法来等待 处理结束（手动结束或因为错误而结束） 
5. 可以通过streamingContext.stop()来手动结束流计算 进程

EX：

如果要运行一个Spark Streaming程序，就需要首先生成一个 StreamingContext对象，它是Spark Streaming程序的主入口

~~~python
from pyspark import SparkContext, SparkConf
from pyspark.streaming import StreamingContext
conf = SparkConf()
conf.setAppName('TestDStream')
conf.setMaster('local[2]')
sc = SparkContext(conf = conf)
ssc = StreamingContext(sc, 1)
~~~

### 3.4 输入源

**文件流** 

- 在pyspark中创建文件流

  ~~~python
  $ cd /usr/local/spark/mycode
  $ mkdir streaming
  $ cd streaming
  $ mkdir logfile
  $ cd logfile
  
  
  
  >>> from pyspark import SparkContext
  >>> from pyspark.streaming import StreamingContext
  >>> ssc = StreamingContext(sc, 10)
  >>> lines = ssc. \
  ... textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
  >>> words = lines.flatMap(lambda line: line.split(' '))
  >>> wordCounts = words.map(lambda x :
  (x,1)).reduceByKey(lambda a,b:a+b)
  >>> wordCounts.pprint()
  >>> ssc.start()
  >>> ssc.awaitTermination()
  上面在pyspark中执行的程序，一旦你输入ssc.start()以后，程序就开
  始自动进入循环监听状态。
  
  ~~~

- 采用独立应用方式新建

  ~~~python
  $ cd /usr/local/spark/mycode
  $ cd streaming
  $ cd logfile
  $ vim FileStreaming.py
  
  
  from pyspark import SparkContext, SparkConf
  from pyspark.streaming import StreamingContext
  conf = SparkConf()
  conf.setAppName('TestDStream')
  conf.setMaster('local[2]')
  sc = SparkContext(conf = conf)
  ssc = StreamingContext(sc, 10)
  lines = ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
  words = lines.flatMap(lambda line: line.split(' '))
  wordCounts = words.map(lambda x : (x,1)).reduceByKey(lambda a,b:a+b)
  wordCounts.pprint()
  ssc.start()
  ssc.awaitTermination()
  
  $ cd /usr/local/spark/mycode/streaming/logfile/
  $ /usr/local/spark/bin/spark-submit FileStreaming.py
  $ cd /usr/local/spark/mycode/streaming/logfile/
  $ /usr/local/spark/bin/spark-submit FileStreaming.py
  ~~~

下面的以后再补

**套接字流** 

**RDD**队列流

高级数据源KafKa

### **3.5** **Struct Streaming**

Structured Streaming的关键思想是将实时数据流视为一张正 在不断添加数据的表 

可以把流计算等同于在一个静态表上的批处理查询，Spark会 在不断添加数据的无界输入表上运行计算，并进行增量查询

![image-20230114191114903](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191114903.png)

在无界表上对输入的查询将生成结果表，系统每隔一定的周期会 触发对无界表的计算并更新结果表

![image-20230114191141121](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191141121.png)

#### 3.5.1 两种处理模型

**微批处理模型**：

Structured Streaming默认使用微批处理执行模型，这意味着Spark 流计算引擎会定期检查流数据源，并对自上一批次结束后到达的新 数据执行批量查询 

数据到达和得到处理并输出结果之间的延时超过100毫秒

**持续处理模型**：

Spark从2.3.0版本开始引入了持续处理的试验性功能，可以实现流计 算的毫秒级延迟 

在持续处理模式下，Spark不再根据触发器来周期性启动任务，而是 启动一系列的连续读取、处理和写入结果的长时间运行的任务

![image-20230114191303795](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114191303795.png)

**Struct Streaming 和Spark Streaming对比**：

Structured Streaming处理的数据跟Spark Streaming一样，也是源 源不断的数据流，区别在于，Spark Streaming采用的数据抽象是 DStream（本质上就是一系列RDD），而Structured Streaming采用 的数据抽象是DataFrame。 

Structured Streaming可以使用Spark SQL的DataFrame/Dataset来 处理数据流。虽然Spark SQL也是采用DataFrame作为数据抽象， 但是，Spark SQL只能处理静态的数据，而Structured Streaming可 以处理结构化的数据流。这样，Structured Streaming就将Spark SQL和Spark Streaming二者的特性结合了起来。

#### **3.5.2实例**

编写Structured Streaming程序的基本步骤包括： 

- 导入pyspark模块 
- 创建SparkSession对象 
- 创建输入数据源 
- 定义流计算过程 
- 启动流计算并输出结果

~~~python
#实例任务：一个包含很多行英文语句的数据流源源不断
#到达，Structured Streaming程序对每行英文语句进行
#拆分，并统计每个单词出现的频率

一导入数据
导入PySpark模块，代码如下：
from pyspark.sql import SparkSession
from pyspark.sql.functions import split
from pyspark.sql.functions import explode
由于程序中需要用到拆分字符串和展开数组内的所有单词的功能，
所以引用了来自pyspark.sql.functions里面的split和explode函数。

2.步骤2：创建SparkSession对象
创建一个SparkSession对象，代码如下：
if __name__ == "__main__":
 spark = SparkSession \
 .builder \
 .appName("StructuredNetworkWordCount") \
 .getOrCreate()
 spark.sparkContext.setLogLevel('WARN')
    
3．步骤3：创建输入数据源
创建一个输入数据源，从“监听在本机（localhost）的9999端口上
的服务”那里接收文本数据，具体语句如下：
 lines = spark \
 .readStream \
 .format("socket") \
 .option("host", "localhost") \
 .option("port", 9999) \
 .load()
4.步骤4：定义流计算过程
有了输入数据源以后，接着需要定义相关的查询语句，具体如下：
 words = lines.select(
 explode(
 split(lines.value, " ")
 ).alias("word")
 )
 wordCounts = words.groupBy("word").count()

5.步骤5：启动流计算并输出结果
定义完查询语句后，下面就可以开始真正执行流计算，具体语句如下：
 query = wordCounts \
 .writeStream \
 .outputMode("complete") \
 .format("console") \
 .trigger(processingTime="8 seconds") \
 .start()
 query.awaitTermination()


把代码写入文件StructuredNetworkWordCount.py
在执行StructuredNetworkWordCount.py之前，需要启动HDFS。
启动HDFS的命令如下：

cd /usr/local/spark/mycode/structuredstreaming/
$ /usr/local/spark/bin/spark-submit StructuredNetworkWordCount.py	
~~~

## 4 Spark MLib

### 4.1Spark MLib简介

Spark提供了一个基于海量数据的机器学习库，它提供 了常用机器学习算法的分布式实现

​	pyspark的即席查询也是一个关键。算法工程师可以边 写代码边运行，边看结果

- 需要注意的是，MLlib中只包含能够在集群上运行良好 的并行算法，这一点很重要 
- 有些经典的机器学习算法没有包含在其中，就是因为它 们不能并行执行 
- 相反地，一些较新的研究得出的算法因为适用于集群， 也被包含在MLlib中，例如分布式随机森林算法、最小交 替二乘算法。这样的选择使得MLlib中的每一个算法都适 用于大规模数据集

MLlib是Spark的机器学习（Machine Learning）库，旨在 简化机器学习的工程实践工作 

•MLlib由一些通用的学习算法和工具组成，包括分类、回 归、聚类、协同过滤、降维等，同时还包括底层的优化原 语和高层的流水线（Pipeline）API，具体如下： 

- 算法工具：常用的学习算法，如分类、回归、聚类和协 同过滤； 
- 特征化工具：特征提取、转化、降维和选择工具； 
- 流水线(Pipeline)：用于构建、评估和调整机器学习工 作流的工具; •持久性：保存和加载算法、模型和管道; 
- 实用工具：线性代数、统计、数据处理等工具。

支持算法举例：

![image-20230114194027142](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230114194027142.png)