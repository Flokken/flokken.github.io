---
title: 一个简单的scrapy爬虫站长之家
date: 2023-03-01 00:00:00
tags: 
  - Scrapy
categories: 
  - Spider
  - Scrapy框架
permalink: /pages/06f4dd/
---

>参考链接：
>
>https://www.cnblogs.com/blogwd/p/15780478.html
>
>https://docs.scrapy.org/en/latest/

## 一 查看网站，明确爬取信息

爬取网站，站长之家的行业网站排行榜的信息。链接https://top.chinaz.com/all/

![image-20230306164328494](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230306164328494.png)

需要爬取图中标出五个字段：

- web_name
- rank
- domin
- description
- score

## 二构建项目

```scrapy startproject ZhanZhangZhiJia```新建一个项目

![image-20230301104700817](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230301104700817.png)

解释一下目录结构：

```text
scrapy_demo
├── scrapy_demo
│   ├── items.py       # 数据模型文件
│   ├── middlewares.py # 中间件文件，配置所有中间件
│   ├── pipelines.py   #  pipeline 文件，用于存放自定义pipeline的处理逻辑，比如配置保存数据库的逻辑
│   ├── settings.py    # 项目的配置文件，自定义的外部配置都可以放在这里
│   └── spiders        # Spider类文件夹，我们编写的解析代码均存放在这里
└── scrapy.cfg         # 项目的部署配置文件

```

可以用scrapy shell测试能否获取网页源码

```
scrapy shell https://top.chinaz.com/all/ --nolog
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x0000011BEAE84748>
[s]   item       {}
[s]   request    <GET https://top.chinaz.com/hangye/index_wangluo.html>
[s]   response   <200 https://top.chinaz.com/hangye/index_wangluo.html>
[s]   settings   <scrapy.settings.Settings object at 0x0000011BEB0F1B88>
[s]   spider     <TechWebSpider 'tech_web' at 0x11beb538488>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
In [1]: 
```

此时你可以使用 `response.text` 来检查我们是否获取了整个页面的源码，scrapy的所有资源解析操作都被集成在了`response`这个对象中，使用 `Tab` 建可以提示补全相关的内容。

### **spider编写**

首先使用 scrapy genspider demoSpider https://top.chinaz.com/all/  创建一个爬虫如下

![image-20230306154731225](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230306154731225.png)

并且我们需要爬取的列表在这里，使用xpath定位资源

![image-20230306155104687](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230306155104687.png)

xpath规则如下：

| 表达式 | 说明                           | 举例                                                         |
| ------ | ------------------------------ | ------------------------------------------------------------ |
| `/`    | 从当前节点选取直接子节点       | **/html/div/span**                                           |
| `//`   | 从当前节点选取子孙节点         | **//input**                                                  |
| `.`    | 选取当前节点                   |                                                              |
| `..`   | 选取当前节点的父节点           | **//input/..** 会选取 input 的父节点                         |
| `@`    | 选取属性，或者根据属性选取     | **//input[@data]** 选取具备 data 属性的 input 元素   //@data 选取所有 data 属性 |
| `*`    | 通配符，表示任意节点或任意属性 |                                                              |

除此之外，xpath表达式中可以使用text()来获取文本。

并且该网站的url构造十分简单，第一页是https://top.chinaz.com/all/index.html，第二页是https://top.chinaz.com/all/index_2.html,因此可以根据url构造规律来爬取接下来的网页。

最终，demoSpider的代码如下：

```python
import scrapy
import datetime
from hashlib import md5
from ..items import ZhanzhangzhijiaItem
class Demospider(scrapy.Spider):
    name = "demoSpider"
    allowed_domains = ["top.chinaz.com"]
    start_urls = ["http://top.chinaz.com/all/index.html"]
    url = 'https://top.chinaz.com/all/index_{}.html'
    pagesize = 1

    def parse(self, response):
        li_list = response.xpath('//ul[@class="listCentent"]/li')#这样会返回所有的li
        for li in li_list:
            web_name = li.xpath('.//h3/a/text()').get()#因为li中是已经提取出的列表，因此加上.表示从当前节点也就是li开始提取
            domain = li.xpath('.//h3/span/text()').get()#h3下面有多个span，默认匹配第一个，正好第一个就是domain
            description = li.xpath('./div[@class="CentTxt"]/p/text()').get(default='')
            rank = li.xpath('.//div[@class="RtCRateCent"]/strong/text()').get(default='')
            score = li.xpath('.//div[@class="RtCRateCent"]/span/text()').get(default='')
            # 封装数据
            item = ZhanzhangzhijiaItem()
            date = datetime.datetime.now()
            item['_id'] = md5(str(web_name).encode('utf-8')).hexdigest()
            item['web_name'] = web_name
            item['domain'] = domain
            item['abstract'] = description
            item['rank'] = rank
            item['score'] = score
            item['create_time'] = date
            item['update_time'] = date
            yield item
            # 构造下一页的请求
        self.pagesize = self.pagesize + 1
        url = self.url.format(self.pagesize)
        if self.pagesize<40:
            yield scrapy.Request(url=url, callback=self.parse)
```

运行spider,注意是name不是文件名

```shell
scrapy crawl demoSpider
```

但是这样只会在控制台输出结果，因此需要保存爬取的文件

可以用-o命令指定保存文件

```
Scrapy` 提供了许多`Feed exports`的方法，可以将输出数据保存为`json, json lines, csv, xml
```

在启动命令后面加 `-o xx.json` 就可以将文件保存为`json`格式。

例如使用如下命令将抓取的数据保存到一个 `json` 文件中：

```shell
scrapy crawl demoSpider -o result.json
```

打开保存的 json 文件，发现出现了乱码，出于历史原因，JSON 输出使用安全数字编码（`\uXXXX`序列），如果想要 UTF-8 用于 JSON，请使用 `FEED_EXPORT_ENCODING = 'utf-8'`。

### **书写Pipeline文件**

其次需要介绍一下 `scrapy` 的 `pipeline` ，在每一个item 被抓取之后，都会被发送到 `pipeline` 中，每个 `pipeline` 都是一个实现简单方法的 `python` 类，

它们接收一个 `item` 并对其执行操作，同时决定该 `item` 是应该继续进入下一个 `pipeline` 还是被丢弃不再处理。

**pipeline 的典型用途如下：**

- 清洗 HTML 数据
- 验证抓取的数据（检查项目是否包含某些字段）
- 检查重复项（并删除它们）
- 将抓取的项目存储在数据库中

**编写自己的 pipeline**

每个 pipeline 组件都是一个必须实现 `process_item` 方法的 Python 类：

- **process_item ( \*self\* , \*item\* , \*spider\* )[¶参考](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#process_item)**

  处理每个 item 都会调用此方法。item是一个[item 对象](https://docs.scrapy.org/en/latest/topics/items.html#item-types)，请参阅 [支持所有项目类型](https://docs.scrapy.org/en/latest/topics/items.html#supporting-item-types)。[`process_item()`](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#process_item)必须要么：返回一个[项目对象](https://docs.scrapy.org/en/latest/topics/items.html#item-types)，返回一个[`Deferred`](https://twistedmatrix.com/documents/current/api/twisted.internet.defer.Deferred.html)或引发 [`DropItem`](https://docs.scrapy.org/en/latest/topics/exceptions.html#scrapy.exceptions.DropItem)异常。丢弃的项目不再由进一步的 pipeline 组件处理。

  参数

  - **item** ( [item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types) ) – 抓取的项目
  - **spider** ( [`Spider`](https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider)object) – 抓取物品的spider

  返回值

  - 返回一个 [Item 对象](https://docs.scrapy.org/en/latest/topics/items.html#item-types)，让后续的 pipeline 处理
  - 返回一个[`Deferred`](https://twistedmatrix.com/documents/current/api/twisted.internet.defer.Deferred.html)或引发 [`DropItem`](https://docs.scrapy.org/en/latest/topics/exceptions.html#scrapy.exceptions.DropItem)异常，丢弃 item 不再由后续的 pipeline 组件处理。

此外，它们还可以实现以下方法：

- **open_spider（self，spider）**[¶](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#open_spider)

  这个方法在 spider 打开时被调用。参数**spider**– 打开的 spider

- **close_spider（self，spider）**[¶](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#close_spider)

  当 spider 关闭时调用此方法。参数 **spider** – 关闭的spider

- **from_crawler ( \*cls\* , \*crawler\* )**[¶参考](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#from_crawler)

  如果存在，必须返回 pipeline 的新实例，通常在这个方法中传入一些外部配置，构造一个新的 pipeline 实例。Crawler 对象提供对所有 Scrapy 核心组件的访问，如 settings 和 signals ；这是 pipeline 访问它们并将其功能挂钩到 Scrapy 的一种方式。参数**crawler** ( [`Crawler`](https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler)object) – 使用这个 pipeline 的爬虫。

知道了 pipeline 的作用和定义方法后，我们定义一个保存数据到 `mongodb` 的 `pipeline` ，如下所示：

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
import pymongo
from itemadapter import ItemAdapter


class ZhanzhangzhijiaPipeline:
    def process_item(self, item, spider):
        return item

class saveToMongoPipelines:
    def __init__(self,mongo_url,mongo_db,collection):
        self.mongo_url=mongo_url
        self.mongo_db = mongo_db
        self.collection = collection
    @classmethod
    def from_crawler(cls,crawler):#参数cls就是class的缩写，他相当于__init__，因此需要给里面的参数传参
        """
          Scrapy会先通过getattr判断我们是否自定义了from_crawler,有则调它来完
          成实例化,早于__init__方法执行
  　　　　 自己要的参数要去settings.py文件配置　　　　
        """
        env = crawler.settings.get('ENV','local')
        config = crawler.settings.get('CONFIG')
        config = config[env]
        mongo_url = config.MONGO_URL
        mongo_db = config.MONGODB_DB
        collection = config.COLLECTION_NAME
        return cls(
            mongo_url = mongo_url,
            mongo_db = mongo_db,
            collection = collection
        )
    def  open_spider(self,spider):
        """
        打开spider的时候调用一次，可以在这里创建连接
        """
        self.client = pymongo.MongoClient(self.mongo_url)
        self.db = self.client[self.mongo_db]
        print(self.client)
    def process_item(self,item,spider):
        """
        每一个item都会调用的方法，可以在这里清洗数据，保存到数据库
        :return:
        """
        adapter = ItemAdapter(item)
        coll = self.db[self.collection]
        # 使用 ItemAdapter 的 asdict() 方法可以处理嵌套的 item 格式，获取 json 字符串
        #下面的代码为了查重
        doc = adapter.asdict()
        count = coll.count_documents({'_id': doc.get('_id')})
        if count == 0:
            coll.insert_one(doc)
        else:
            del doc['create_time']
            coll.update_one({"_id": doc.get('_id')}, {'$set': doc})
        return item

    def close_spider(self,spider):
        """
        spider关闭时调用一次，可以关闭连接，释放资源
        """
        self.client.close()

```

`ITEM_PIPELINES` 是一个字典，它的 `key` 是 `pipeline` 的类路径，它的值是一个数字， 这个数字决定了 pipeline 的执行顺序，它的执行顺序为从低到高，数字越大越后执行，自定义的数字范围为 `0 - 1000`。

上述的pipeline 中的 `from_crawler` 方法使用了 settings 中配置的 `mongodb` 的地址，`settings.py` 文件的配置如下所示：

```python
# Scrapy settings for ZhanZhangZhiJia project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = "ZhanZhangZhiJia"

SPIDER_MODULES = ["ZhanZhangZhiJia.spiders"]
NEWSPIDER_MODULE = "ZhanZhangZhiJia.spiders"


# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.70'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'User-Agent': USER_AGENT
}
# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
#    "Accept-Language": "en",
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    "ZhanZhangZhiJia.middlewares.ZhanzhangzhijiaSpiderMiddleware": 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    "ZhanZhangZhiJia.middlewares.ZhanzhangzhijiaDownloaderMiddleware": 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    "scrapy.extensions.telnet.TelnetConsole": None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
    "ZhanZhangZhiJia.pipelines.saveToMongoPipelines":300
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = "httpcache"
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = "scrapy.extensions.httpcache.FilesystemCacheStorage"

class LocalConfig:
    # 本地环境mongeDB地址
    MONGODB_HOST = 'localhost:27017'
    # MONGODB_USERNAME = 'root'
    # MONGODB_PASSWORD = 'password123'
    MONGODB_DB = 'admin'
    COLLECTION_NAME = 'demoScrapy'
    MONGO_URL = "mongodb://{server}/{database}". \
        format( server=MONGODB_HOST, database=MONGODB_DB)
    # format(username=MONGODB_USERNAME, password=MONGODB_PASSWORD, server=MONGODB_HOST, database=MONGODB_DB)
class DevelopmentConfig:
    # 开发环境mongeDB地址
    MONGODB_HOST = 'localhost:27017'
    MONGODB_USERNAME = 'root'
    MONGODB_PASSWORD = 'password123'
    MONGODB_DB = 'admin'
    COLLECTION_NAME = 'demoScrapy'
    MONGO_URI = "mongodb://{server}/{database}". \
        format(username=MONGODB_USERNAME, password=MONGODB_PASSWORD, server=MONGODB_HOST, database=MONGODB_DB)


CONFIG = {
    'local': LocalConfig,
    'dev': DevelopmentConfig,
}

ENV = 'local'


# Set settings whose default value is deprecated to a future-proof value
REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
FEED_EXPORT_ENCODING = "utf-8"

```

控制台scrapy crawl demoSpider即可将数据保存到mongoDB

![image-20230306222040270](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230306222040270.png)

### 部署定时爬虫

