---
title: Scrapy爬取某云的所有云服务
date: 2023-03-07 00:00:00
tags: 
  - Scrapy
categories: 
  - Spider
  - 爬虫
permalink: /pages/3e9992/
---

>参考：
>
>https://playwright.dev/docs
>
>python3网络爬虫实践
>
>https://docs.scrapy.org/en/latest/topics/downloader-middleware.html

###  调试网页

首先我去测试官网首页的数据，发现只有鼠标悬浮的时候才有下拉列表，并且划开就会消失

![image-20230307171156563](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230307171156563.png)

因此猜测使用了ajax来传输数据，然后再用js对数据渲染后插入到页面中。

右键点击检查，点击网络，点击xhr，由于我什么都没点击，所以这里还是空的。

![image-20230307171404884](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230307171404884.png)

但我鼠标悬浮到产品上面时，就会发现下面有了一些数据，其中json文件显然是数据

![image-20230307171605656](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230307171605656.png)

双击那个json文件，可以发现就是传输的数据文件。

![image-20230307171644004](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230307171644004.png)

但是这个样子的数据很不利于我们清洗，因此考虑用**playwright**模拟动作后，通过解析返回的页面来爬取数据。

**Scrapy录制操作生成代码**

playwright的这个功能十分方便，命令行运行`playwright codegen -o aliproduct.py -b chromium`来使用chromium录制我们的行为，并且保存到aliproduct中。

运行上面的命令后，访问https://aliyun.com然后点击产品，关闭,会生成如下代码，不过为了方便查看，我将结果保存到了result.txt文件中

```python
from playwright.sync_api import Playwright, sync_playwright, expect


def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://www.aliyun.com/")
    page.wait_for_load_state('networkidle', timeout=0)
    page.get_by_role("link", name="产品", exact=True).click()
    body = page.content()
    with open('result.txt','w+',encoding='utf-') as f:
        f.write(body)
        f.close()
    page.close()
    # ---------------------
    context.close()
    browser.close()


with sync_playwright() as playwright:
    run(playwright)

```

但是很可惜，我发现一个麻烦的问题，playwright不管是click()还是hover()都不能唤起产品页面，爬取的页面代码中也没有想要的信息。

因此放弃直接从官网首页爬取，过于麻烦。

不过经过寻找，发现可以爬取https://www.aliyun.com/product/list

![image-20230307172615717](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230307172615717.png)

他这里可以通过点击来切换不同大类的标签，进而爬取下面的数据，**这个也是动态渲染的，需要模拟点击操作后**才会有接下来的数据。

首先录制的代码如下：

~~~python
from playwright.sync_api import Playwright, sync_playwright, expect


def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://www.aliyun.com/product/list")
    page.get_by_role("tab", name="计算", exact=True).locator("div").nth(1).click()
    page.get_by_role("tab", name="存储").locator("div").nth(1).click()
    page.get_by_role("tab", name="网络").locator("div").nth(1).click()
    page.get_by_role("tab", name="安全").locator("div").nth(1).click()
    page.get_by_role("tab", name="容器与中间件").locator("div").nth(1).click()
    page.get_by_role("tab", name="数据库").locator("div").nth(1).click()
    page.get_by_role("tab", name="大数据计算").locator("div").nth(1).click()
    page.get_by_role("tab", name="人工智能与机器学习").locator("div").nth(1).click()
    page.get_by_role("tab", name="CDN 与云通信").locator("div").nth(1).click()
    page.get_by_role("tab", name="企业服务与媒体服务").locator("div").nth(1).click()
    page.get_by_role("tab", name="物联网").locator("div").nth(1).click()
    page.get_by_role("tab", name="开发工具").locator("div").nth(1).click()
    page.get_by_role("tab", name="运维管理").locator("div").nth(1).click()
    page.get_by_role("tab", name="专有云").locator("div").nth(1).click()
    page.close()

    # ---------------------
    context.close()
    browser.close()


with sync_playwright() as playwright:
    run(playwright)

~~~

接下来通过downloadmiddleware来实现渲染：

主要有以下几个函数：

**process_request****(***request***,** *spider***)**[¶](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request)

他有四种返回值

- return `None`, 此时scrapy会继续处理这个请求，执行其他的中间件，和其他的下载中间件
- return a `Response` object, 那么scrapy不会再执行其他的[`process_request()`](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request) or [`process_exception()`](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception) methods，并且对应的process_response也会被调用
- return a [`Request`](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request) object  Scrapy 不会调用任何其他 process_request() 或 process_exception() 方法，或适当的下载函数；它会返回那个响应。安装的中间件的 process_response() 方法总是在每次响应时调用
- raise [`IgnoreRequest`](https://docs.scrapy.org/en/latest/topics/exceptions.html#scrapy.exceptions.IgnoreRequest). 将调用安装的下载器中间件的 process_exception() 方法。如果它们都没有处理异常，则调用请求的 errback 函数 (Request.errback)。如果没有代码处理引发的异常，它将被忽略并且不会被记录（与其他异常不同）

**process_response**(*request*, *response*, *spider*)[¶](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response)

- 如果它返回一个 Response（它可能是相同的给定响应，或者一个全新的响应），该响应将继续使用链中下一个中间件的 process_response() 进行处理
- 如果它返回一个 Request 对象，中间件链将停止，返回的请求将重新安排在将来下载。这与从 process_request() 返回请求的行为相同。
- 如果它引发 IgnoreRequest 异常，则调用请求的 errback 函数 (Request.errback)。如果没有代码处理引发的异常，它将被忽略并且不会被记录（与其他异常不同）。

**process_exception**(*request*, *exception*, *spider*)[¶](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception)

- 当下载处理程序或 process_request()（来自下载器中间件）引发异常（包括 IgnoreRequest 异常）时，Scrapy 调用 process_exception()
- 如果它返回 None，Scrapy 将继续处理这个异常，执行已安装中间件的任何其他 process_exception() 方法，直到没有中间件留下并且默认异常处理开始。
- 如果它返回一个 Response 对象，则启动已安装中间件的 process_response() 方法链，Scrapy 将不会调用任何其他中间件的 process_exception() 方法。

**from_crawler(*cls*, *crawler*)**[¶](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=download#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler)

如果存在，则调用此类方法以从 Crawler 创建中间件实例。它必须返回一个新的中间件实例。爬虫对象提供对所有 Scrapy 核心组件的访问，如设置和信号；这是中间件访问它们并将其功能挂接到 Scrapy 的一种方式。

### **playwright集成到scrapy出错：**

上面的代码直接可以跑，但是集成到scrapy里的middlewares时，会报错

~~~python
 def process_request(self, request: scrapy.Request, spider):
     with sync_playwright() as p:
       browser =  p.chromium.launch(headless=False)
       page =  browser.new_page()
        page.goto(request.url)
        page.wait_for_load_state('networkidle',timeout=0)
       for i in self.name_list:
            page.get_by_role("tab", name=i).locator("div").nth(1).click()
            page.wait_for_load_state('networkidle',timeout=0)
           time.sleep(0.5)
       body= page.content()
        browser.close()
    #body是一个字符串，应当返回一个resposne对象
       resp = HtmlResponse(
           url=page.url,
           body=body,
           request=request
       )
       return resp
~~~

报错信息：

~~~shell
playwright._impl._api_types.Error: It looks like you are using Playwright Sync API inside the asyncio loop.

~~~

我猜想可能是scrapy是用twist编写异步框架,将sync（同步API）换成异步API之后，可以运行但是没有爬到东西，有警告信息如下：

~~~python
    async def process_request(self, request: scrapy.Request, spider):
        async with async_playwright() as p:
           browser = await p.chromium.launch(headless=False)
           page = await browser.new_page()
           await page.goto(request.url)
           await page.wait_for_load_state('networkidle',timeout=0)
           for i in self.name_list:
               await page.get_by_role("tab", name=i).locator("div").nth(1).click()
               await page.wait_for_load_state('networkidle',timeout=0)
               time.sleep(0.5)
           body=await page.content()
           await browser.close()
#body是一个字符串，应当返回一个resposne对象
           resp = HtmlResponse(
               url=page.url,
               body=body,
               request=request
           )
           return resp
~~~



~~~shell
2023-03-10 08:57:12 [scrapy.core.engine] INFO: Spider closed (finished)
2023-03-10 08:57:12 [py.warnings] WARNING: sys:1: RuntimeWarning: coroutine 'playwirghtMiddleware.from_crawler' was never awaited

~~~

最终我决定放弃用scrapy，直接自己写，反而很快就弄好了

### 完整代码

以阿里云为例，下面代码会把爬取到的产品列表输出并得到一个excel文件

~~~python
from playwright.sync_api import Playwright, sync_playwright, expect
import time
from config import ALIYUNCONFIG
from lxml import etree
from openpyxl import Workbook
class AliYunSPider:
    def __init__(self,url):
        self.url = url
        self.response=None
        self.getInfo()
        self.saveExcel()

    def getInfo(self):
        with sync_playwright() as playwright:
            browser = playwright.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()
            print("开始爬取阿里云的云产品列表")
            page.goto(self.url)
            page.wait_for_load_state('networkidle')
            for i in ALIYUNCONFIG.NAME_LIST:
                page.get_by_role("tab", name=i).locator("div").nth(1).click()
                page.wait_for_load_state('networkidle', timeout=0)
                time.sleep(0.5)
            body = page.content()
            page.close()
            context.close()
            browser.close()
            self.response=body
            print("阿里云云产品列表爬取完成")


    def saveExcel(self):
        html = etree.HTML(self.response)
        item_list = html.xpath('//div[contains(@class,"ace-tabs-content")]/div[@role="tabpanel"]')
        i = 0
        wb = Workbook()  # 新建工作簿
        ws = wb.active  # 获取工作表
        ws.append(['产品大类', '服务', '备注'])  # 追加一行数据
        for item in item_list:
            tmp = []
            pro_list = item.xpath('./div[@class="item-box"]//div[@class="citem-box"]')
            for pro in pro_list:
                name = pro.xpath('./div[@class="citem-box-grid"]//div[@class="sitem-title"]/span/text()')
                # tmp_url = pro.xpath('./div[@class="citem-box-grid"]//a/@href')
                # print(tmp_url)
                tmp.extend(name)
            # coding=utf-8
            ws.append([ALIYUNCONFIG.NAME_LIST[i]])  # 追加一行数据
            for k in tmp:
                ws.append(['', k])
            i += 1
        wb.save(r'阿里云云产品列表.xlsx')  # 保存到指定路径，保存的文件必须不能处于打开状态，因为文件打开后文件只读
~~~



### Requirements.txt生成

如果是一个项目一个的虚拟环境，可以使用

```python
pip freeze > requirements.txt
```

这个方法会将你整个Python环境的包全把生成出来，如果不是使用虚拟环境，生成的文件里面有很多其他包

**pipreqs(更方便)** 

使用 pipreqs 可以自动检索到当前项目下的所有组件及其版本，并生成 requirements.txt 文件，极大方便了项目迁移和部署的包管理。相比直接用pip freeze 命令，能直接隔离其它项目的包生成。

使用步骤：
1、先安装pipreqs库

`pip install pipreqs`
2、在当前目录使用生成`pipreqs . --encoding=utf8  `

~~~shell
--encoding=utf8 ：为使用utf8编码

--force ：强制执行，当 生成目录下的requirements.txt存在时覆盖 

. /: 在哪个文件生成requirements.txt 文件
~~~

