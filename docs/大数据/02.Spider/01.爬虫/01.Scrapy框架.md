---
title: Scrapy框架
date: 2023-02-02 00:00:00
tags: 
  - scrapy
sidebar: auto
categories: 
  - Spider
  - 爬虫
permalink: /pages/122686/
---

Scrapy 框架，是python爬虫中封装好的 一个明星框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式等等。

## 一 框架介绍

1、Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。

2、Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。

3、Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，

4、Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，**并将需要跟进的URL提交给引擎（使用Request）**，再次进入Scheduler(调度器)，

5、Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.

6、Downloader Middlewares（下载中间件）： 下载器中间件是一个与Scrapy的请求/响应处理挂钩的框架。它是一个轻量级的、低层次的系统，**用于全面改变Scrapy的request和response**。例如需要用playwrigth模拟访问时就可以自定义一个中间件，使得requests都用浏览器模拟访问的方式。

7、Spider Middlewares（Spider中间件）：spider中间件，在这里你可以插入自定义功能来处理发送到spider的response，以及处理从spider产生的request和response。

其架构图如下：

![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/1324415-20180514145258553-377347092.png)

## 二 一个实例

首先配置环境，可以直接用pip install scrapy安装。

这里IDE使用pycharm，选择解释器为刚才安装了scrapy的环境。下面开始进行demo项目。

常用的控制台命令（方括号为可选参数）：

- scrapy startproject <project_name> [project_dir]  新建一个项目
- scrapy genspider [-t template] <name> <domain> 生成一个spider.py文件
- scrapy crawl <spider> 运行某个爬虫（注意name是和类中的名字一样才行）

首先，`scrapy startproject demoProject`,会在当前目录下创建一个demoProject。

![image-20230226095752899](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230226095752899.png)

其中各个模块作用如下：

- spiders就是我们书写爬虫的地方，但是无需写请求过程，保存数据等，只需关注解析页面和爬取的地址等等。其中每个爬虫都是一个类，继承自scrapy.Spider
- items中可以定义自己需要的数据类型，每一个item也是一个类，继承自scrapy.Item
- middlewares,中间件，在我们需要对request和response进行一些操作时使用到。自动创建的项目中也会自动生成`class ScrapyQianSpiderMiddleware`和`class ScrapyQianDownloaderMiddleware`。

  ```markdown
  1.spider的yeild将request发送给engine
  2.engine对request不做任何处理发送给scheduler
  3.scheduler，生成request交给engine
  4.engine拿到request，通过middleware发送给downloader
  5.downloader在\获取到response之后，又经过middleware发送给engine
  6.engine获取到response之后，返回给spider，spider的parse()方法对获取到的response进行处理，解析出items或者requests
  7.将解析出来的items或者requests发送给engine
  8.engine获取到items或者requests，将items发送给ItemPipeline，将requests发送给scheduler（注意：只有调度器中不存在request时，程序才停止，及时请求失败scrapy也会重新进行请求）
  ```

- pipelines：当spider返回一个item之后，就会发送到pipeline，我们可以在pipeline中定义一些方法来处理item，常见的用途有去重，保存item等等。
- setting：配置文件，比如定义的请求头，开关上面的组件（注意自定义一个组件后，，要在setting中写上才能生效）等等。

接下来，在spider里新建一个爬虫，这里使用`scrapy genspider  demoSpider www.baidu.com`,**注意应当先cd 到spider文件夹下**，会生成demoSpider.py这个文件，下面是自动生成的代码

```python
import scrapy

class DemospiderSpider(scrapy.Spider):
    name = "demoSpider"
    allowed_domains = ["www.baidu.com"]
    start_urls = ["http://www.baidu.com/"]

    def parse(self, response):
        pass
```

其中一些参数含义

- name是该爬虫的名字，也是scrapy crawl中调用所需名字
- allowed_domains是运行访问域名，**会自动过滤掉不属于该域名的url**，直接删掉就不会过滤了
- start_urls:起始爬取的url



## 参考文档

- https://docs.scrapy.org/en/latest/

  