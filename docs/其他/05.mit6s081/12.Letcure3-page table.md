---
title: L3 page table
date: 2023-12-20
tags: 
  - 操作系统
categories: 
  - 其他
  - mit6s081
---

## Paging Hardware

xv6运行于Sv39 RISC-V，即在64位地址中只有最下面的39位被使用作为虚拟地址，其中底12位是页内偏移，高27位是页表索引，**即4096字节($2^{12}$)作为一个page**，一个进程的虚拟内存可以有 $2^{27}$个page，对应到页表中就是$2^{27}$个**page table entry (PTE)**。每个PTE有一个44位的physical page number (PPN)用来映射到物理地址上和10位flag，总共需要54位，也就是一个PTE需要8字节存储。即每个物理地址的高44位是页表中存储的PPN，低12位是页内偏移，一个物理地址总共由56位构成。

>页表使操作系统能够以 4096 ( $2^{12}$ ) 字节的对齐块的粒度控制虚拟地址到物理地址的转换，这样的块称为页（page）
>
>页表让每个进程都拥有自己独立的内存地址，从而实现内存隔离

![image-20231220150055726](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20231220150055726.png)

在实际中，页表并不是作为一个包含了$2^{27}$个PTE的大列表存储在物理内存中的，而是采用了三级树状的形式进行存储，这样可以让页表分散存储。

> 每个页表就是一页

页表以三级的树型结构存储在物理内存中。**该树的根是一个4096字节的页表页(第一级页表)**，其中包含512个PTE，每个PTE中包含该树下一级页表页的物理地址。这些页中的每一个PTE都包含该树最后一级的512个PTE（每个PTE占8个字节），这是第二级页表。第三级列表由512*512个页构成，因为分页硬件使用27位中的前9位在根页表页面中选择PTE，中间9位在树的下一级页表页面中选择PTE，最后9位选择最终的PTE。**第一级根页表的物理页地址存储在`satp`寄存器中，每个CPU拥有自己独立的`satp`**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/p2.png" alt="img" style="zoom: 80%;" />

>如果转换地址所需的三个PTE中的任何一个不存在，页式硬件就会引发页面故障异常（page-fault exception），并让内核来处理该异常

每个PTE包含标志位(PTE Flag)，这些标志位告诉分页硬件允许如何使用关联的虚拟地址。比如`PTE_V`表明这个PTE是否存在，`PTE_R`、`PTE_W`、`PTE_X`控制这个页是否允许被读取、写入和执行，`PTE_U`控制user mode是否有权访问这个页，如果`PTE_U`=0，则只有supervisor mode有权访问这个页。

## 内核地址空间

**每个进程有一个页表，用于描述进程的用户地址空间，还有一个内核地址空间（所有进程共享这一个描述内核地址空间的页表）**。为了让内核使用物理内存和硬件资源，内核需要按照一定的规则排布内核地址空间，以能够确定哪个虚拟地址对应自己需要的硬件资源地址。用户地址空间不需要也不能够知道这个规则，因为用户空间不允许直接访问这些硬件资源。

QEMU会模拟一个从0x80000000开始的RAM，一直到0x86400000。QEMU会将设备接口以控制寄存器的形式暴露给内核，这些控制寄存器在0x80000000以下。kernel对这些设备接口控制寄存器的访问是直接和这些设备而不是RAM进行交互的。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20210123171523476.png" alt="image-20210123171523476" style="zoom: 80%;" />

左边和右边分别是kernel virtual address和physical address的映射关系（直接映射）。

> 内核使用“直接映射”获取内存和内存映射设备寄存器；也就是说，将资源映射到等于物理地址的虚拟地址。例如，内核本身在虚拟地址空间和物理内存中都位于`KERNBASE=0x80000000`。直接映射简化了读取或写入物理内存的内核代码。例如，当`fork`为子进程分配用户内存时，分配器返回该内存的物理地址；`fork`在将父进程的用户内存复制到子进程时直接将该地址用作虚拟地址。

有几个内核虚拟地址不是直接映射：

- 蹦床页面(trampoline page)。它映射在虚拟地址空间的顶部；用户页表具有相同的映射。第4章讨论了蹦床页面的作用，但我们在这里看到了一个有趣的页表用例；一个物理页面（持有蹦床代码）在内核的虚拟地址空间中映射了两次：一次在虚拟地址空间的顶部，一次直接映射。
- 内核栈页面(kernel stack page)。每个进程都有自己的内核栈(kstack)，它将映射到偏高一些的地址，每个kstack下面有一个没有被映射的guard page，guard page的作用是防止kstack溢出影响其他kstack。当进程运行在内核态时使用内核栈，运行在用户态时使用用户栈。**注意**：还有一个内核线程，这个线程只运行在内核态，不会使用其他进程的kstack，内核线程没有独立的地址空间。

## 创建一个地址空间

xv6中和页表相关的代码在`kernel/vm.c`中。最主要的结构体是`pagetable_t`，这是一个指向页表的指针。`kvm`开头的函数都是和kernel virtual address相关的，`uvm`开头的函数都是和user virtual address相关的，其他的函数可以用于这两者

几个比较重要的函数：

- `walk`：给定一个虚拟地址和一个页表，返回一个PTE指针
- `mappages`：给定一个页表、一个虚拟地址和物理地址，创建一个PTE以实现相应的映射
- `kvminit`用于创建kernel的页表，使用`kvmmap`来设置映射
- `kvminithart`将kernel的页表的物理地址写入CPU的寄存器`satp`中，然后CPU就可以用这个kernel页表来翻译地址了
- `procinit`(kernel/proc.c)为每一个进程分配(`kalloc`)kstack。`KSTACK`会为每个进程生成一个虚拟地址（同时也预留了guard pages)，`kvmmap`将这些虚拟地址对应的PTE映射到物理地址中，然后调用`kvminithart`来重新把kernel页表加载到`satp`中去。

每个RISC-V **CPU**会把PTE缓存到Translation Look-aside Buffer (TLB，转译后备缓冲器?)中，当xv6更改了页表时，必须通知CPU来取消掉当前的TLB，取消当前TLB的函数是`sfence.vma()`，在`kvminithart`中被调用

## 物理内存分配

**内核必须在运行时为页表、用户内存、内核栈和管道缓冲区分配和释放物理内存**。xv6使用内核末尾到`PHYSTOP`之间的物理内存进行运行时分配。

**它一次分配和释放整个4096字节的页面。它使用链表的数据结构将空闲页面记录下来。分配时需要从链表中删除页面；释放时需要将释放的页面添加到链表中**。

xv6对kernel space和PHYSTOP之间的物理空间在运行时进行分配，分配以页(4096 bytes)为单位。分配和释放是通过对空闲页链表进行追踪完成的，分配空间就是将一个页从链表中移除，释放空间就是将一页增加到链表中

kernel的物理空间的分配函数在`kernel/kalloc.c`中，每个页在链表中的元素是`struct run`，每个`run`存储在空闲页本身中。这个空闲页的链表`freelist`由**spin lock（自旋锁）**保护，包装在`struct kmem`（一个结构体）中 。

- `kinit()`：对分配函数进行初始化，将kernel结尾到PHYSTOP之间的所有空闲空间都添加到kmem链表中，这是通过调用`freerange(end, PHYSTOP)`实现的
- `freerange()`对这个范围内所有页都调用一次`kfree`来将这个范围内的页添加到`freelist`链表中

## 进程地址空间

每个进程都有一个单独的页表，当xv6在进程之间切换时，也会更改页表。

如图2.3所示，**一个进程的用户内存从虚拟地址零开始，可以增长到MAXVA** (kernel/riscv.h:348)，原则上允许一个进程内存寻址空间为256G。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/p5.png" alt="img" style="zoom:67%;" />

我们在这里看到了一些使用页表的很好的例子。 

首先，不同进程的页表将用户地址转换为物理内存的不同页面，这样每个进程都拥有私有内存。第二，每个进程看到的自己的内存空间都是以0地址起始的连续虚拟地址，而进程的物理内存可以是非连续的。第三，内核在用户地址空间的顶部映射一个带有蹦床（trampoline）代码的页面，这样在所有地址空间都可以看到一个单独的物理内存页面。

当进程向xv6索要更多用户内存时，xv6先用`kalloc`来分配物理页，然后向这个进程的页表增加指向这个新的物理页的PTE，同时设置这些PTE的flag

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20210123220911443.png" alt="image-20210123220911443" style="zoom: 67%;" />

图3.4是一个进程在刚刚被`exec`调用时的用户空间下的内存地址，stack只有一页，包含了`exec`调用的命令的参数从而使`main(argc, argv)`可以被执行。stack下方是一个guard page来检测stack溢出，一旦溢出将会产生一个page fault exception。

## code sbrk

**`sbrk`是一个可以让进程增加或者缩小用户空间内存的system call**。`sbrk`调用了`growproc`(kernel/proc.c)来改变`p->sz`从而改变**heap**中的program break，`growproc`调用了`uvmalloc`和`uvmdealloc`，前者调用了`kalloc`来分配物理内存并且通过`mappages`向用户页表添加PTE，后者调用了`kfree`来释放物理内存

## Code: exec

`exec`是一个system call，为以ELF格式定义的文件系统中的可执行文件创建用户空间。

>ELF文件格式：在计算机科学中，是一种用于二进制文件、可执行文件、目标代码、共享库和核心转储格式文件。ELF是UNIX系统实验室（USL）作为应用程序二进制接口（Application Binary Interface，ABI）而开发和发布的，也是Linux的主要可执行文件格式。ELF文件由4部分组成，分别是ELF头（ELF header）、程序头表（Program header table）、节（Section）和节头表（Section header table）。实际上，一个文件中不一定包含全部内容，而且它们的位置也未必如同所示这样安排，只有ELF头的位置是固定的，其余各部分的位置、大小等信息由ELF头中的各项值来决定。

`exec`先检查头文件中是否有ELF_MAGIC来判断这个文件是否是一个ELF格式定义的二进制文件，用`proc_pagetable`来为当前进程创建一个还没有映射的页表，然后用`uvmalloc`来为每个ELF segment分配物理空间并在页表中建立映射，然后用`loadseg`来把ELF segment加载到物理空间当中。注意`uvmalloc`分配的物理内存空间可以比文件本身要大。

接下来`exec`分配user stack，它仅仅分配一页给stack，通过`copyout`将传入参数的string放在stack的顶端，在ustack的下方分配一个guard page

如果`exec`检测到错误，将跳转到`bad`标签，释放新创建的`pagetable`并返回-1。`exec`必须确定新的执行能够成功才会释放进程旧的页表(`proc_freepagetable(oldpagetable, oldsz)`)，否则如果system call不成功，就无法向旧的页表返回-1

## Real world

xv6使用分页硬件进行内存保护和映射。大多数操作系统通过结合分页和页面故障异常使用分页，比xv6复杂得多

内核通过使用虚拟地址和物理地址之间的直接映射，以及假设在地址`0x8000000`处有物理RAM (内核期望加载的位置) ，Xv6得到了简化。这在QEMU中很有效，但在实际硬件上却是个坏主意；实际硬件将RAM和设备置于不可预测的物理地址，因此（例如）在xv6期望能够存储内核的`0x8000000`地址处可能没有RAM。

**更严肃的内核设计利用页表将任意硬件物理内存布局转换为可预测的内核虚拟地址布局。**

xv6内核缺少一个类似`malloc`可以为小对象提供内存的分配器，这使得内核无法使用需要动态分配的复杂数据结构。

内存分配是一个长期的热门话题，基本问题是有效使用有限的内存并为将来的未知请求做好准备。今天，人们更关心速度而不是空间效率。此外，一个更复杂的内核可能会分配许多不同大小的小块，而不是（如xv6中）只有4096字节的块；一个真正的内核分配器需要处理小分配和大分配。

> 第一次看感觉这一章的内容很迷糊，太细了，看不太懂
