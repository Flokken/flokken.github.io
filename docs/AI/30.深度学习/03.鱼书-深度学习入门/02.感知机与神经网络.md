---
title: 感知机与神经网络
date: 2023-03-17 00:00:00
categories: 
  - 深度学习
  - 鱼书-深度学习入门
tags: 
  - null
permalink: /pages/11726c/
---

# 一 感知机是什么

感知机就是一个接受多个输入，输出一个信号的东西,下面是一个接受两个输入，输出一个y的感知机。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317174759041.png" style="zoom:67%">

其运行原理就是一个二值函数：

![image-20230320100033100](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320100033100.png)

也就是说,$x_1,x_2$是输入信号，$w_1,w_2$是权重参数，输入信号输入感知机后，计算其和，如果小于阈值$\theta$,那么输出0，否则输出1

如果将$\theta$换成$-b$,则有

![image-20230320100051582](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320100051582.png)

这种形式下，$b$称为偏置参数，$w_1,w_2$称为权重

## 用感知机模拟计算机

我们知道计算机其实就与非门构成的，显然上面的感知机可以实现与非门,与门，或门。

| 输入          | 输出 |
| ------------- | ---- |
| $x_1=0,x_2=0$ | 1    |
| $x_1=0,x_2=1$ | 1    |
| $x_1=1,x_2=0$ | 1    |
| $x_1=1,x_2=1$ | 0    |

**code**:

~~~python
#只有权重和偏置不一样
def NAND(x1,x2):#与非门
    x=np.array([x1,x2])
    w=np.array([-0.5,-0.5])
    b=0.7
    tmp=np.sum(w*x)+b
    if tmp <=0:
        return 0
    else:
        return 1
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b=-0.2
    tmp=np.sum(w*x)+b
    if tmp <=0:
        return 0
    else:
        return 1
def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b=-0.7
    tmp=np.sum(w*x)+b
    if tmp <=0:
        return 0
    else:
        return 1
~~~

但是单层感知机不能实现异或门。

**原因：**我们知道单层感知机，使用的那个函数，只能表示直线，**而一条直线是不能将异或门的四个输出分成两个空间的**

不过多层感知机可以实现：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317201317545.png" style="zoom:70%">

**code:**

~~~python
def XOR(x1,x2):
    s1= NAND(x1,x2)
    s2=OR(x1,x2)
    y=AND(s1,s2)
    return y
~~~

**如何模拟**？

我们知道，只需要实现与非门实际上就可以实现计算机，因为和二极管对应上了。

而且，重要的是，**叠加的多层感知机可以实现非线性的表示，模拟计算机进行计算处理的过程。**

# 二神经网络

我们知道。使用感知机理论上也可以模拟计算机**，但是如何设置合适的参数是一个非常麻烦的事，上面时人工设定的。**而神经网络可以自动的从程序中学习到合适的参数。

## 从感知机到神经网络

首先介绍神经网络的结构，如下图，**输入层，中间层（隐藏层），输出层。**

<img  src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317212440249.png" style="zoom:60%">

> 这个神经网络有三层神经元，其中只有两层有权重，输出层没有，因此一般称作两层神经网络

## **回顾感知机**

我们知道，感知机可以用下面的式子表示：

![image-20230320100108693](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320100108693.png)

为了让上面的式子更简洁，我们用h(x)来替换y，也就是用h来表示这种分情况的函数。

也就是:

![image-20230320100125510](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320100125510.png)

## 激活函数

对于上面的式子，h会将输入信号转换为输出信号，**这种函数称为激活函数**。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317215400268.png" style="zoom:70%">

**激活函数干了什么**？

首先我们令$a=b+w_1x_1+w_2x_2$,即信号加权的值为a，然后a被h()转换为了y，**y就是该神经元的输出值**

**激活函数种类**？

上面我们的感知机的激活函数h(x)都是以阈值为界限，一旦超过，就·切换输出，这种称为”阶跃函数“。

**如果将阶跃函数换为其他函数**，感知机就连接上了神经网络的世界。

### **阶跃函数图像**：

~~~python
import numpy as np
import matplotlib.pyplot as plt

def step_function(x):
    return np.array(x>0)

x= np.arange(-5.0,5.0,0.1)
y=step_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)#指定y轴范围
plt.show
~~~

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317222225098.png" style="zoom:67%">

### **Sigmoid函数图像**：

~~~shell
注意numpy数组的广播功能
>>> import numpy as np
>>> t = np.array([1,2,3])
>>> t+1
array([2, 3, 4])
>>> t/1
array([1., 2., 3.])
>>> 1/sigmoid(t)
array([1.36787944, 1.13533528, 1.04978707])
>>>
~~~

~~~python
import numpy as np
import matplotlib.pyplot as plt
def sigmoid(x):
    return 1/(1+np.exp(-x))#注意x既可以是标量如1,2,3，也可以是一个np数组，为一个np数组时，返回一个结果列表
#注意1/(1+np.exp(-x)) ，如果是x是np数组，这里是广播运算,也就是每个numpy数组元素分别与标量进行运算

x = np.arange(-5.0,5.0,0.1)
y= sigmoid(x)
plt.plot(x,y)
plt.ylim(-.01,1.1)
plt.show()
~~~

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230317222053560.png" style="zoom:67%">

### ReLu函数

这是最近时间多使用的激活函数。其函数如下：

![image-20230320100143600](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320100143600.png)

~~~python
def relu(x):
    return np.maximum(0,x)
x = np.arange(-5.0,5.0,0.1)
y= sigmoid(x)
plt.plot(x,y)
plt.ylim(-.01,1.1)
plt.show()
~~~

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319171221655.png" style="zoom:70%">

### 非线性函数

线性函数就是形如$ax+b$的函数，其图像是一条线。

激活函数有一个特点，必须是非线性函数，上面的sigmoid和阶跃函数都是非线性函数。

**why**?

如果使用线性函数，加深神经网络层数就没有意义了。  如果使用线性函数当作激活函数，**那么不管如何加深层数，总是存在与之对应的无隐藏层的神经网络**

例如：把$h(x)=cx$作为激活函数，把$y=h(h(h(x)))$对应三层神经网络计算，那么也就是$y=C✖C✖C✖X$的成分他运算，但是可以用一个$y=C^3X$来进行一次运算**（也就是没有隐藏层）**来表示。所以如果使用非线性函数，不能发挥多层神经网络优势。**因此激活函数必须使用非线性函数。**

# 三 实现神经网络

## 神经网络内积

> 基础知识
>
> 矩阵A=X*Y  B=Y*Z  ,那么AB=XZ
>
> 矩阵计算：前行乘后列，所以两个矩阵相乘必须保证A的列和B的行相等

假设有如图省略了偏置与激活函数的神经网络：

![image-20230319175211013](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319175211013.png)

神经网络的运算就是矩阵运算

~~~shell
>>> A=np.array([1,2])
>>> A.shape
(2,)
>>> B=np.array([[1,3,5],[2,4,6]])
>>> B.shape
(2, 3)
>>> Y=np.dot(A,B)
>>> Y
array([ 5, 11, 17])
~~~

## 三层神经网络实现

以下图为例，实现三层神经网络从输入到输出的处理**（前向处理）**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319175815909.png" style="zoom:70%">

> P63有详细的运算过程

这里笼统的说一下如何实现，我们知道，三层神经网络分别是输入层，隐藏层1，隐藏层2，（输出层没有参数，这里只计数有参数的）

1.输入层的输入参数X看层矩阵[x1,x2],偏置参数看成b，权重参数看成W(矩阵)

**XW+b就是他们到神经元中的和，但是这个和还要再经过一次激活函数的转换，才是这个神经元的输出**，然后这个神经元的输出相当于新得X输入下一层，然后同样的办法求得和，**注意，第二层后面是输出层**，**输出层的前一层的激活函数要根据输入问题的性质决定**,比如二分类可以用sigmoid函数，多分类可以用softmax函数

这里是书上例子的代码：

~~~python
def init_network():
    #三层网络参数
    network = {}
    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
    network['b1'] = np.array([0.1,0.2,0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1,0.2])
    
    return network
def sigmoid(x):
    return 1/(1+np.exp(-x))

def identity_function(x):
    #这里先假设是恒等函数
    return x

def forward(network,x):
    #前向传播，将输入层转换为输出层
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']
    
    a1 = np.dot(x,W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2,W3) + b3
    y=identity_function(a3)
    #这里是输出层前一层所用的激活函数
    
    return y

network = init_network()
x = np.array([0.1,0.5])
y = forward(network,x)
print(y)#[0.31234736 0.6863161 ]
~~~

## Softmax函数实现

softmax的函数表达式如：

$y_k=\frac{exp(a_k)}{\displaystyle \sum_{i=1}^n exp(a_i)}$

> exp(x)就是$e^x$
>
> 分子是第k个的指数函数值，分母是指数函数值的和

因此softmax函数可以输出多个值

并且，输出层的各个神经元都受到所有输入信号影响

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319204158851.png"  style="zoom:70%">

**但是，这里有一个重要的问题，如果输入的a是1000,10000这样很大的数，他的指数就是极大的数，此时进行运算会得出nan**

利用指数函数性质 对公式进行转换：

$y_k=\frac{exp(a_k)}{\displaystyle \sum_{i=1}^n exp(a_i)}=\frac{exp(a_k+C')}{\displaystyle \sum_{i=1}^n exp(a_i+C')}$

也就是，如果输入信号都加上一个常数，对结果没有影响，因此，规定，**softmax每个参数减去输入信号中的最大值**，这样还有一个好处，

- **那就是他输出信号的和为1，因此把softmax的输出解释为概率**
- **输出信号的取值在0,1之间的实数**

~~~python
def softmax(a):
    c=np.max(a)
    exp_a = np.exp(a-c)#广播，每个数都会减去c
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
~~~

# 四 手写数字识别

如果假设神经网络学习已经全部结束，用学习到的参数来推理处理，把这个推理处理（比如分类）称为**前向传播**。

> 也就是说，前向传播是已经学习好了参数之后，才能进行，这里假定已经训练好参数了

这里前面需要自己弄好数据集。**并且存储为pickle对象**，下面的代码可以显示一张图片：

> pickle是python的一个模块，他可以将Python的对象保存下来，并且读取一个pickle后可以立即再还原，非常方便

~~~python
import sys,os
sys.path.append(os.pardir)
from dataset.mnist import load_mnist
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()
    
(x_train,t_train),(x_test,t_test) = load_mnist(flatten=True,normalize=False)#保存时将其拉成了一维numpy数组，故读入时要让其转换回来1,28,28
img  = x_train[0]
label = t_train[0]
print(label)

print(label)
print(img.shape)
img_show(img)

#输出
#5
#5
#(784,)
~~~

## 推理处理（前向传播）

分析数据集可以知道，输入层应该有784个参数(1X28X28),输出层10个神经元（0-9），然后设置隐藏层1神经元为50，第二层为100（可以使任意值）

~~~python
def get_data():
    (x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,flatten=True,one_hot_label=False)
    return x_test,t_test

def init_network():
    with open("sample_weight.pkl",'rb') as f:
        network = pickle.load(f)
    return network

def predict(network,x):
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']
    a1 = np.dot(x,W1) +b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2)
    z2 = sigmoid(a2)
    a3 = np.dot(z2,W3)
    y= softmax(a3)
    
    return y
  
x ,t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network,x[i])
    p = np.argmax(y)#获取概率最高的元素索引
    if p == t[i]:
        accuracy_cnt += 1
print("Accuracy:" + str(float(accuracy_cnt)/len(x)))
#Accuracy:0.9328
~~~

补充load_mnist三个参数含义：

- normalize: 正规化（正则化）  ，就是把数据限定到某个范围
- one-hot-label（将标签保存为one-hot数组）: one-hot编码即[0,0,0,0,1,0,0]只有正确解为1，其他解都为0的数组

## 批处理

假设只读入一张图片，输入信号形状的变化：

![image-20230319220723822](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319220723822.png)

### 如果一次输入100张图片，如何处理？

把x的形状改为100X784,将这100张打包的图作为输入

![image-20230319220913999](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230319220913999.png)

可以发现，这样做可以让100次的结果一次性输出，**这种打包的输入数据称作批**

对于计算机而言，一般处理计算的库都会对这种大型的数据进行优化，**运算效率比一张张读入好，因此一般读入数据都是分批读入而不是一个个读入**

**批处理实现**

~~~python
x,t=get_data()
network = init_network()
batch_size=100
cnt = 0
for i in range(0,len(x),batch_size):
    x_batch = x[i:i+batch_szie]
    y_batch = predict(network,x_batch)
    p = np.argmax(y_batch,axis=1)#沿着第一维找到最大的元素索引，也就是每一行找最大，返回每行最大值索引的数组
    cnt+=np.sum(p == t[i:i+batch_size])#[True,false,...]然后sum加起来
print("Accuracy:" + str(float(cnt)/len(x)))
~~~

> 矩阵的第0维是列， 第一维是行

~~~shell
>>> x = np.array([[0.1,0.8,0.1],[0.3,0.1,0.6],[0.2,0.5,0.3],[0.8,0.1,0.1]])
>>> np.argmax(x,axis=1)
array([1, 2, 1, 0], dtype=int64)
~~~

