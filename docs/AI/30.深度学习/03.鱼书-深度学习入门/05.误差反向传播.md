---
title: 误差反向传播
date: 2023-03-28 00:00:00
categories: 
  - 深度学习
  - 鱼书-深度学习入门
tags: 
  - null
permalink: /pages/81deb7/
---

# 误差反向传播

如何计算梯度？根据数值微分，我们可以求得梯度，但是计算很麻烦，还有一种方法叫反向传播，计算梯度非常快捷。

## 计算图

计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。

### 苹果问题

问题**1**：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329194233781.png" style="zoom:70%">

**在圆圈里的表示某种运算，外面的是数字**

问题**2**：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100日元，橘子每个150日元。消费税是10%，请计算支付金额。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329194341505.png" style="zoom:70%">

这里的区别就是多了一个输入，并且多了一个`+`的圆圈

上面的问题中，从左向右传播称为正向传播，**如果是从右向左计算，就是反向传播**。

### 链式法则

介绍怎么从左向右计算的时候啊，先介绍链式法则（就是高数里那个链式法则）。

我们需要先从复合函数说起。复合函数是由多个函数成的函数。举例如下：

$z=(x+y)^2$

$z=t^2$

$t=x+y$

链式法则是关于复合函数的导数的性质，定义如下:

**如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示**。

以上面的例子为例，如果我需要求$\frac {\partial z} {\partial x}$ ,那么有：

$\frac {\partial z} {\partial x} = \frac {\partial z} {\partial t}\frac {\partial t} {\partial x}$

**接下来使用链式法则求导**

$\frac {\partial z} {\partial x} = 2t$

$\frac {\partial t} {\partial x} = 1$

$\frac {\partial z} {\partial x} = \frac {\partial z} {\partial t}\frac {\partial t} {\partial x}=2t·1=2(x+y)$

> 数值微分可以求导，这种直接公式推也是求导，叫解析性求导，反向传播就是基于解析性求导的，算起来很快

### 用计算图表示链式法则

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329200459299.png" style="zoom:70%">

其中需要注意：

1. **2表示平方运算，导数是$2t$
2. 最右边传过来是1，因为$\frac {\partial z} {\partial z} =1$

## 反向传播

反向传播，就是用链式求导法则，倒着来推输入，**其中求导用的是解析性求导的方式**

### 加法层

以$z=x+y$​为例：**观察它的反向传播**，**首先解析性求导**

$\frac {\partial z} {\partial x} = 1$

$\frac {\partial z} {\partial y} = 1$

我们发现导数都是1，也就是从上游传过来的导数都乘以1，相当于什么也没干

注意加上上游传来的值为$\frac {\partial L} {\partial z}$

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329201235572.png" style="zoom:70%">

如果有具体数值，类似于下面

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329201326897.png" style="zoom:70%">

### 乘法层

同样的，我们举个例子，这里考虑$z=xy$,则他的导数是

$\frac {\partial z} {\partial x} = y$

$\frac {\partial z} {\partial y} = x$

那么他的计算图如下：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329201644666.png" style="zoom:70%">



**注意**：加法的反向传播只是将上游的值传给下游，

并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输

入信号值。**因此，实现乘法节点的反向传播时，要保存正向传播的输入信号（也把乘法节点叫翻转）**

换成实际的数字就是

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329201746654.png" style="zoom:70%">

### 再看苹果的例子

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329201859746.png" style="zoom:70%">

如前所述，乘法节点的反向传播会将输入信号翻转后传给下游。苹果的价格的导数是2*.*2，苹果的个数的导数是110，消费税的导数是200。

**对于买几种水果的例子则是**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329202110235.png" style="zoom:70%">

## 简单层的实现

### 加法层实现

~~~python
#加法层
class AddLayer:
    def __init__(self):
        pass
    def forward(self,x,y):
        out = x + y
        return out
    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
~~~

### 乘法层实现

~~~python
#乘法层
class MulLayer:
    def __init__(self):
        self.x = None
        self.y =None
    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x * y
        
        return out
    def backward(self,dout):
        dx = dout * self.y#就像翻转一样
        dy = dout * self.x
        
        return dx,dy
~~~

将其表示为代码如下，注意，backforward调用层的顺序时，是forward相反的顺序

~~~python
apple = 100
apple_num=2
orange = 150
orange_num = 3
tax = 1.1

#Layer
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

#foward
apple_price = mul_apple_layer.forward(apple, apple_num) #(1)
orange_price = mul_orange_layer.forward(orange, orange_num) #(2)
all_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)
price = mul_tax_layer.forward(all_price, tax) #(4)

#backward
# backward
dprice = 1 #这个是上游来的值，
dall_price, dtax = mul_tax_layer.backward(dprice) #(4)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)
dorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)
dapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)

print(price) # 715
print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650
~~~

## 激活函数层的实现

现在，考虑神经网络中的具体实现，同样的，把相关的层设置为一个类

### ReLU层

激活函数ReLU（Rectified Linear Unit）由下式表示

![image-20230329205240832](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329205240832.png)

其导数为

![image-20230329205258524](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329205258524.png)

**分析一下**：

在正向传播的时候，如果输入的x>0,反向传播就会原封不动的将上游的值传给下游（**导数为1**），如果正向传播时的*x*小于等于0，则反向

传播中传给下游的信号将停在此处,如下图所示

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329205745513.png" style="zoom:70%">

实现：注意在神经网络实现中，**一般假定接受的参数是numpy数组**

~~~python
class ReLU:
    def __init__(self):
        self.mask = None
        
    def forward(self,x):
        self.mask = (x<=0)
        out = x.copy()
        out[self.mask] = 0
        
    def backward(self,dout):
        dout[self.mask] = 0
        dx = dout
        
        return dx
~~~

Relu类有实例变量mask。这个变量mask是由True/False构成的NumPy数组，它会把正向传播时的输入x的元素中小于等于0的地方保存为True，其他地方（大于0的元素）保存为False。

**例如**

~~~python
>>> x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )
>>> print(x)
[[ 1. -0.5]
 [-2. 3. ]]
>>> mask = (x <= 0)
>>> print(mask)
[[False True]
 [ True False]]
~~~

>ReLU层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为 ON；
>
>没有电流通过的话，就将开关设为 OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。

### sigmoid层

首先再回顾sigmoid函数式

$y=\frac {1} {1+exp(-x)}$

用计算图表示他的**正向与反向传播**过程,假设上游传来的值是$\frac {\partial L} {\partial y}$

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329211410817.png" style="zoom:70%">

**注意**

1. `/`节点表示$y=\frac {1}{x}$:

$\frac {\partial y} {\partial x} =- \frac {1} {x^2}=-y^2$

也就意味着，反向传播时，会将上游的值乘以 $-y^2$ （正向传播的输出的平方乘以*−*1后的值）后，再传给下游

2. `exp`节点表示$y=exp(x)$,其导数：$\frac {\partial y} {\partial x} =exp(x)$     反向传播时，上游的值乘以正向传播时的输出（这个例子中是exp(*−**x*)）后，

   再传给下游。

整理计算过程：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329212451430.png" style="zoom:70%">

根据上面的流程，**我们可以只关注最开始和最末尾，而不去在意中间细节，这样可以简约的表示该层**，也就是

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329212335431.png" style="zoom:70%">

**代码实现**

~~~python
class Sigmoid:
    def __init__(self):
        self.out = None
        
    def forward(self,x):
        out  = 1/(1+np.exp(-x))
        self.out = out
        
        return out
    def backward(self,dout):
        dx = dout * (1.0-self.out) *self.out
        
        return dx
~~~

## 其他层

### Affine层

神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy中是np.dot()）

![image-20230329214648905](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214648905.png)

这里用计算图的方式表示该过程（**也就是正向传播**）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329213035623.png" style="zoom:70%">

>（注意变量是矩阵，各个变量的上方标记了该变量的形状
>
>(2,)表示一个2行一列的列向量，可以看成(2,1)

其中，将乘积运算用“dot”节点表示，再加上B，就可以求矩阵的乘积与偏置的和

**如何考虑他的反向传播呢？**

其实以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同，其导数式

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214247790.png" style="zoom:70%">

$W^T$表示转置

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214424523.png" style="zoom:70%">

其反向传播计算图

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214527587.png" style="zoom:70%">

**补充，一定要注意矩阵的形状变化**

$X与\frac {\partial L} {\partial X}$,形状相同，$W与\frac {\partial L} {\partial W}$,形状相同。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214715815.png" style="zoom:70%">

### 批处理的Affine层

也就是加入了数量，多了一个维度

其计算图

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329214940214.png" style="zoom:70%">

加**上偏置时，需要特别注意**。正向传播时，偏置被加到**X**·**W**的各个数据上。比如，*N* = 2（数据为2个）时，偏置会被分别加到这2个数据（各自的计算结果）上；反向传播时，各个数据的反向传播的值需要汇总为偏置的元素


~~~python
>>> dY = np.array([[1, 2, 3,], [4, 5, 6]])
>>> dY
array([[1, 2, 3],
 [4, 5, 6]])
>>>
>>> dB = np.sum(dY, axis=0)
>>> dB
array([5, 7, 9])#反向传播的值
~~~

**Affine层实现：**

~~~python
class Affine:
 def __init__(self, W, b):
     self.W = W
     self.b = b
     self.x = None
     self.dW = None
     self.db = None
 def forward(self, x):
     self.x = x
     out = np.dot(x, self.W) + self.b
     return out
 def backward(self, dout):
     dx = np.dot(dout, self.W.T)
     self.dW = np.dot(self.x.T, dout)
     self.db = np.sum(dout, axis=0)
     return dx
~~~

### Softmax-with-Loss 层

softmax函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax层的输出如下

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329215807750.png" style="zoom:70%">

>输入图像通过Affi ne层和ReLU层进行转换，10个输入通过Softmax层进行正 规化。在这个例子中，“0”的得分是5.3，这个值经过Softmax层转换为0.008 （0.8%）；“2”的得分是10.1，被转换为0.991（99.1%）
>
>神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用 Softmax层。比如，用图 5-28的网络进行推理时，会将最后一个 Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28中 Softmax层前面的 Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax层。不过，神经网络的学习阶段则需要 Softmax层。

Softmax-with-Loss 层实际计算图比较复杂，这里略去那些，只关心输入输出，则有

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230329220051568.png" style="zoom:70%">

Softmax层的反向传播得到了 （y1 − t1, y2 − t2, y3 − t3）这样“漂亮”的结果。由于（y1, y2, y3）是Softmax层的 输出，（t1, t2, t3）是监督数据，所以（y1 − t1, y2 − t2, y3 − t3）是Softmax层的输 出和教师标签的差分。神**经网络的反向传播会把这个差分表示的误差传递给 前面的层，这是神经网络学习中的重要性质。**

**神经网络学习的目的就是通过调整权重参数**，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高 效地传递给前面的层。刚刚的（y1 − t1, y2 − t2, y3 − t3）正是Softmax层的输出 与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。



这里考虑一个具体的例子，比如思考教师标签是（0, 1, 0），Softmax层 的输出是(0.3, 0.2, 0.5)的情形。因为正确解标签处的概率是0.2（20%），这个 时候的神经网络未能进行正确的识别。此时，Softmax层的反向传播传递的 是(0.3, −0.8, 0.5)这样一个大的误差。因为这个大的误差会向前面的层传播， 所以Softmax层前面的层会从这个大的误差中学习到“大”的内容。

再举一个例子，比如思考教师标签是(0, 1, 0)，Softmax层的输出是(0.01, 0.99, 0)的情形（这个神经网络识别得相当准确）。此时Softmax层的反向传播 传递的是(0.01, −0.01, 0)这样一个小的误差。这个小的误差也会向前面的层 传播，因为误差很小，所以Softmax层前面的层学到的内容也很“小”。

**实现**

~~~python
class SoftmaxWithLoss:
     def __init__(self):
    	 self.loss = None # 损失
     	 self.y = None # softmax的输出
    	 self.t = None # 监督数据（one-hot vector）
 	def forward(self, x, t):
   	 	 self.t = t
    	 self.y = softmax(x)
    	 self.loss = cross_entropy_error(self.y, self.t)
 		 return self.loss
     def backward(self, dout=1):
    	 batch_size = self.t.shape[0]
    	 dx = (self.y - self.t) / batch_size#消除数量的影响
     	 return dx
~~~
