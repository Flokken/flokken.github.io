---
title: 损失函数
date: 2023-03-26 00:00:00
categories: 
  - 深度学习
  - 鱼书-深度学习入门
tags: 
  - null
permalink: /pages/6c444b/
---

# 是什么？

神经网络的学习，就是指可以从数据中学习到权重参数，那么如何衡量学习的好坏呢？答案是损失函数。也就是说，损失函数是衡量神经网络学习好坏的一个指标。这个函数可以有很多种。一般使用均方误差和交叉熵误差。

## 均方误差

均方误差函数公式一般如下所示：

$E= \frac 1 2 \sum\limits_k{(y_k-t_k)^2}$

> 其中，$y_k表示神经网络输出，t_k表示监督数据，k表示数据维数(特征数)$

### 举例

假设手写数字识别中$y_k$,$t_k$由如下10个元素构成

~~~shell
>>> y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
>>> t=[0,0,1,0,0,0,0,0,0,0]
~~~

这里我们假设“2”为正确解，所以2的标签是1，其他的都是0，y是神经网络输出，t是正确解

当然，这里的t用了one-hot编码，也就是只有正确解对应标签为1，其他都是0

~~~python
import numpy as np
def mean(y,t):
    return 0.5*np.sum((y-t)**2)
#输出中2的概率最大时，对应的损失函数值
y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]
t= [0,0,1,0,0,0,0,0,0,0]
mean(np.array(y),np.array(t))
#0.09750000000000003
#输出中7的概率最大的损失函数值
y=[0.1,0.05,0.0,0.0,0.05,0.1,0.6,0.1,0.0,0.0]
mean(np.array(y),np.array(t))
#0.6975
~~~

因此，可以看出，均方差误差值越小，表示该数据与监督情况更吻合

## 交叉熵误差

交叉熵误差公式

$E = - \sum\limits_kt_klogy_k$

> 这里log以e为底数的自然对数$log_e$,$y_k$是神经网络输出，$t_k$是正确解得标签。

实际上，显然交叉熵误差只计算了**正确解对应的神经网络输出**的log值。以上面的例子为例，如果正确解标签是2，第一个对应神经网络输出为0.6，那么交叉熵误差就是$-log0.6=0.51$ ，同样的，第二个的对应的输出是0,那么交叉熵误差就是$-log0=\infty$。所以交叉熵误差也能衡量神经网络学习好坏。

~~~python
import numpy as np
def crosss_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))
~~~

注意，**这里加一个delta，是为了防止np.log(0)=-inf**

## mini-Batch

机器学习使用训练数据进行学习，而进行学习的目的是尽可能的将损失函数值最小化，以得到最优参数

因此，计算损失函数时，应当计算所有训练数据作为对象。比如，有100个训练数据（100个样本），就应当把这100个的损失函数的总和作为学习指标。当然，一般为了让指标与样本量无关，一般还会除以样本数N.

### 不止一个样本后的交叉熵误差

$E = - \frac 1 N \sum\limits_n\sum\limits_kt_{nk}logy_{nk}$

假设这里有N个样本，k可以理解为该样本的特征数，$t_{nk}$表示第n个元素的第k个元素值(当然y仍然是神经网络的输出，t是监督数据(就是已知正确的东西)，该式把求单个损失函数的式子扩大到了N份数据，不过最后需要除以N进行正规化。**这样这个指标可以与训练数据数量无关**

### 为什么一次只取一部分

以minist为例，他的训练集数据有60000个，如果以全部对象为数据求损失函数值，**计算起来比较花时间**。因此可以从全部数据中，选出一部分，作为全部数据的近似。神经网络一般就是这么学习的，每次从全部数据中选出一部分（称为mini-batch,小批量），然后对每个mini-batch进行学习。比如从60000个数据中随机选100笔，再用这100笔进行学习，称为mini-batch学习。

## 使用损失函数的理由

###  为什么不用精度当指标？

因为当利用识别精度当指标，参数的导数在大多数地方都是零，如果导数为零，就不能继续更新下去了。

> 导数为正，就会向减小导数的方向更新参数，为负，增大的方向更新，为零停止更新。

**举例**：

假设有100笔识别数据，此时识别精度为32%,即使稍微改变权重参数的值，识别精度也基本不会有什么改变，计算有改变，**也是离散的改变，比如变成33%,34%,而不是32.023%这种连续的改变**，**此时他的导数大多数地方都是0**

**同理，阶跃函数也不能当激活函数**

> 因为他绝大多数地方导数都是0，，并且不能体现参数的微小变化（因为变了还是零）

**而sigmoid函数任何地方，导数都不是零，这对神经网络的学习非常重要**



