---
title: C-NTM
date: 2023-07-20
tags: 
  - 
categories: 
  - 深度学习
  - 论文阅读
---

> [C-NTM](https://arxiv.org/pdf/2110.12764.pdf     )

## C-NTM

[Abstract]

对抗主题模型（ATM）可以通过将文档与另一个不相似的样本区分来成功捕捉文档的语义模式。然而，利用这种区分性-生成性架构存在两个重要缺点：（1）该架构不涉及相似的文档，这些文档具有相同的显著单词分布；（2）它限制了集成外部信息的能力，例如文档的情感，这已经被证明有助于神经主题模型的训练。为了解决这些问题，作者从数学分析的角度重新审视了对抗主题架构，提出了一种新的方法来将区分性目标重新制定为优化问题，并设计了一种新的采样方法，以便于集成外部变量。

[Method]

提出了一个损失函数

### 构造正样本和负样本

这个论文用的词袋模型Bow来表示文档，对于一篇文档，计算所有单词的tf-idf；**认为其中 tf-idf 值最大的那些单词就是 salient words（重要显著单词）；而 tf-idf 值最小的那些就是最不重要的单词**（a, an, the 之类的）

取topK个tf-idf最大/最小的2K个单词，然后根据tf-idf进行采样即可得到G该文档的正负样本

## UCTopic

- 论文题目：UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining
- 论文链接：https://arxiv.org/abs/2202.13469
- 论文发表：ACL 2022

**「Introduction」**

- 使用类似 BERT 的模型进行编码，而不是 BOW。损失函数也采用的 InfoNCE
- 构造正负样本的思路跟 C-NTM 很像，只不过是短语（phrase）层面的，而不是 word 层面的

**「Method」**

1. Training 分为两个阶段

- In-batch pretraining 阶段

本文认为，**topic 相同的 ducoment 应该 share 相似的短语（phrase）**，即多个 token 组成的序列，例如 United States。这样的样本对互为**正样本**，这一步应该也是正则之类的进行筛选的。



例如：

He lived on the east coast of the United States.

How much does it cost to fly to the United States.



In-batch negative sampling 策略是非常常用的一种构造负样本的方法，在 [SimKGC](https://www.yuque.com/lirt1231/mq52vb/ndqp67nt15wvtr0h#mMcC1) 中也有，即认为**同一个 batch 中的其他样本都是负样本**。得到 pre-training 阶段的损失函数



![img](https://cdn.nlark.com/yuque/0/2023/png/805730/1687612276775-8099a2f8-d52f-4b7c-9250-d97a753de9cc.png)

- Finetune阶段

由于 in-batch 采样的方法存在一个致命的缺点，即有些语义相似的 ducoment 也可能被作为负样本。因此 finetune 阶段要进一步对 pre-training 阶段可能造成的语义损失问题进行修正。

Finetune 阶段还是 contrastive learning，只不过正负样本的选择有变化。**对 pre-training 阶段学习到所有 document 的 embedding 进行聚类**，则可以粗略的得到每个 topic 下的一些 document，将每个 topic 下的 document 作为正样本，不同 topic 之间的 document 作为负样本，即可完成 finetune。可以说是强化 pre-training 得到的结果吧。

[Notes]

**实际上还是根据词频（share 共同的 phrase）来确定的正负样本，没有考虑词语词之间的语义相似性、句子之间的语义相似性等**

## UNTM

这篇文章主要提出一个`Term Weighting`，也即是术语加权。

我们使用对比学习框架来帮助语言模型灌输
预先训练的知识，并使模型能够集中在有影响的词汇上。该模型分为三个阶段:

(1)识别语义聚类，用一个预训练模型去做文档的embedding，并把他分成一些集合

> 文中就是用Bert，把这些文档分成不同的集合，损失函数用的InfoNCE

(2)计算词权值 。   就是计算每个集合中每个词的术语权重（tf-idf），然后去top-k项权重的词作为BOW最终词集。

> 诶，这里的term和C-NTM的salient word感觉就是一个东西
>
> 只不过C-NTM用这个词来弄正负样本，这里拿来当bow的词集，然后计算潜在分布？不知道是啥

> 这里注意术语权重定义，在一个语义相近的一个集合中频繁出现的词，但是在其他集合中没出现的词，权重很高
>
> 也就是这个词最能代表这个集合的词权重高

(3)估计潜在的话题分布。模型通过重建BoW表示来估计潜在的主题分布。