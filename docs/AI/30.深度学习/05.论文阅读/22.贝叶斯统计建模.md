---
title: 贝叶斯统计建模
date: 2023-07-05
tags: 
  - 
categories: 
  - 深度学习
  - 论文阅读
---

> 参考天哥博客 https://www.yuque.com/lirt1231/bad1ye/iti5o9opro6ih9hf

## 贝叶斯统计

### 定理

设 X1, X2, . . . , Xk 是两两互不相容的事件组，且设事件 Y 仅可能伴随 事件组 X1, X2, . . . , Xk 其中之一发生，则有

> 这是离散型贝叶斯



![image-20230705100419859](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705100419859.png)

连续型贝叶斯

![image-20230705100616168](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705100616168.png)

> 其中 f (∗) 为概率密度函数。

### 先验，后验，似然概率                      

![image-20230705102501121](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705102501121.png)

### 极大似然估计

>（Maximum Likelihood Estimation, MLE）

它基于概率论的思想，通过寻找最大化观测数据出现概率的参数值，来估计未知参数的值。

具体来说，假设我们有一组独立同分布的观测数据 {x₁, x₂, ..., xₙ}，我们假设这些数据服从某个参数为 θ 的概率分布 P(x; θ)。那么，给定观测数据，我们可以定义一个关于参数 θ 的似然函数 L(θ)，表示观测数据出现的概率。似然函数可以通过将每个观测数据的概率密度函数相乘得到：

L(θ) = P(x₁; θ) * P(x₂; θ) * ... * P(xₙ; θ)=P(X|θ)

为了寻找最大似然估计值，我们需要找到使得似然函数取得最大值的参数值 θ̂。通常，为了方便计算，**我们对似然函数取对数**，得到对数似然函数 LL(θ)：

LL(θ) = log L(θ) = log P(x₁; θ) + log P(x₂; θ) + ... + log P(xₙ; θ)

然后，我们通过最大化对数似然函数 LL(θ)来估计参数值 θ̂。**这可以通过求导数或使用优化算法来实现。**

在下面抛硬币的例子中，模型参数 θ 就是硬币正面和反面朝上的概 率，而 X 代表本次实验硬币是正面朝上还是反面朝上。下图展示了 MLE 估计的过程

![image-20230705103151283](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705103151283.png)

![image-20230705103211953](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705103211953.png)

> 所以极大似然估计其实就是在根据x现在的统计结果，判断硬币是向上的概率大还是向下的概率大
>
>  例如我 们抛出 100 次硬币，60 次正面朝上，40 次反面朝上。在没有任何其 他影响因素掺入进来时，我们给出的答案就是硬币正面朝上的概率为 0.6。而这，就是极大似然的直观表达。

### 交叉熵

![image-20230705104754488](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705104754488.png)



![image-20230705104802155](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705104802155.png)

### 最大后验估计（正则化）

>（Maximum A Posterior, MAP)
>
>后验概率实际 上就是当新的证据、新的观测，新的数据点及其标签出现后，我们对 模型参数先验的修正

这里把P(X)略去了，原因下面有解释

![image-20230705105128749](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705105128749.png)

L2正则化  

![image-20230705105543984](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705105543984.png)

### 总结

![image-20230705112821778](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705112821778.png)

![image-20230705112833464](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230705112833464.png)

## 生成模型和判别模型

### 判别模型

生成模型的目标是通过**学习不同类别之间的分类边界**，进而完成分类任务的。再比如 SVM，其初衷就是想学习一个超平面将不同的类别分开：

![image-20230713204018998](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713204018998.png)

从概率的角度来讲，判别模型要做的就是学习似然概率![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/7bf3eecb75780fa32bdc4587c8dee043.svg)，即给定样本点![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/94e79ad0c1aabeafef9e2fc4af6adf66.svg)判断它属于每一类的概率。也可以将模型看做一个函数，输入是![img](https://cdn.nlark.com/yuque/__latex/94e79ad0c1aabeafef9e2fc4af6adf66.svg)输出是![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/6204886f5cc39a4b860ea98a7e95af1d.svg)

给定训练数据，判别模型的的任务就是**最大化似然概率**![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/6ffd1cff89df2026ab626db1a45a9546.svg)，当然一般采用的就是**极大似然法**了。交叉熵损失函数就是极大似然法指导下的产物。

常见的判别模型有：

- Logistic 回归
- SVM
- 决策树
- 随机森林
- 大多数深度神经网络

### 生成模型

例如输入![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/94e79ad0c1aabeafef9e2fc4af6adf66.svg)为一个苹果，其有三个特征：

| 特征                                                         | 取值             |
| ------------------------------------------------------------ | ---------------- |
| ![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/0e8831d88c93179dbe6c8b5e3678ca20.svg)果皮颜色 | 绿色 0 \| 红色 1 |
| ![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/d8a3b4f592953b89e7cbeac6f027b2d0.svg)形状 | 圆形 0 \| 方形 1 |
| ![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/d7a8111634ca196ffa86313509574285.svg)口感 | 脆 0 \| 面 1     |

标签是苹果是酸的（0）甜的（1）还是没什么味道的（2）。

判别模型当然就是，输入![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/5357757f85f8ecf01ddfa1927d361056.svg)，直接输出它对应的类别![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/0a93ce61f01ddfb88655ee2103c37b59.svg)。

而生成模型的目标则是**给定一个样本的类别**![img](https://cdn.nlark.com/yuque/__latex/0a93ce61f01ddfb88655ee2103c37b59.svg)**，它能学习出该样本在取值空间中取每一个值概率**，即：

![image-20230713204448796](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713204448796.png)

常见的生成模型：

- 隐狄利克雷分配 LDA
- 贝叶斯网络
- 隐马尔可夫模型 HMM
- 生成对抗网络 GAN 
- 变分自编码器 VA

### 区别

- 生成模型是对数据的分布建模，所以不容易过拟合：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713205221128.png" style="zoom:70%">

而如果是判别模型，就容易错误分类：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713210222213.png" style="zoom:70%">

生成模型缺点，当数据有很多异常点（outliers）其分类性能会比较差：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713210314750.png" style="zoom:70%">

> 一个故事：
> 一位父亲有两个孩子，孩子 A 和孩子 B。孩子 A 有一个特殊的性格，但他可以深入学习一切。孩子 B 有一个特殊的性格，而他只能学习他所看到的差异。
> 一天晴朗，父亲带着他的两个孩子（孩子 A 和孩子 B）去动物园。这个动物园很小，只有狮子和大象两种动物。从动物园出来后，父亲给他们看一只动物，问他们俩：“这只动物是狮子还是大象？”
> 孩子A，孩子突然根据在动物园里看到的东西，在一张纸上画出了狮子和大象的形象（`生成`模型）。他将这两个图像与前面站立的动物进行比较，并根据图像和动物最接近的匹配来回答（`贝叶斯公式进行分类`），他回答：“动物是狮子”。
> Kid B 只知道区别，根据所学的不同特性，他回答：“动物是狮子”。
> 在这里，我们可以看到他们都在寻找动物的种类，但是学习的方式和寻找答案的方式是完全不同的。在机器学习中，我们通常将 Kid A 称为生成模型，将 Kid B 称为判别模型。
> 一般来说，判别模型对类之间的决策边界进行建模。生成模型明确地模拟每个类的实际分布。最后，他们都在预测条件概率 P(Animal | Features)。但是两种模型都学习了不同的概率。

## EM算法

EM 算法（Expectation Maximization Algorithm, EM 算法）是一种用于求解极大似然概率的**迭代算法**，其每一次迭代都由 E 步（求期望）和 M 步（求极大似然）组成，故名 EM 算法，位列数据挖掘十大算法之首。准确一点，EM 算法常用来求含有隐变量（观测不完整）的极大似然估计，那什么是隐变量、什么是观测不完整。  

推导

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713211536532.png" style="zoom:80%">

其中红色的曲线是对数似然函数值![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/af17899b752bd3f5761873e29b41ce0c.svg)，而绿色曲线是代表式（15）中的![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/08d50413e24ee6970c64e449933b954b.svg)。每次 E 步求期望都会得到一个新的函数![img](https://cdn.nlark.com/yuque/__latex/08d50413e24ee6970c64e449933b954b.svg)，随后对这个函数求极大值进而得到![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/2f3847695c87313a72d0c48078ec1af7.svg)。经过**反复迭代最终逼近原极大似然估计得到的参数估计值**。

![image-20230713212453662](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713212453662.png)

>https://www.yuque.com/lirt1231/bad1ye/enggvo

## 概率分布采样

>Sampling from distributions 

概率分布采样（sampling）和变分推断（variational inference）是求解传统贝叶斯模型参数的两种最常用的方式

> **推理（inference）**在贝叶斯学习中指的就是求解**生成样本以及模型参数**

### 蒙特卡洛法的引入

采样当然就是字面意思，就是从一个分布中抽取样本的过程。记随机变量![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/94e79ad0c1aabeafef9e2fc4af6adf66.svg)的概率分布为![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/6178245e6cc0629e1f5fe1f82a4c192c.svg)，从中抽取的样本是![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/712ecf7894348e92d8779c3ee87eeeb0.svg)：



![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/a258ddad4a5cd7f4a134d4c80dc813eb.svg)



说起采样，就不得不提到**蒙特卡洛方法（Monte Carlo method）**，也被称为统计模拟方法（statistical simulation method）。大家最早接触的一个例子可能就是用计算机抽样模拟计算单位圆的面积：

```python
import numpy as np


# total number of samples
num_samples = 100000
# sample from uniform(-1, 1)
x_square = np.random.uniform(-1, 1, (num_samples, 2))
# get samples inside the unit circle
x_circle = x_square[x_square[:, 0]**2 + x_square[:, 1]**2 <= 1]
# probability of points inside the circle
p_circle = x_circle.shape[0] / x_square.shape[0]
# estimated area of the unit circle
area_circle = 4 * p_circle
print(f"area = {area_circle:.5f}")import numpy as np


# total number of samples
num_samples = 100000
# sample from uniform(-1, 1)
x_square = np.random.uniform(-1, 1, (num_samples, 2))
# get samples inside the unit circle
x_circle = x_square[x_square[:, 0]**2 + x_square[:, 1]**2 <= 1]
# probability of points inside the circle
p_circle = x_circle.shape[0] / x_square.shape[0]
# estimated area of the unit circle
area_circle = 4 * p_circle
print(f"area = {area_circle:.5f}")
```

- 首先从横坐标与纵坐标范围均为（-1， 1）的面积为 4 的正方形区域中随机采样
- 随后计算样本落在单位圆（![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/5cc06f48756e44f2b5fc0e42e5052964.svg)）中的概率
- 用正方形的面积 4 乘以样本点落在单位圆中的概率，最终得到面积

**蒙特卡洛思想**就是**采样、就是想办法从给定概率分布中随机抽取样本**。**蒙特卡洛法的核心也就是随机采样**。通过采样，进而用样本期望估计原分布的期望、估计模型参数、求解积分等。正如上例中，将**面积计算转换成求解期望是我们对问题的建模，而求解期望的具体方法是蒙特卡洛方法**

### 采样作用

1 **用于生成模型（generative models）生成新的样本**

生成模型就是对数据分布建模，近似数据的原始分布，最后通过从模拟分布中采样来生成样本。

2 **估计模型参数**

除了上面求单位圆面积的例子，我们还可以举出求样本均值和方差的例子。对于蒙特卡洛法还是要强调，首先我们要建立概率模型，然后才是用蒙特卡洛法求解模型。

现在记某学校男生身高的随机变量为![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/94e79ad0c1aabeafef9e2fc4af6adf66.svg)，希望求身高的分布，记为![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/6178245e6cc0629e1f5fe1f82a4c192c.svg)。我们假设该分布为高斯分布：

![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/4788509813d6570a7afd8e909df752bf.svg)

其中![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/3aec12fbf626fa7c8588cea35442bb0f.svg)是高斯分布也是我们模型的参数，即均值与方差。求解模型也就变成了估计参数![img](https://cdn.nlark.com/yuque/__latex/3aec12fbf626fa7c8588cea35442bb0f.svg)的过程。

基于蒙特卡洛法，我们只需要从某学校抽取足够多的男生身高样本，也就是从![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/6178245e6cc0629e1f5fe1f82a4c192c.svg)中采样，**就可以根据MLE 极大似然估计求出**![img](https://cdn.nlark.com/yuque/__latex/3aec12fbf626fa7c8588cea35442bb0f.svg)。只不过通过 MLE 估计出来的![img](https://cdn.nlark.com/yuque/__latex/3aec12fbf626fa7c8588cea35442bb0f.svg)恰好就是样本的均值和方差。

所以在我看来，如果用来求解概率分布（模型）参数，**抽样**实际上就是为了**对样本做极大似然估计 MLE**或者最大后验估计 MAP** **而服务的**。只不过对似然概率分布采样，最后求得就是 MLE；对后验分布采样，最后做的就是 MAP。