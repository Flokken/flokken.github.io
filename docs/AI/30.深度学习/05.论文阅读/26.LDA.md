---
title: LDA
date: 2023-07-13
tags: 
  - 
categories: 
  - 深度学习
  - 论文阅读
---

## LDA

>Latent Dirichlet Allocation LDA 主题模型

**主要是以LDA 为例思考通过贝叶斯生成模型给问题建模、并解决问题的方式**。

生成式模型求解问题的思路总结起来就三步：

1. 想想数据的生成过程
2. 通过概率模型描述你想的这个生成过程
3. 运用各种手段解模型参数

### LDA介绍

**LDA (Latent Dirichlet Allocation)** 是 NLP 中解决**主题模型（topic modeling）**这一子任务的模型，中文名叫潜在狄利克雷分配。

**主题模型（topic modeling）**这一任务实际上说的就是，给定一堆文档（博客、评论、朋友圈动态等，总之就是一段文本，可长可短）![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/7f1b68a2484cb07eaaca91d4a5f05d53.svg)，模型的任务就是输入一篇文档，输出这篇文档对应的主题（topic）是什么，比如体育、新闻、经济等。一篇文章可以对应多个主题，同样也可能有多篇文档符合同一个主题。

LDA 中的这三个词也比较好理解啦：

- Latent 就是隐空间、隐变量的“隐”，指的是 LDA 是通过隐变量来建模的，这也符合贝叶斯学习框架
- Dirichlet 狄利克雷，指的是 LDA 模型中用的一个主要概率分布是狄利克雷分布
- Allocation 分配，指的是  LDA 是通过“分配”的方式来考虑一篇文档的生成过程

#### BoW词袋模型

>https://blog.csdn.net/Elenstone/article/details/105134863

BoW(Bag of Words)词袋模型最初被用在文本分类中，将文档表示成**特征矢量**。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。

如三个句子如下：

句子1：小孩喜欢吃零食。
句子2：小孩喜欢玩游戏，不喜欢运动。
句子3 ：大人不喜欢吃零食，喜欢运动。
首先根据语料中出现的句子分词，然后构建词袋（每一个出现的词都加进来）。计算机不认识字，只认识数字，那在计算机中怎么表示词袋模型呢？其实很简单，给每个词一个位置索引就可以了。小孩放在第一个位置，喜欢放在第二个位置，以此类推。

`{“小孩”:1，“喜欢”:2，“吃”:3，“零食”:4，“玩”:5，“游戏”:6，“大人”:7，“不”:8，“运动”:9}`

其中key为词，value为词的索引，语料中共有9个单词， 那么每个文本我们就可以使用一个9维的向量来表示。
如果文本中含有的一个词出现了一次，就让那个词的位置置为1，词出现几次就置为几，那么上述文本可以表示为：

句子1：[1,1,1,1,0,0,0,0,0]
句子2：[1,2,0,0,1,1,0,1,1]
句子3：[0,2,1,1,0,0,1,1,1]

该向量与原来文本中单词出现的顺序没有关系，仅仅是词典中每个单词在文本中出现的频率。

**总结一下：**

- 向量的维度等于**词汇表**的大小，**一般为所有文档中不重复词的个数**
- 向量没有体现原文档中词的顺序
- 文档中每个元素代表了该单词在文档中出现的次数 —— 词频

###  LDA for  topic modeling LDA 的话题识别思路

首先要强调的是，**LDA 是无监督生成模型**，而不是有监督的判别模型。如果是一般判别模型的思路来解 topic modeling 问题，**可能就是给好几篇文档，然后给每个文档打上不同话题的标签，最后做一个分类。**

而 LDA 作为一种无监督生成模型，我们的目标是识别每一篇文档对应的话题，那么自然就要**对一篇文档的生成方式进行建模**。

LDA 的话题分类思路是，以话题为指标对文档进行无监督聚类。通过无监督聚类解决分类问题的思路是：

1. 首先指定簇个数$K$为超参数
2. 让模型在训练集上聚类，将样本点聚集成$K$簇
3. 训练结束后，人为给每一个簇打上标签![img](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/66be86c07518b7ce2c5a16163c627e49.svg)
4. 在模型部署或测试时，模型预测输入样本的类别，就是它被模型分派到的簇上对应的类别

举例：

而 LDA 识别文档主体的思路就是一种无监督聚类的方法，如下图所示

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230713223352537.png" style="zoom:70%">

1. 首先我们指定模型的超参数，即要聚类的话题的个数K=3
2. 随后在训练集中对以上文档依据话题进行聚类
3. 训练完成后，人为的观察每一个簇并给每一个指定一个具体的 topic，也就是上图中的 Science, Politics 以及 Sports。
4. 最后在模型测试过程中，可以将模型用于话题分类任务

> 这样的好处是，不需要人为给数据集打标签（指的是对每篇文档逐一打标签），灵活性很高，而且**模型也许能发掘一些人们想不到的 topic**
>
> 坏处是仍然要人干预，每个人指定的 topic 可能都不相同；而且当话题个数增大时，人力成本也会显著提高

- 由于 **LDA 是概率模型**，所以在上图的三角中，文档越靠近顶点，说明模型越确信它输入那一类 topic，也就是概率越大；反之越远离三角形顶点，模型认为它属于单一 topic 的概率越小

- - 上面的三角形实际上是[狄利克雷分布（dirichlet distribution）](https://www.yuque.com/herormlihaotian/pugkgw/bc3i4b)的图

- 如何观察每一个簇并给每一个指定一个具体的 topic？是观察每个topic 下的文档？

- - 如果通过观察每一个 topic 簇下的文档来判断这个 topic 具体是关于什么的，工作量未免有点大
  - 事实上，因为我们**将文档建模成了词的集合**，所以指定文档的话题就变成了识别一组单词的话题。所以 LDA 最后会返回**每个话题下最经常出现的词汇都有哪些**，**这样人们可以通过判断这些词共同描述了那一件类似的事情，进而给 topic 打标签**

### LDA文本生成过程

为了生成一篇文档，并且能够结合 topic modeling 的问题假设，LDA 引入了 **topic 这个隐变量**来辅助生成文档。

LDA 认为，**话题是由单词（的分布）定义的**，而**不同话题 topic 之间的差异在于，每个话题下单词出现的频率是不一样的**。如下图所示

![image-20230714100423889](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714100423889.png)

可以看出，因为蓝色 topic 下“Galaxy”“Planet”出现的概率较高，所以将其打成“Science”的标签，其他话题同理。这样反过来讲，**每个单词也会有自己的主题，它属于不同主题的概率是不同的**。

基于上面两点观察，我们引出 LDA 的**两个重要假设**：

1. 每个单词（word）也最好只属于一个主题（topic）—— 文档![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)中每个单词![img](https://cdn.nlark.com/yuque/__latex/c237419c2f00f32dfd25dd4571132c6c.svg)都有自己的主题
2. 每篇文档（document）最好只有一个主题（topic）—— 文档![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)的主题就是其下所有单词所属最多的主题

**结论**

由于 LDA 采用的是词袋模型，文档 documents 被抽象成了词汇的序列。所以在 LDA 看来生成一篇文档$d_m$,本质上就是如何生成一个具体的词汇$w_{mn}$

的问题，把这个过程重复$N_m$遍，就可以得到一篇文档$d_m$，把生成文档的过程重复M遍，就得到了整个数据集$D={d_1,...,d_m..,d_M}$

**也就是只需要描述 LDA 是怎么生成一篇文档![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)下的一个具体词汇$w_{mn}$**

### 词汇的生成

首先要明确那句话，LDA 是个概率模型，描述所有问题都是**从概率出发的**。换句话说，document 属于哪个 topic 是有概率的，具体一个 word 属于哪个 topic 也是有一定概率的，这些都不是确定的：

- **doc - topic 概率**：每篇文档属于哪个主题的概率是不同的
- **topic - word 概率**：每个 topic 下不同 word 出现的概率是不同的



首先给定超参数 topic 的数量$K$,用$Z= {x_1,...z_k,...z_K}$表示所有话题集合

LDA生成一篇文档$d_m$下一个具体词汇$w_{mn}$分为两步

1. 根据![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)的 doc - topic 概率决定（采样)一个主题$z_k$
2. 根据$z_k$的的 topic - word 概率决定（采样）得到$w_{mn}$这个单词

将上述两步重复相应的次数就可以得到整篇文档![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)以及整个数据集$D$

#### **topic->word**

先来说如果通过一个给定的话题$z_k$决定（采样）一个$w_v$。**这里 LDA 采用了 unigram 模型**，那就是

- 上面我们说 LDA 假设 document 是一个一个 word 构成的序列，这实际上就是 unigram 模型的第一个假设 —— 即先给定文档的数学模型
- **unigram 生成一个单词的时候认为：**

- - 有一个![img](https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg)个面硬币（或者或是骰子），代表词汇表中的![img](https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg)个单词，且每个面的朝上的概率不同
  - 当生成第![img](https://cdn.nlark.com/yuque/__latex/4760e2f007e23d820825ba241c47ce3b.svg)个文档的第![img](https://cdn.nlark.com/yuque/__latex/df378375e7693bdcf9535661c023c02e.svg)个词$w_{mn}$时，上帝抛出了这个![img](https://cdn.nlark.com/yuque/__latex/9f493997c33913987175caf4a4849955.svg)个面的硬币，如果第v个面朝上，则$w_{mn}$就是词汇表中第$v$个单词$w_v$

- 

- 

- Unigram 生成一个单词的过程如下图所示：这实际上就是一个**从多个单词中依照每个单词出现的概率选一个**的问题

![image-20230714104011782](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104011782.png)

这个**骰子代表的就是一个具体话题 topic** $z_k$，更准确地说是 topic - word 概率分布。上面也说过，**每个话题下各个单词出现的频率（概率）是不一样的**，所以超参数假设有![img](https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg)个不同的话题，这里也就会有![img](https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg)个不同的骰子。

这![img](https://cdn.nlark.com/yuque/__latex/38a3f4d664b7a723d138f9d57be0c783.svg)个骰子每个面取值概率就是 LDA 要学习的参数之一。

#### **document -> topic**

![image-20230714104223105](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104223105.png)

### 文档生成过程

总结一下 LDA 模型对整个数据集生成的过程

![image-20230714104313119](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714104313119.png)

需要注意的是：

- 所有筛子$\theta$,*φ*都是**提前生成好**的（需要结合先验来理解)
- 所有文档的生成互相独立的，而一篇文档内的单词出现的概率也是相互独立

- - 所以叫做 unigram 模型，如果考虑一个单词的生成依赖于之前 k 个单词，这就是 k-gram 模型

### LDA的训练

>Training LDA with Gibbs sampling 用吉布斯采样训练 LDA

如何学习模型参数训练 LDA?   用吉布斯采样

>在统计学和机器学习中，我们经常需要从一个复杂的概率分布中获取样本，以便进行推断、估计或模型训练。然而，有时直接从这种概率分布中采样是困难的或不可行的。Gibbs sampling 提供了一种通过对各个变量进行逐一采样的方法，从而实现从联合分布中采样的技术。
>
>Gibbs sampling 基于马尔可夫链的思想。它通过定义一个马尔可夫链，其中每个状态都是变量的一个取值组合。在每一步中，Gibbs sampling 从当前状态中选择一个变量，然后根据该变量的条件分布对它进行采样。在采样过程中，其他变量的取值被保持不变。然后，该变量的新取值被用于更新当前状态，然后继续进行下一步。这个过程会经过多个步骤，直到达到收敛。

举例:

现在有三个主题，用红绿蓝表示，有四篇文档，每篇文档五个单词如图：

![image-20230714105240210](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714105240210.png)

用 Gibbs sampling 的方法确定每个单词的 topic：

1. 首先给所有 word **随机赋予一个 topic**，然后**一个一个单词挨着来决定 topic**。
2. 在决定每一个单词的 topic 时，都假定其他单词的 topic 都是已经确定好的（就像房间里已经摆好位置的物品）。
3. 给该单词赋予“离它最近”的 topic

那什么叫离它最近呢？—— 我们可以这样考虑：

- $w_{mn}$这个单词很有可能被赋予该篇文档![img](https://cdn.nlark.com/yuque/__latex/063e1078dcf5123757cd420fc9012744.svg)中出现次数最多的 topic
- 假设$w_{mn}$是单词表中第$v$个单词$w_v$,因为同性相吸，$w_{mn}$也有可能是$w_v$被赋予次数最多的topic（注意一开始所有 word 的 topic 是随机初始化的，所以不同文档中同一个$w_v$可能被赋予不同的 topic)

直接来看最终结果

![image-20230714105741649](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230714105741649.png)
