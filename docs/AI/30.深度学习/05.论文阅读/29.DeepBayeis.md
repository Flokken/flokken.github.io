---
title: DeepBayes
date: 2023-07-17
tags: 
  - 
categories: 
  - 深度学习
  - 论文阅读
---

## Deep Bayeis

### ETM

ETM是LDA的深度学习的版本

![image-20230716224426537](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230716224426537.png)

这里有一个Logistic-norm，其实也是为了重参数，让他可以做反向传播，之所以没有用vae的那个，是因为这篇论文更早，还没发明这个trick，因此编了一个。和高斯分布那个重参数化很像。

![image-20230717101339930](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717101339930.png)



### WAE

WAE可以看做是 WGAN 和 VAE 的产物，可以说既解决了 GAN 的一些问题，也解决了 VAE 的顽疾。

**主要是把只把 KL 散度那一项改进了**

复习VAE的elbo

![image-20230717103710460](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717103710460.png)

**WAE的的改进就是不用KL散度度量两个分布之间距离了，而是使用MMD损失**

#### Maximum Mean Discrepancy 最大平均偏差 MMD

**MMD (Maximum Mean Discrepancy)** 最早是 WGAN 为了解决 GAN 相关问题时引入的，后来被 WAE 引入到了 VAE 的计算框架下，实际上也是机器学习、统计学中早就开始用的东西。

一言以蔽之，MMD 告诉我们，想要**度量两个分布之间的距离**，可以通过**度量从两个分布中采样的点之间的距离**来完成。这样我们**不需要计算两个分布之间 KL 散度的表达式，更不需要做 reparameterization trick**，只需要从分布中采样点，用 MMD 即可计算两个分布之间的距离，作为 VAE 损失中的正则化项。



![image-20230716224440844](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230716224440844.png)

![image-20230717103819875](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717103819875.png)

> MMDLoss   就代表了  Wasserstein distance

![image-20230717104336222](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104336222.png)

![image-20230717104326885](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104326885.png)

> 为什么用高位映射？低维的线性不可分到了高维，也许就是线性可分的了

#### 核函数

![image-20230717104541998](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104541998.png)

![image-20230717104550156](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104550156.png)

![image-20230717104648939](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230717104648939.png)

#### 总结

**值得注意的是，WAE 通过 MMD 做到了：**

1. WAE 告诉我们，度量对两个分布中采样的点的距离，也可以作为 度量分布之间距离的方法 
2. WAE 帮我们解决了 VAE 中 KL 计算复杂、无法应用于复杂分布 的问题 
3.  WAE 损失计算关键在 MMD，关键在于 kernel 的选择*

