---
title: Bert
date: 2023-07-02 
categories: 
  - 深度学习
  - 深度学习-李宏毅2022
tags: 
  - null
---

## Self-supervised Learning

我们之前跑的模型都是有监督训练，那现在就有无监督训练(unsupervised),自监督训练属于无监督训练的一种

自监督训练指的就是在没有标注的文档中，模型自己给他标注了来训练自己

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702180427387.png" style="zoom:70%">

### 笼统介绍Bert

Bert要做的事仍然是输入一堆向量，然后输出一段向量。

> 也就是和Transformer Encoder一样，是输入一个seq，输出一个等长的seq
>
> 区别于seq2seq,因为这个的seq是模型自己预测的

但是Bert在训练时，需要把输入的tokensMasked，这个Mask可以是随机替换其中的东西，也可以替换为某一固定的token

> Bert在训练的时候就需要自己猜，被遮住的是什么



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702215548739.png" style="zoom:70%">

同时，Bert还需要做一个任务叫Next Sentence Prediction ，也就是判断两个句子是否要拼接在一起

> 但是后来的研究发现这个任务对Bert本身的任务没啥用（可能因为太简单了）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702220302062.png" style="zoom:70%">



Bert的应用

Bert通过训练，可以得到一个pre-train,这些pre-train经过Fine-tune，可以用来搞各种各样得任务

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702214045771.png" style="zoom:70%">

一些评测集

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702214134307.png" style="zoom:70%">

>GLUE also has Chinese version (https://www.cluebenchmarks.com/)

### 怎么用Bert（下游任务）

这里都用文字举例，但实际上Bert也能用于其他模态比如语音和图片

#### 判断句子的类型

注意这里的Bert是已经预训练过的了，**所以这个任务其实是微调**

> 需要提供一下标注资料去训练Bert，比如哪些句子是什么类的

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702220701470.png" style="zoom:70%">

下面是GLUE评测集上，pre-train和随机初始化bert模型训练结果展示（scratch）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221001428.png" style="zoom:70%">

#### 词性标注

给Bert输入一段话，bert对其中的每个词语都标注词性

> 更笼统的说，应该是输入一个sequence，输出一个一样长的sequence

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221222714.png" style="zoom:70%">

#### 推理

比如NLi，给出两个句子，给出前提和假设，推导这两个句子的关系，比如矛盾，赞成，中立

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221903667.png" style="zoom:70%">

Bert解决如下:

我们把两个句子隔开，输入Bert，但是我们只取CLS这个的输出，当成类别

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222126299.png" style="zoom:70%">

#### 有限制问答

要求输入和问题都是一个seq，**输出的s，e代表的是文章里的s到e的就是问题的答案**



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222320108.png" style="zoom:70%">

Bert如何解决该问题？

首先对于输入，要用一个分隔符，把Q和D分开输入到Bert，然后Bert从D中得到两个向量的输出（都是随机初始化的），得到的两个输出当成下标就是答案

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222734328.png" style="zoom:70%">



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222821951.png" style="zoom:70%">

### Bert怎么工作的？

#### 解释一

最常见的解释就是，Bert输出的是embedding，在空间不但考虑了单词本身，也考虑了上下文

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223616937.png" style="zoom:70%">

下面是将同一个字，不同上下文的文本输进去的结果

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223721416.png" style="zoom:70%">

可以看见，Bert的结果的确是前五个和后五个之间相似度不高，但他们内部由于上下文相近，相识度高

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223846990.png" style="zoom:70%">

仔细探究

当把Mask的向量输入Bert时，其实Bert做的事和CBOW一模一样，都是把他上下文的向量输进去，来得到这个向量的输出

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224103174.png" style="zoom:70%">

#### 解释二（待补充）

### 多语言Bert

**如果对Bert进行多语言的训练，在进行英文的问答，居然在中文的测试集上也能有良好的表现**

> 当然要求预料要很高



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224638199.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224714692.png" style="zoom:70%">

#### 一种解释

对于不同语言，其意思相近得词他们的向量表示在空间上也比较相近

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224922572.png" style="zoom:70%">

### 奇怪的地方

如果把Bert英文的embedding和中文的embedding相加减，是可以让他实现中英文输出互换的

> 也就是Multi - Bert他里面蕴含了语义的信息

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702225343460.png" style="zoom:70%">

### GPT的野望

GPT的结构一般认为就是Transformer的Decoder，都是预测下一个输出，也就是训练的时候，预测这个时，会把右边的遮住，只能使用左边输入的embedding。

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709162045007.png" style="zoom:70%">

#### 一些应用

少样本学习（no gradient,），one样本学习，零样本学习

> 一般的学习都有gradient，GPT的不改，所以起来个名字叫In-Context Learning

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709162432236.png" style="zoom:70%">

## 作业

BERT的应用，一般就是给他很多资料训练，然后在对不同的下游任务进行微调即可

### 问题描述

输入一个paragraph和question，bert输出从paragraph里找的答案

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709165727865.png" style="zoom:70%">

数据形式，其实在代码里，就是用json来表示的

训练集有answer，测试集没有，让bert输出answer

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709165858370.png" style="zoom:80%">



### tokenize

> 助教叫他断词

首先，要把东西输入Bert，要经过tokenize

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709163608890.png" style="zoom:70%">

有了id之后，还需要把这些东西转换成vector,这里使用one-hot vector得到vector之后，还需要做preprocess

我们要传入Bert的有三个部分：

- input_ids:包括特殊token;question，document
  - [CLS]表示开始，[SEP]隔开两个输入，[PAD]是占位的，好让输入长度一样
  - token_type_ids:主要用在输入有两段话以上才用到，这里第一段话是0，第二段话是1;   (不知道为什么PAD算0？)
  - attention_mask:希望Bert学习的部分，这里除了[PAD]占位的不需要学习设为0，其他的设为1
- attention_mask

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709163741008.png" style="zoom:70%">

### 长句子

因为自注意力机制时间复杂度是O(n^2),因此限制每次输入的长度，这里指的是token个数（包括特殊符，问题，段落）就是512

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709170125147.png" style="zoom:70%">

这里根据训练还是测试用不同的办法解决，

> 其实就是设置一个windows_size限制输入大小

#### Training

对于训练集，已经知道正确答案的位置，这时候可以以正确位置为中心，**展开一个固定大小的windowsize**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709170818678.png" style="zoom:70%">

#### Testing

对于测试集，不知道正确答案，因此会对每一个windows，去预测一个起始点和终点的得分，取得分高的为answer

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230709171042035.png" style="zoom:70%">

### Learning-rate的修改

随着训练轮次增加，当然应该减小学习率，loss才能继续下降，可以使用pytorch的scheduler来完成

### Hints

1 训练时，我们都以正中心展开windows，这会导致Bert学习到没有必要得东西（比如Bert会认为正确答案都在windos中间，这在test中显然不一定）

2 有可能start_pos>end_pos
