---
title: Bert
date: 2023-07-02 
categories: 
  - 深度学习
  - 深度学习-李宏毅2022
tags: 
  - null
---

## Self-supervised Learning

我们之前跑的模型都是有监督训练，那现在就有无监督训练(unsupervised),自监督训练属于无监督训练的一种

自监督训练指的就是在没有标注的文档中，模型自己给他标注了来训练自己

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702180427387.png" style="zoom:70%">

### 笼统介绍Bert

Bert要做的事仍然是输入一堆向量，然后输出一段向量。

> 也就是和Transformer Encoder一样，是输入一个seq，输出一个等长的seq
>
> 区别于seq2seq,因为这个的seq是模型自己预测的

但是Bert在训练时，需要把输入的tokensMasked，这个Mask可以是随机替换其中的东西，也可以替换为某一固定的token

> Bert在训练的时候就需要自己猜，被遮住的是什么



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702215548739.png" style="zoom:70%">

同时，Bert还需要做一个任务叫Next Sentence Prediction ，也就是判断两个句子是否要拼接在一起

> 但是后来的研究发现这个任务对Bert本身的任务没啥用（可能因为太简单了）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702220302062.png" style="zoom:70%">



Bert的应用

Bert通过训练，可以得到一个pre-train,这些pre-train经过Fine-tune，可以用来搞各种各样得任务

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702214045771.png" style="zoom:70%">

一些评测集

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702214134307.png" style="zoom:70%">

>GLUE also has Chinese version (https://www.cluebenchmarks.com/)

### 怎么用Bert（下游任务）

这里都用文字举例，但实际上Bert也能用于其他模态比如语音和图片

#### 判断句子的类型

注意这里的Bert是已经预训练过的了，**所以这个任务其实是微调**

> 需要提供一下标注资料去训练Bert，比如哪些句子是什么类的

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702220701470.png" style="zoom:70%">

下面是GLUE评测集上，pre-train和随机初始化bert模型训练结果展示（scratch）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221001428.png" style="zoom:70%">

#### 词性标注

给Bert输入一段话，bert对其中的每个词语都标注词性

> 更笼统的说，应该是输入一个sequence，输出一个一样长的sequence

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221222714.png" style="zoom:70%">

#### 推理

比如NLi，给出两个句子，给出前提和假设，推导这两个句子的关系，比如矛盾，赞成，中立

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702221903667.png" style="zoom:70%">

Bert解决如下:

我们把两个句子隔开，输入Bert，但是我们只取CLS这个的输出，当成类别

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222126299.png" style="zoom:70%">

#### 有限制问答

要求输入和问题都是一个seq，**输出的s，e代表的是文章里的s到e的就是问题的答案**



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222320108.png" style="zoom:70%">

Bert如何解决该问题？

首先对于输入，要用一个分隔符，把Q和D分开输入到Bert，然后Bert从D中得到两个向量的输出（都是随机初始化的），得到的两个输出当成下标就是答案

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222734328.png" style="zoom:70%">



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702222821951.png" style="zoom:70%">

### Bert怎么工作的？

#### 解释一

最常见的解释就是，Bert输出的是embedding，在空间不但考虑了单词本身，也考虑了上下文

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223616937.png" style="zoom:70%">

下面是将同一个字，不同上下文的文本输进去的结果

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223721416.png" style="zoom:70%">

可以看见，Bert的结果的确是前五个和后五个之间相似度不高，但他们内部由于上下文相近，相识度高

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702223846990.png" style="zoom:70%">

仔细探究

当把Mask的向量输入Bert时，其实Bert做的事和CBOW一模一样，都是把他上下文的向量输进去，来得到这个向量的输出

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224103174.png" style="zoom:70%">

#### 解释二（待补充）

### 多语言Bert

**如果对Bert进行多语言的训练，在进行英文的问答，居然在中文的测试集上也能有良好的表现**

> 当然要求预料要很高



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224638199.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224714692.png" style="zoom:70%">

#### 一种解释

对于不同语言，其意思相近得词他们的向量表示在空间上也比较相近

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702224922572.png" style="zoom:70%">

### 奇怪的地方

如果把Bert英文的embedding和中文的embedding相加减，是可以让他实现中英文输出互换的

> 也就是Multi - Bert他里面蕴含了语义的信息

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702225343460.png" style="zoom:70%">

