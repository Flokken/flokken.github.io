---
title: Transformer
date: 2023-07-02 
categories: 
  - 深度学习
  - 深度学习-李宏毅2022
tags: 
  - null
---

## Seq2seq

之前将self-attention时，可以知道，对于一组输入，可以有等长的输出，也可以由模型定义输出长度。

当输出一个seq，输出model决定的seq，就称为seq2seq

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702112139453.png" style="zoom:70%">

QA问题一般就可以用seq2seq Model解决

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154146586.png" style="zoom:70%">

### 应用

Syntactic Parsing （**文法剖析**）

核心就是虽然文法剖析任务是树状的，但是可以利用花括号来表示层次关系，就可以硬是给他转成一个seq，这样就可以放到seq2seq model训练

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154504302.png" style="zoom:70%">

>https://arxiv.org/abs/1412.7449

Multi-label Classification（多标签分类）

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702154728163.png" style="zoom:70%">

> https://arxiv.org/abs/1909.03434
>
> https://arxiv.org/abs/1707.05495

甚至还可以做目标检测等等

### 分析

首先来看下transformer的架构

> 当然，transformer也属于是seq2seq，下面**左边是笼统的看seq2seq（就是Encoder和Decoder）**，右边是transformer具体咋实现左边那两个Encode和Decoder

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702155531239.png" style="zoom:90%">

#### Encoder

**首先Encoder就是为了把输入一排向量，输出一排向量，transformer里用了self-attention**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702160611264.png" style="zoom:70%">

transformer的Encoder比较复杂，大致有self-attention和FC但是更加复杂

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702161309505.png" style="zoom:70%">

更详细的说，对于输入的向量，经过self-attention后得到$a$，然后用输入的向量$b$+$a$(也就是残差)，然后再进行Layer norm，区别于Batch norm,详细计算过程如图

> 残差就是求和，把输入的向量加上经过某个层变换后得到的输出

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702161510965.png" style="zoom:70%">

所以回到transformer的Encoder真实实现就是

![image-20230702162453632](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702162453632.png)

#### Decoder

Decoder一般有两种方法实现,AT和NAT，这里主要说AT

**Autogregressive**

首先，Dedcoder当然要读入Encoder的结果，并同时读入一个start，表示句子的开始（独热编码表示）。

然后对于输入，比如机器学习，就经过一个softmax，然后选择其中概率最高的当输出，这里也就是机

> softmax的性质是会给各个向量对应一个输出，可以理解为概率，因为其总和为1

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163046371.png" style="zoom:70%">



然后循环这样得到输出，需要注意的是，**Decoder输出时要依赖上一个自己的输出**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163550918.png" style="zoom:70%">

> 这样其实可能导致一步错，步步错，**也即是Error propagation**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702164854547.png" style="zoom:70%">

##### Decoder框架

**下面是Decoder架构，可以看到除开红色框起来的地方，和Encoder完全一样**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702163821063.png" style="zoom:70%">

###### Mask-self-attention

从字面上理解，Masked就是遮住的意思。在这里指的是，我们现在的attention，输出的时候不再是考虑全部，而是只考虑当前生成的向量的左边的向量（包括自己），比如本来我们输出a2时，self-attention考虑a1-a4,现在只考虑其 从他开始左边的，也就是a1,a1，**所以像遮起来一部分一样，叫Mask-self-attention**

>这里跟decoder的输出有关，decoder本来就生成的时候考虑的就是他上一个输出，当然这里就不应该把后面的就给他弄进来

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702164137778.png" style="zoom:70%">



###### Stop Token

上面Deocder生成句子时，我们还有一个问题就是不知道该生成多长的句子，所以此时我们需要加一个END表示结尾，否则会一直生成下去

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165310689.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165211275.png" style="zoom:70%">



这样就能在合适的时候停止了

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702165358823.png" style="zoom:70%">

###### Cross Attention

现在我们来考虑之前用红线框起来的地方，**也就是Encoder和Decoder之间如何连接的**

![image-20230702170046345](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170046345.png)\

可以看到两个来自于Eecoder（左边框起来那两个）一个来自于Decoder（右边那个），

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170235171.png" style="zoom:70%">

当然，对于下一个字“机”，会进行一样的处理

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702170451526.png" style="zoom:70%">



### Training

**训练时采用Teaching -Force**

其实我们发现，比较关键的地方就是Decoder每次预测输出时，预测的是那个，

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171103998.png" style="zoom:70%">

因此，我们训练时，直接把正确答案给Decoder，而不是让他用自己生成那个来生成下一个，**以此来减小交叉熵**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171313088.png" style="zoom:70%">

#### Scheduled Sampling

之前提到Error propagation，这里一种办法就是输入的时候，就给他一些把中间错了的字给他，让transformer能适应那些有错误情况的例子

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702171640513.png" style="zoom:70%">

