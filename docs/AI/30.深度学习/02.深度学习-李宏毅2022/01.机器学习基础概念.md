---
title: 机器学习基础概念
date: 2023-03-20 00:00:00
categories: 
  - 深度学习
  - 深度学习-李宏毅2022
tags: 
  - null
permalink: /pages/24fc52/
---



# 机器学习基础概念

## Model

机器学习中的model实际就是一个带有未知参数的函数，如linear model 为$y=b+wx_1$

其中：

- x是已知的，称为feature
- w是weight，b是偏置，也称为Bias

## Define Loss from Training Data

损失也是一个函数，一般就叫损失函数。例如$L(b,w)$

其中：

- 输入的b,w是已经前面求出来的参数,$L{(b,w)}$是预测值$y'$

- 这里引入$e= |y' - y|(MAE)$，其实也可以是$e={(y-y')^2}(MSE)$

- 注意，e这个东西可以有好多种算法，根据需要来选

- $Loss = \frac 1 N \sum_1^ne_n$）就叫损失，N是样本的个数，

- Loss越大说明效果越差，越小说明效果越好

- 根据L,w,b画出的等高线图叫做Error surface(误差曲面)

  <img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320170859748.png" style="zoom:70%">

  

## Optimization

**超参数:**需要自己设置的

### **Gradient descent:**

就是通过求微分（梯度）来寻找Loss最小的时候，w,b的取值的一种方法，**我们先只关注w与Loss的关系**

- 首先  (Randomly) Pick an initial value $w_0$， 选定一个$\eta$,（超参数，学习速率）
- 计算 $\frac {\partial L} {\partial x}|w=w_0$（这个东西的值也叫梯度）
- 我们的目的是寻找梯度为0的点，如果值为正， 就应该减小w，为负，就应该增大w
- 更新，$w_1<-w_0-\eta\frac{\partial L}{\partial w}|w=w_0$这样就可以达到我们的目的

上面的过程在PPT上是这样的

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320173036128.png" style="zoom:70%">

**注意局部最小和全局最小**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320173129224.png" style="zoom:70%">

**实际上，我们会同时更新w和b的值来找最小的Loss**

![image-20230320173714066](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230320173714066.png)

# 深度学习基本概念

## 问题引入：

**利用上面的一个函数的线性模型，没有办法拟合下面的红色图像**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321101712092.png" style="zoom:70%">

这种情况叫**model bias**（区别上节课的b）

## 解决办法

**可以用一个从常数项加上一个线性函数的集合去拟合这样的图像，哪怕是曲线也可以这样拟合**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321102300626.png" style="zoom:70%">

利用sigmoid函数来代表蓝色函数：

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321102712714.png" style="zoom:70%">

通过改变sigmoid的参数，可以变换sigmoid，就可以去拟合红色图像

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321103017859.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321103035549.png" style="zoom:70%">

## 更有弹性的model

总结上面的东西，**得到利用sigmoid函数来模拟红色图像的的公式**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321103218969.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321103348745.png" style="zoom:70%">

**因为上面那样写比较复杂，所有可以写成矩阵与向量相乘**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321103556055.png" style="zoom:70%">

再经过simoid计算

> $\sigma$指的是整个计算过程，就是先求和，再用sigmoid计算一下（transpose）
>
> $a$代表把和用sigmoid转换一个的结果
>
> 注意两个b是不一样的东西

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321104033074.png" style="zoom:70%">

参数很多，其中只有x是已知的：，参数太多不好表示，所以用一个$\theta$表示来

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321104629897.png" style="zoom:70%">

## 对新模型微分来算Loss

之前的模型只要两个参数，现在有有很多个

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321104835472.png" style="zoom:70%">

计算方法完全一样，$\theta^0$代表第0个向量，$\theta_1$代表向量的第1个参数

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321104930227.png" style="zoom:70%">

## Batch

实际上如果有很多样本，一次读取所有，或者一个一个读入都不常用**，一般将数据划分为等大的很多个batch（批）**

**也就是说，实际上一次并不是用所有数据的Loss来更新参数，而是用一个batch的来更新**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321105444056.png" style="zoom:70%">

### 注意epoch，batch与参数更新次数的关系

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321105630783.png" style="zoom:70%">

## 增加网络层数

如果把上面的一层层连接起来（把一层的中间输出当成输入，输入到下一层），就叫神经网络

**之所以这么做，是因为可能会有更好的效果。**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321110131241.png" style="zoom:70%">

### 过拟合

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230321110250659.png" style="zoom:70%">



# 常用名词：

hyper-parameter tuning： 调参

feature selection： 特征选取

one-hot-vector： 只有对应的解为1，其他都是0的一个向量