---
title: self-Atterntion
date: 2023-07-02 
categories: 
  - 深度学习
  - 深度学习-李宏毅2022
tags: 
  - null

---

## **Vector Set as Input**

之前的神经网络（CNN）都是一个单独的vecotr比如图像输入到网络，然后得到类别概率啥的

但是现在遇到一些问题要输入很多向量

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702093352267.png" style="zoom:70%">

比如一段话，一个图，一段音频等等，此时网络输出也有变化，可以输出一个label，固定长label，或者由model决定长度的label

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702100206855.png" style="zoom:70%">



先关注 每个输入都有以一个对应输出label的类型

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702100501681.png" style="zoom:70%">

假设输入一段话 i saw a sae   用全连接层做，也就是每个单词输入FC中，能得到与一个输出，**但是问题是，没有语义（上下文关系），比如两个saw应该输出不一样的东西（意思都不一样），但FC显然输出一样的**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702100821597.png" style="zoom:70%">



我们可以改进输入，设计一个window包含上下几个单词，输入进去，就有一定上下文语义了，**但是问题是如果我希望要整个sequence的输入，window就太大了**

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702100559782.png" style="zoom:70%">

## Self-Attention

### 介绍

引入self-attention这个层来解决，当然，self-attention可以交替连接使用，比如self-attention连接Fc层再连接self-attention



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702101325822.png" style="zoom:70%">

那么接下来关注这个层内部是如何工作的

假设$\vec{a}$是输入，$\vec{b}$是输出，对于每一个$a^1$都会去寻找这个sequence中和他相关的输入，然后综合他们，在输出$b^1$

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702102401182.png" style="zoom:70%">



**如何计算两个向量之间关联性即$\alpha$?**现在常见用的方式是dot-product，也就是把输入的向量分别乘以不同的系数矩阵，然后把他们的乘机再乘起来

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702102747888.png" style="zoom:70%">

**接下来，self-attention中就要去计算该向量与各个向量之间的关联性，关联性也叫做attention score** 

>$a^1$也会跟自己做关联性计算，下面图没写，这很重要

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702103059864.png" style="zoom:70%">

最后把attention-score经过某个层，这里是soft-max（好像也可以是ReLu）层，然后就得到一排标准化输出$\alpha ^`$

> 这里好像是为了标准化？

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702103553433.png" style="zoom:70%">

最后，利用上面的$\alpha ^`$和下面的$v$就能得到一个$b^1$的输出，就是把他们的乘机全部加起来

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702104002030.png" style="zoom:70%">

总结一下，感觉就是先对输入点积算关联度，然后用关联度输入到softmax算一下（标准化），最后得到一个标准化输出，再累积输入乘以一个权重再乘刚才的标准化输出得到b

> 后面还有讲从矩阵出发去理解，也就是怎么去算，暂时略去

### **Multi-head Self-attention**

多头注意力机制，主要是想考虑当我们计算相关性的时候，可能有不同种类的相关性，可以让$q^1$和$q^2$代表不同的注意力

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702105439545.png" style="zoom:70%">



<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702105750394.png" style="zoom:70%">

然后可以把他们乘一个系数矩阵得打一个b

![image-20230702105855735](https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702105855735.png)

### Positional Encoding

之前的注意力机制，没有考虑到位置的影响

> 点积，也就是点对，显然是两点之间没有考虑距离的

因此可以考虑人造的 position vector，也可以让模型自己去学习

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702110033384.png" style="zoom:70%">

这个问题还待研究

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702110225858.png" style="zoom:70%">

### 一些应用

比如用在语音上，由于他的sequence太长，可以缩短每次处理的sequence，叫做Truncated Self-attention

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702111144562.png" style="zoom:70%">

比如用在图像上,CNN把输入看成很长的一个vector，也可以把他看成vector的集合，用self-attention来解决

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702111447641.png" style="zoom:70%">

<img src="https://typora-1309665611.cos.ap-nanjing.myqcloud.com/typora/image-20230702111322252.png" style="zoom:70%" style="zoom:70%">

